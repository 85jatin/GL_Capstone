{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video Classification Model : https://towardsdatascience.com/transfer-learning-using-mobilenet-and-keras-c75daf7ff299"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1:  import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, InputLayer, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications import MobileNet\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.python.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from IPython.display import Image\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Read each frame and their corresponding tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A_OffSide_shot1.mp4_frame100.jpg</td>\n",
       "      <td>OffSide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A_OffSide_shot1.mp4_frame101.jpg</td>\n",
       "      <td>OffSide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A_OffSide_shot1.mp4_frame102.jpg</td>\n",
       "      <td>OffSide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A_OffSide_shot1.mp4_frame103.jpg</td>\n",
       "      <td>OffSide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>A_OffSide_shot1.mp4_frame104.jpg</td>\n",
       "      <td>OffSide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              image    class\n",
       "0  A_OffSide_shot1.mp4_frame100.jpg  OffSide\n",
       "1  A_OffSide_shot1.mp4_frame101.jpg  OffSide\n",
       "2  A_OffSide_shot1.mp4_frame102.jpg  OffSide\n",
       "3  A_OffSide_shot1.mp4_frame103.jpg  OffSide\n",
       "4  A_OffSide_shot1.mp4_frame104.jpg  OffSide"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train_new.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Read the frames and then store them as a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2596/2596 [00:38<00:00, 67.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2596, 299, 299, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an empty list\n",
    "train_image = []\n",
    "\n",
    "# for loop to read and store frames\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    # loading the image and keeping the target size as (224,224,3)\n",
    "    img = image.load_img('train_1/'+train['image'][i], target_size=(299,299,))\n",
    "    # converting it to array\n",
    "    img = image.img_to_array(img)\n",
    "    # normalizing the pixel value\n",
    "    img = img/255\n",
    "    # appending the image to the train_image list\n",
    "    train_image.append(img)\n",
    "    \n",
    "# converting the list to numpy array\n",
    "X = np.array(train_image)\n",
    "\n",
    "# shape of the array\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: Create the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the target\n",
    "y = train['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training and validation set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: Create 4 different columns in the target, one for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dummies of target variable for train and validation set\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_test = pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OffSide    1756\n",
       "Legside     840\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: Use the InceptionResnetV3 pre-trained model to create the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now re-use InceptionResnetV3 as it records best performance, freeze the base layers and lets add and train the top few layers (https://medium.com/neuralspace/kaggle-1-winning-approach-for-image-classification-challenge-9c1188157a86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 53985280/219055592 [======>.......................] - ETA: 2:03:3 - ETA: 1:22:5 - ETA: 1:13:5 - ETA: 1:10:3 - ETA: 48:09  - ETA: 48:2 - ETA: 32:4 - ETA: 28:5 - ETA: 27:1 - ETA: 19:3 - ETA: 17:5 - ETA: 16:2 - ETA: 12:3 - ETA: 10:2 - ETA: 10:2 - ETA: 9:3 - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 10:0 - ETA: 10:0 - ETA: 10:0 - ETA: 10:1 - ETA: 10:1 - ETA: 10:1 - ETA: 10:1 - ETA: 10:2 - ETA: 10:2 - ETA: 10:2 - ETA: 10:3 - ETA: 10:3 - ETA: 10:3 - ETA: 10:3 - ETA: 10:3 - ETA: 10:3 - ETA: 10:4 - ETA: 10:4 - ETA: 10:4 - ETA: 10:4 - ETA: 10:4 - ETA: 10:4 - ETA: 10:5 - ETA: 10:5 - ETA: 10:5 - ETA: 10:5 - ETA: 10:5 - ETA: 10:5 - ETA: 10:5 - ETA: 10:5 - ETA: 11:0 - ETA: 11:0 - ETA: 11:0 - ETA: 11:0 - ETA: 11:0 - ETA: 11:0 - ETA: 11:0 - ETA: 11:0 - ETA: 11:0 - ETA: 11:0 - ETA: 11:0 - ETA: 11:1 - ETA: 11:1 - ETA: 11:1 - ETA: 11:1 - ETA: 11:1 - ETA: 11:1 - ETA: 11:1 - ETA: 11:1 - ETA: 11:1 - ETA: 11:1 - ETA: 11:2 - ETA: 11:2 - ETA: 11:2 - ETA: 11:2 - ETA: 11:2 - ETA: 11:2 - ETA: 11:2 - ETA: 11:2 - ETA: 11:2 - ETA: 11:3 - ETA: 11:3 - ETA: 11:3 - ETA: 11:3 - ETA: 11:3 - ETA: 11:3 - ETA: 11:3 - ETA: 11:3 - ETA: 11:3 - ETA: 11:3 - ETA: 11:3 - ETA: 11:3 - ETA: 11:4 - ETA: 11:4 - ETA: 11:4 - ETA: 11:4 - ETA: 11:5 - ETA: 11:5 - ETA: 11:5 - ETA: 12:0 - ETA: 12:0 - ETA: 12:0 - ETA: 12:1 - ETA: 12:1 - ETA: 12:1 - ETA: 12:1 - ETA: 12:1 - ETA: 12:2 - ETA: 12:2 - ETA: 12:2 - ETA: 12:2 - ETA: 12:3 - ETA: 12:2 - ETA: 12:3 - ETA: 12:3 - ETA: 12:3 - ETA: 12:3 - ETA: 12:3 - ETA: 12:3 - ETA: 12:4 - ETA: 12:4 - ETA: 12:4 - ETA: 12:4 - ETA: 12:4 - ETA: 12:5 - ETA: 12:5 - ETA: 12:5 - ETA: 12:5 - ETA: 12:5 - ETA: 13:0 - ETA: 13:0 - ETA: 13:0 - ETA: 13:0 - ETA: 13:0 - ETA: 13:0 - ETA: 13:0 - ETA: 13:0 - ETA: 13:1 - ETA: 13:1 - ETA: 13:1 - ETA: 13:1 - ETA: 13:1 - ETA: 13:1 - ETA: 13:2 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:4 - ETA: 13:4 - ETA: 13:4 - ETA: 13:4 - ETA: 13:4 - ETA: 13:4 - ETA: 13:4 - ETA: 13:4 - ETA: 13:4 - ETA: 13:4 - ETA: 13:4 - ETA: 13:4 - ETA: 13:4 - ETA: 13:4 - ETA: 13:4 - ETA: 13:4 - ETA: 13:3 - ETA: 13:4 - ETA: 13:4 - ETA: 13:4 - ETA: 13:4 - ETA: 13:4 - ETA: 13:4 - ETA: 13:3 - ETA: 13:4 - ETA: 13:4 - ETA: 13:3 - ETA: 13:4 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:3 - ETA: 13:2 - ETA: 13:2 - ETA: 13:2 - ETA: 13:2 - ETA: 13:2 - ETA: 13:2 - ETA: 13:2 - ETA: 13:2 - ETA: 13:2 - ETA: 13:2 - ETA: 13:2 - ETA: 13:2 - ETA: 13:2 - ETA: 13:1 - ETA: 13:1 - ETA: 13:1 - ETA: 13:1 - ETA: 13:1 - ETA: 13:1 - ETA: 13:1 - ETA: 13:1 - ETA: 13:1 - ETA: 13:1 - ETA: 13:0 - ETA: 13:0 - ETA: 13:0 - ETA: 13:0 - ETA: 13:0 - ETA: 13:0 - ETA: 13:0 - ETA: 12:5 - ETA: 12:5 - ETA: 12:5 - ETA: 12:5 - ETA: 12:5 - ETA: 12:5 - ETA: 12:4 - ETA: 12:4 - ETA: 12:4 - ETA: 12:4 - ETA: 12:4 - ETA: 12:4 - ETA: 12:3 - ETA: 12:3 - ETA: 12:3 - ETA: 12:3 - ETA: 12:2 - ETA: 12:2 - ETA: 12:2 - ETA: 12:1 - ETA: 12:1 - ETA: 12:1 - ETA: 12:1 - ETA: 12:1 - ETA: 12:0 - ETA: 12:0 - ETA: 12:0 - ETA: 12:0 - ETA: 11:5 - ETA: 11:5 - ETA: 11:5 - ETA: 11:5 - ETA: 11:4 - ETA: 11:4 - ETA: 11:4 - ETA: 11:3 - ETA: 11:3 - ETA: 11:3 - ETA: 11:3 - ETA: 11:2 - ETA: 11:2 - ETA: 11:2 - ETA: 11:2 - ETA: 11:1 - ETA: 11:1 - ETA: 11:1 - ETA: 11:1 - ETA: 11:0 - ETA: 11:0 - ETA: 10:5 - ETA: 10:5 - ETA: 10:5 - ETA: 10:4 - ETA: 10:4 - ETA: 10:4 - ETA: 10:4 - ETA: 10:3 - ETA: 10:3 - ETA: 10:3 - ETA: 10:2 - ETA: 10:2 - ETA: 10:2 - ETA: 10:1 - ETA: 10:1 - ETA: 10:1 - ETA: 10:1 - ETA: 10:0 - ETA: 10:0 - ETA: 10:0 - ETA: 9:5 - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 9: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6:02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214384640/219055592 [============================>.] - ETA: 6: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 59s - ETA: 59 - ETA: 59 - ETA: 59 - ETA: 58 - ETA: 58 - ETA: 58 - ETA: 58 - ETA: 57 - ETA: 57 - ETA: 57 - ETA: 57 - ETA: 57 - ETA: 56 - ETA: 56 - ETA: 56 - ETA: 55 - ETA: 55 - ETA: 55 - ETA: 55 - ETA: 54 - ETA: 54 - ETA: 54 - ETA: 54 - ETA: 54 - ETA: 53 - ETA: 53 - ETA: 53 - ETA: 52 - ETA: 52 - ETA: 52 - ETA: 52 - ETA: 52 - ETA: 51 - ETA: 51 - ETA: 51 - ETA: 51 - ETA: 50 - ETA: 50 - ETA: 50 - ETA: 50 - ETA: 49 - ETA: 49 - ETA: 49 - ETA: 49 - ETA: 49 - ETA: 48 - ETA: 48 - ETA: 48 - ETA: 48 - ETA: 47 - ETA: 47 - ETA: 47 - ETA: 47 - ETA: 46 - ETA: 46 - ETA: 46 - ETA: 46 - ETA: 45 - ETA: 45 - ETA: 45 - ETA: 45 - ETA: 44 - ETA: 44 - ETA: 44 - ETA: 44 - ETA: 43 - ETA: 43 - ETA: 43 - ETA: 43 - ETA: 42 - ETA: 42 - ETA: 42 - ETA: 42 - ETA: 42 - ETA: 41 - ETA: 41 - ETA: 41 - ETA: 41 - ETA: 40 - ETA: 40 - ETA: 40 - ETA: 40 - ETA: 39 - ETA: 39 - ETA: 39 - ETA: 39 - ETA: 39 - ETA: 38 - ETA: 38 - ETA: 38 - ETA: 38 - ETA: 37 - ETA: 37 - ETA: 37 - ETA: 37 - ETA: 37 - ETA: 36 - ETA: 36 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA: 3s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219062272/219055592 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 172s 1us/step\n"
     ]
    }
   ],
   "source": [
    "#imports the mobilenet model and discards the last 1000 neuron layer.\n",
    "base_model = InceptionResNetV2(weights='imagenet',include_top=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: Extract features from this pre-trained model for our training and validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2076, 8, 8, 1536)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting features for training frames\n",
    "X_train = base_model.predict(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(520, 8, 8, 1536)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting features for validation frames\n",
    "X_test = base_model.predict(X_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_X_train = X_train.shape\n",
    "shape_X_test = X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8: reshape the images into a single dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the training as well as validation frames in single dimension\n",
    "X_train = X_train.reshape(shape_X_train[0], shape_X_train[1]*shape_X_train[2]*shape_X_train[3])\n",
    "X_test = X_test.reshape(shape_X_test[0], shape_X_test[1]*shape_X_test[2]*shape_X_test[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 9: Normalize the pixel values (between 0 and 1, helps the model to converge faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the pixel values\n",
    "max = X_train.max()\n",
    "X_train = X_train/max\n",
    "X_test = X_test/max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 10: Create the architecture of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2076, 98304)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of images\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_X_train = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jatin_Thakkar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "#defining the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu', input_shape=(shape_X_train[1],)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 11: Train our model using the training frames and validate using validation frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to save the weights of best model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "mcp_save = ModelCheckpoint('weight_InceptionResnetV2.hdf5', save_best_only=True, monitor='val_loss', mode='min', verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving Generalization Performance by Switching from Adam to SGD\n",
    "(https://towardsdatascience.com/normalized-direction-preserving-adam-switching-from-adam-to-sgd-and-nesterov-momentum-adam-with-460be5ddf686)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jatin_Thakkar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Jatin_Thakkar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# compiling the model\n",
    "from keras.optimizers import SGD\n",
    "opt = SGD()\n",
    "model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1024)              100664320 \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 101,353,602\n",
      "Trainable params: 101,353,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2076 samples, validate on 520 samples\n",
      "Epoch 1/100\n",
      "2076/2076 [==============================] - ETA: 9:51 - loss: 0.6964 - acc: 0.476 - ETA: 4:56 - loss: 0.6926 - acc: 0.496 - ETA: 3:18 - loss: 0.6925 - acc: 0.502 - ETA: 2:29 - loss: 0.6935 - acc: 0.505 - ETA: 1:58 - loss: 0.6927 - acc: 0.517 - ETA: 1:38 - loss: 0.6927 - acc: 0.526 - ETA: 1:23 - loss: 0.6908 - acc: 0.535 - ETA: 1:12 - loss: 0.6898 - acc: 0.542 - ETA: 1:03 - loss: 0.6884 - acc: 0.554 - ETA: 55s - loss: 0.6896 - acc: 0.551 - ETA: 49s - loss: 0.6894 - acc: 0.55 - ETA: 44s - loss: 0.6892 - acc: 0.55 - ETA: 39s - loss: 0.6899 - acc: 0.55 - ETA: 35s - loss: 0.6903 - acc: 0.55 - ETA: 32s - loss: 0.6898 - acc: 0.55 - ETA: 29s - loss: 0.6888 - acc: 0.55 - ETA: 26s - loss: 0.6878 - acc: 0.56 - ETA: 24s - loss: 0.6868 - acc: 0.56 - ETA: 21s - loss: 0.6866 - acc: 0.56 - ETA: 19s - loss: 0.6863 - acc: 0.56 - ETA: 17s - loss: 0.6852 - acc: 0.57 - ETA: 16s - loss: 0.6852 - acc: 0.57 - ETA: 14s - loss: 0.6844 - acc: 0.57 - ETA: 12s - loss: 0.6833 - acc: 0.58 - ETA: 11s - loss: 0.6823 - acc: 0.58 - ETA: 9s - loss: 0.6822 - acc: 0.5862 - ETA: 7s - loss: 0.6811 - acc: 0.591 - ETA: 6s - loss: 0.6802 - acc: 0.596 - ETA: 4s - loss: 0.6805 - acc: 0.596 - ETA: 3s - loss: 0.6805 - acc: 0.596 - ETA: 1s - loss: 0.6795 - acc: 0.598 - ETA: 0s - loss: 0.6784 - acc: 0.603 - 47s 22ms/step - loss: 0.6784 - acc: 0.6036 - val_loss: 0.6603 - val_acc: 0.6769\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66026, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 2/100\n",
      "2076/2076 [==============================] - ETA: 28s - loss: 0.6532 - acc: 0.67 - ETA: 24s - loss: 0.6559 - acc: 0.64 - ETA: 22s - loss: 0.6689 - acc: 0.61 - ETA: 20s - loss: 0.6677 - acc: 0.61 - ETA: 19s - loss: 0.6658 - acc: 0.62 - ETA: 18s - loss: 0.6633 - acc: 0.64 - ETA: 17s - loss: 0.6625 - acc: 0.64 - ETA: 16s - loss: 0.6618 - acc: 0.64 - ETA: 15s - loss: 0.6606 - acc: 0.64 - ETA: 14s - loss: 0.6608 - acc: 0.64 - ETA: 14s - loss: 0.6617 - acc: 0.64 - ETA: 13s - loss: 0.6610 - acc: 0.64 - ETA: 12s - loss: 0.6615 - acc: 0.63 - ETA: 11s - loss: 0.6611 - acc: 0.63 - ETA: 11s - loss: 0.6594 - acc: 0.64 - ETA: 10s - loss: 0.6599 - acc: 0.64 - ETA: 9s - loss: 0.6580 - acc: 0.6480 - ETA: 9s - loss: 0.6575 - acc: 0.650 - ETA: 8s - loss: 0.6583 - acc: 0.649 - ETA: 8s - loss: 0.6568 - acc: 0.652 - ETA: 7s - loss: 0.6556 - acc: 0.655 - ETA: 6s - loss: 0.6544 - acc: 0.657 - ETA: 6s - loss: 0.6531 - acc: 0.660 - ETA: 5s - loss: 0.6536 - acc: 0.656 - ETA: 4s - loss: 0.6522 - acc: 0.659 - ETA: 4s - loss: 0.6515 - acc: 0.659 - ETA: 3s - loss: 0.6512 - acc: 0.658 - ETA: 2s - loss: 0.6505 - acc: 0.659 - ETA: 2s - loss: 0.6509 - acc: 0.658 - ETA: 1s - loss: 0.6514 - acc: 0.658 - ETA: 0s - loss: 0.6504 - acc: 0.658 - ETA: 0s - loss: 0.6494 - acc: 0.659 - 22s 10ms/step - loss: 0.6485 - acc: 0.6611 - val_loss: 0.6284 - val_acc: 0.6769\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.66026 to 0.62842, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 3/100\n",
      "2076/2076 [==============================] - ETA: 18s - loss: 0.6028 - acc: 0.74 - ETA: 19s - loss: 0.6053 - acc: 0.74 - ETA: 18s - loss: 0.6061 - acc: 0.73 - ETA: 18s - loss: 0.6152 - acc: 0.70 - ETA: 17s - loss: 0.6311 - acc: 0.68 - ETA: 16s - loss: 0.6325 - acc: 0.67 - ETA: 16s - loss: 0.6362 - acc: 0.66 - ETA: 15s - loss: 0.6327 - acc: 0.67 - ETA: 14s - loss: 0.6313 - acc: 0.67 - ETA: 14s - loss: 0.6329 - acc: 0.67 - ETA: 14s - loss: 0.6281 - acc: 0.68 - ETA: 14s - loss: 0.6274 - acc: 0.68 - ETA: 13s - loss: 0.6275 - acc: 0.68 - ETA: 13s - loss: 0.6269 - acc: 0.68 - ETA: 12s - loss: 0.6255 - acc: 0.68 - ETA: 11s - loss: 0.6247 - acc: 0.68 - ETA: 10s - loss: 0.6235 - acc: 0.68 - ETA: 9s - loss: 0.6246 - acc: 0.6858 - ETA: 9s - loss: 0.6260 - acc: 0.682 - ETA: 8s - loss: 0.6255 - acc: 0.681 - ETA: 7s - loss: 0.6289 - acc: 0.677 - ETA: 6s - loss: 0.6298 - acc: 0.674 - ETA: 6s - loss: 0.6317 - acc: 0.669 - ETA: 5s - loss: 0.6357 - acc: 0.664 - ETA: 4s - loss: 0.6370 - acc: 0.660 - ETA: 4s - loss: 0.6363 - acc: 0.661 - ETA: 3s - loss: 0.6365 - acc: 0.660 - ETA: 2s - loss: 0.6344 - acc: 0.664 - ETA: 2s - loss: 0.6360 - acc: 0.661 - ETA: 1s - loss: 0.6358 - acc: 0.663 - ETA: 0s - loss: 0.6353 - acc: 0.663 - ETA: 0s - loss: 0.6335 - acc: 0.666 - 25s 12ms/step - loss: 0.6329 - acc: 0.6664 - val_loss: 0.6101 - val_acc: 0.6769\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62842 to 0.61010, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 4/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.6037 - acc: 0.71 - ETA: 18s - loss: 0.5929 - acc: 0.73 - ETA: 17s - loss: 0.6206 - acc: 0.68 - ETA: 16s - loss: 0.6303 - acc: 0.66 - ETA: 15s - loss: 0.6311 - acc: 0.66 - ETA: 15s - loss: 0.6310 - acc: 0.66 - ETA: 14s - loss: 0.6233 - acc: 0.67 - ETA: 14s - loss: 0.6248 - acc: 0.67 - ETA: 13s - loss: 0.6275 - acc: 0.67 - ETA: 12s - loss: 0.6309 - acc: 0.66 - ETA: 12s - loss: 0.6288 - acc: 0.66 - ETA: 11s - loss: 0.6292 - acc: 0.66 - ETA: 11s - loss: 0.6290 - acc: 0.66 - ETA: 10s - loss: 0.6290 - acc: 0.66 - ETA: 10s - loss: 0.6326 - acc: 0.66 - ETA: 9s - loss: 0.6348 - acc: 0.6562 - ETA: 9s - loss: 0.6327 - acc: 0.658 - ETA: 8s - loss: 0.6338 - acc: 0.656 - ETA: 7s - loss: 0.6289 - acc: 0.663 - ETA: 7s - loss: 0.6299 - acc: 0.660 - ETA: 6s - loss: 0.6296 - acc: 0.661 - ETA: 6s - loss: 0.6288 - acc: 0.661 - ETA: 5s - loss: 0.6290 - acc: 0.661 - ETA: 4s - loss: 0.6278 - acc: 0.662 - ETA: 4s - loss: 0.6262 - acc: 0.665 - ETA: 3s - loss: 0.6271 - acc: 0.663 - ETA: 3s - loss: 0.6284 - acc: 0.660 - ETA: 2s - loss: 0.6275 - acc: 0.662 - ETA: 2s - loss: 0.6268 - acc: 0.663 - ETA: 1s - loss: 0.6261 - acc: 0.664 - ETA: 0s - loss: 0.6252 - acc: 0.666 - ETA: 0s - loss: 0.6232 - acc: 0.669 - 20s 10ms/step - loss: 0.6221 - acc: 0.6708 - val_loss: 0.5953 - val_acc: 0.6769\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.61010 to 0.59531, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 5/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.5787 - acc: 0.68 - ETA: 17s - loss: 0.5804 - acc: 0.71 - ETA: 16s - loss: 0.6057 - acc: 0.67 - ETA: 16s - loss: 0.5956 - acc: 0.69 - ETA: 15s - loss: 0.6064 - acc: 0.68 - ETA: 15s - loss: 0.6064 - acc: 0.68 - ETA: 14s - loss: 0.6040 - acc: 0.69 - ETA: 14s - loss: 0.6036 - acc: 0.69 - ETA: 13s - loss: 0.6051 - acc: 0.68 - ETA: 13s - loss: 0.6053 - acc: 0.68 - ETA: 12s - loss: 0.6029 - acc: 0.68 - ETA: 12s - loss: 0.6031 - acc: 0.68 - ETA: 11s - loss: 0.6072 - acc: 0.68 - ETA: 11s - loss: 0.6028 - acc: 0.68 - ETA: 10s - loss: 0.6066 - acc: 0.67 - ETA: 9s - loss: 0.6032 - acc: 0.6841 - ETA: 9s - loss: 0.6024 - acc: 0.684 - ETA: 8s - loss: 0.6066 - acc: 0.678 - ETA: 7s - loss: 0.6081 - acc: 0.676 - ETA: 7s - loss: 0.6103 - acc: 0.672 - ETA: 6s - loss: 0.6111 - acc: 0.671 - ETA: 6s - loss: 0.6123 - acc: 0.668 - ETA: 5s - loss: 0.6122 - acc: 0.669 - ETA: 4s - loss: 0.6096 - acc: 0.672 - ETA: 4s - loss: 0.6083 - acc: 0.674 - ETA: 3s - loss: 0.6084 - acc: 0.674 - ETA: 3s - loss: 0.6103 - acc: 0.674 - ETA: 2s - loss: 0.6091 - acc: 0.674 - ETA: 2s - loss: 0.6100 - acc: 0.673 - ETA: 1s - loss: 0.6104 - acc: 0.673 - ETA: 0s - loss: 0.6107 - acc: 0.672 - ETA: 0s - loss: 0.6094 - acc: 0.675 - 21s 10ms/step - loss: 0.6083 - acc: 0.6758 - val_loss: 0.5800 - val_acc: 0.6769\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.59531 to 0.57998, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2076/2076 [==============================] - ETA: 17s - loss: 0.5842 - acc: 0.71 - ETA: 17s - loss: 0.5893 - acc: 0.68 - ETA: 18s - loss: 0.5751 - acc: 0.70 - ETA: 19s - loss: 0.5889 - acc: 0.68 - ETA: 19s - loss: 0.5970 - acc: 0.67 - ETA: 18s - loss: 0.5871 - acc: 0.69 - ETA: 17s - loss: 0.5870 - acc: 0.68 - ETA: 17s - loss: 0.5825 - acc: 0.69 - ETA: 16s - loss: 0.5837 - acc: 0.69 - ETA: 15s - loss: 0.5814 - acc: 0.69 - ETA: 14s - loss: 0.5804 - acc: 0.70 - ETA: 13s - loss: 0.5836 - acc: 0.69 - ETA: 13s - loss: 0.5875 - acc: 0.68 - ETA: 12s - loss: 0.5865 - acc: 0.68 - ETA: 11s - loss: 0.5899 - acc: 0.67 - ETA: 10s - loss: 0.5892 - acc: 0.68 - ETA: 10s - loss: 0.5915 - acc: 0.67 - ETA: 9s - loss: 0.5946 - acc: 0.6736 - ETA: 8s - loss: 0.5927 - acc: 0.678 - ETA: 8s - loss: 0.5900 - acc: 0.682 - ETA: 7s - loss: 0.5924 - acc: 0.681 - ETA: 6s - loss: 0.5920 - acc: 0.682 - ETA: 6s - loss: 0.5911 - acc: 0.683 - ETA: 5s - loss: 0.5902 - acc: 0.683 - ETA: 4s - loss: 0.5912 - acc: 0.682 - ETA: 4s - loss: 0.5927 - acc: 0.681 - ETA: 3s - loss: 0.5927 - acc: 0.681 - ETA: 2s - loss: 0.5937 - acc: 0.680 - ETA: 2s - loss: 0.5950 - acc: 0.680 - ETA: 1s - loss: 0.5958 - acc: 0.679 - ETA: 0s - loss: 0.5965 - acc: 0.679 - ETA: 0s - loss: 0.5955 - acc: 0.680 - 22s 11ms/step - loss: 0.5965 - acc: 0.6792 - val_loss: 0.5661 - val_acc: 0.6769\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.57998 to 0.56609, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 7/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.6728 - acc: 0.60 - ETA: 17s - loss: 0.6368 - acc: 0.63 - ETA: 16s - loss: 0.5912 - acc: 0.70 - ETA: 15s - loss: 0.5940 - acc: 0.68 - ETA: 15s - loss: 0.5779 - acc: 0.69 - ETA: 15s - loss: 0.5746 - acc: 0.69 - ETA: 14s - loss: 0.5707 - acc: 0.70 - ETA: 14s - loss: 0.5710 - acc: 0.70 - ETA: 13s - loss: 0.5773 - acc: 0.69 - ETA: 13s - loss: 0.5816 - acc: 0.68 - ETA: 12s - loss: 0.5749 - acc: 0.69 - ETA: 11s - loss: 0.5726 - acc: 0.69 - ETA: 11s - loss: 0.5747 - acc: 0.69 - ETA: 11s - loss: 0.5774 - acc: 0.69 - ETA: 11s - loss: 0.5764 - acc: 0.69 - ETA: 10s - loss: 0.5762 - acc: 0.69 - ETA: 9s - loss: 0.5782 - acc: 0.6866 - ETA: 9s - loss: 0.5798 - acc: 0.683 - ETA: 8s - loss: 0.5790 - acc: 0.684 - ETA: 7s - loss: 0.5776 - acc: 0.685 - ETA: 7s - loss: 0.5809 - acc: 0.681 - ETA: 6s - loss: 0.5792 - acc: 0.684 - ETA: 5s - loss: 0.5776 - acc: 0.686 - ETA: 5s - loss: 0.5768 - acc: 0.687 - ETA: 4s - loss: 0.5759 - acc: 0.686 - ETA: 3s - loss: 0.5743 - acc: 0.689 - ETA: 3s - loss: 0.5759 - acc: 0.687 - ETA: 2s - loss: 0.5764 - acc: 0.685 - ETA: 2s - loss: 0.5792 - acc: 0.682 - ETA: 1s - loss: 0.5808 - acc: 0.681 - ETA: 0s - loss: 0.5801 - acc: 0.681 - ETA: 0s - loss: 0.5809 - acc: 0.679 - 21s 10ms/step - loss: 0.5816 - acc: 0.6787 - val_loss: 0.5492 - val_acc: 0.6769\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.56609 to 0.54925, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 8/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.5609 - acc: 0.68 - ETA: 17s - loss: 0.5945 - acc: 0.64 - ETA: 16s - loss: 0.6047 - acc: 0.62 - ETA: 18s - loss: 0.5800 - acc: 0.66 - ETA: 19s - loss: 0.5702 - acc: 0.68 - ETA: 18s - loss: 0.5705 - acc: 0.68 - ETA: 17s - loss: 0.5674 - acc: 0.69 - ETA: 16s - loss: 0.5747 - acc: 0.67 - ETA: 15s - loss: 0.5741 - acc: 0.67 - ETA: 14s - loss: 0.5709 - acc: 0.68 - ETA: 13s - loss: 0.5621 - acc: 0.69 - ETA: 12s - loss: 0.5645 - acc: 0.69 - ETA: 12s - loss: 0.5622 - acc: 0.69 - ETA: 11s - loss: 0.5632 - acc: 0.69 - ETA: 10s - loss: 0.5672 - acc: 0.68 - ETA: 10s - loss: 0.5671 - acc: 0.68 - ETA: 9s - loss: 0.5661 - acc: 0.6857 - ETA: 8s - loss: 0.5654 - acc: 0.687 - ETA: 8s - loss: 0.5658 - acc: 0.685 - ETA: 7s - loss: 0.5654 - acc: 0.687 - ETA: 6s - loss: 0.5645 - acc: 0.689 - ETA: 6s - loss: 0.5657 - acc: 0.688 - ETA: 5s - loss: 0.5651 - acc: 0.688 - ETA: 5s - loss: 0.5689 - acc: 0.683 - ETA: 4s - loss: 0.5679 - acc: 0.684 - ETA: 3s - loss: 0.5691 - acc: 0.684 - ETA: 3s - loss: 0.5719 - acc: 0.682 - ETA: 2s - loss: 0.5718 - acc: 0.681 - ETA: 2s - loss: 0.5742 - acc: 0.678 - ETA: 1s - loss: 0.5750 - acc: 0.677 - ETA: 0s - loss: 0.5727 - acc: 0.679 - ETA: 0s - loss: 0.5721 - acc: 0.680 - 21s 10ms/step - loss: 0.5703 - acc: 0.6833 - val_loss: 0.5323 - val_acc: 0.6769\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.54925 to 0.53231, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 9/100\n",
      "2076/2076 [==============================] - ETA: 20s - loss: 0.5200 - acc: 0.76 - ETA: 19s - loss: 0.5133 - acc: 0.75 - ETA: 18s - loss: 0.5130 - acc: 0.75 - ETA: 18s - loss: 0.5375 - acc: 0.73 - ETA: 18s - loss: 0.5373 - acc: 0.72 - ETA: 17s - loss: 0.5466 - acc: 0.70 - ETA: 16s - loss: 0.5632 - acc: 0.68 - ETA: 15s - loss: 0.5638 - acc: 0.68 - ETA: 14s - loss: 0.5685 - acc: 0.67 - ETA: 14s - loss: 0.5721 - acc: 0.67 - ETA: 13s - loss: 0.5724 - acc: 0.67 - ETA: 12s - loss: 0.5661 - acc: 0.68 - ETA: 12s - loss: 0.5644 - acc: 0.68 - ETA: 11s - loss: 0.5625 - acc: 0.68 - ETA: 10s - loss: 0.5586 - acc: 0.69 - ETA: 10s - loss: 0.5623 - acc: 0.68 - ETA: 9s - loss: 0.5603 - acc: 0.6875 - ETA: 9s - loss: 0.5599 - acc: 0.688 - ETA: 8s - loss: 0.5559 - acc: 0.689 - ETA: 7s - loss: 0.5532 - acc: 0.694 - ETA: 7s - loss: 0.5543 - acc: 0.691 - ETA: 6s - loss: 0.5578 - acc: 0.687 - ETA: 6s - loss: 0.5555 - acc: 0.688 - ETA: 5s - loss: 0.5546 - acc: 0.688 - ETA: 4s - loss: 0.5550 - acc: 0.687 - ETA: 4s - loss: 0.5570 - acc: 0.685 - ETA: 3s - loss: 0.5558 - acc: 0.685 - ETA: 2s - loss: 0.5549 - acc: 0.686 - ETA: 2s - loss: 0.5568 - acc: 0.685 - ETA: 1s - loss: 0.5556 - acc: 0.686 - ETA: 0s - loss: 0.5555 - acc: 0.686 - ETA: 0s - loss: 0.5571 - acc: 0.683 - 24s 12ms/step - loss: 0.5578 - acc: 0.6830 - val_loss: 0.5160 - val_acc: 0.6798\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.53231 to 0.51601, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 10/100\n",
      "2076/2076 [==============================] - ETA: 22s - loss: 0.5399 - acc: 0.71 - ETA: 21s - loss: 0.5420 - acc: 0.70 - ETA: 20s - loss: 0.5343 - acc: 0.71 - ETA: 19s - loss: 0.5515 - acc: 0.70 - ETA: 19s - loss: 0.5574 - acc: 0.71 - ETA: 18s - loss: 0.5472 - acc: 0.72 - ETA: 17s - loss: 0.5488 - acc: 0.71 - ETA: 16s - loss: 0.5515 - acc: 0.71 - ETA: 16s - loss: 0.5491 - acc: 0.71 - ETA: 15s - loss: 0.5432 - acc: 0.72 - ETA: 14s - loss: 0.5445 - acc: 0.71 - ETA: 13s - loss: 0.5498 - acc: 0.70 - ETA: 13s - loss: 0.5489 - acc: 0.70 - ETA: 12s - loss: 0.5488 - acc: 0.69 - ETA: 11s - loss: 0.5510 - acc: 0.69 - ETA: 11s - loss: 0.5530 - acc: 0.69 - ETA: 11s - loss: 0.5522 - acc: 0.69 - ETA: 10s - loss: 0.5501 - acc: 0.69 - ETA: 9s - loss: 0.5485 - acc: 0.6965 - ETA: 8s - loss: 0.5538 - acc: 0.692 - ETA: 8s - loss: 0.5535 - acc: 0.693 - ETA: 7s - loss: 0.5485 - acc: 0.698 - ETA: 6s - loss: 0.5486 - acc: 0.697 - ETA: 6s - loss: 0.5519 - acc: 0.694 - ETA: 5s - loss: 0.5512 - acc: 0.695 - ETA: 4s - loss: 0.5520 - acc: 0.692 - ETA: 3s - loss: 0.5516 - acc: 0.691 - ETA: 3s - loss: 0.5500 - acc: 0.693 - ETA: 2s - loss: 0.5499 - acc: 0.691 - ETA: 1s - loss: 0.5508 - acc: 0.690 - ETA: 1s - loss: 0.5490 - acc: 0.691 - ETA: 0s - loss: 0.5486 - acc: 0.691 - 24s 12ms/step - loss: 0.5478 - acc: 0.6915 - val_loss: 0.4989 - val_acc: 0.6933\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.51601 to 0.49889, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2076/2076 [==============================] - ETA: 18s - loss: 0.5291 - acc: 0.69 - ETA: 18s - loss: 0.4953 - acc: 0.71 - ETA: 17s - loss: 0.5313 - acc: 0.71 - ETA: 16s - loss: 0.5216 - acc: 0.72 - ETA: 16s - loss: 0.5220 - acc: 0.71 - ETA: 15s - loss: 0.5283 - acc: 0.71 - ETA: 15s - loss: 0.5197 - acc: 0.71 - ETA: 14s - loss: 0.5145 - acc: 0.72 - ETA: 13s - loss: 0.5229 - acc: 0.71 - ETA: 13s - loss: 0.5341 - acc: 0.68 - ETA: 12s - loss: 0.5326 - acc: 0.69 - ETA: 12s - loss: 0.5225 - acc: 0.70 - ETA: 11s - loss: 0.5230 - acc: 0.70 - ETA: 10s - loss: 0.5239 - acc: 0.70 - ETA: 10s - loss: 0.5246 - acc: 0.70 - ETA: 9s - loss: 0.5287 - acc: 0.6978 - ETA: 9s - loss: 0.5343 - acc: 0.691 - ETA: 8s - loss: 0.5344 - acc: 0.690 - ETA: 8s - loss: 0.5340 - acc: 0.689 - ETA: 7s - loss: 0.5346 - acc: 0.692 - ETA: 6s - loss: 0.5358 - acc: 0.691 - ETA: 6s - loss: 0.5315 - acc: 0.696 - ETA: 5s - loss: 0.5326 - acc: 0.695 - ETA: 5s - loss: 0.5324 - acc: 0.695 - ETA: 4s - loss: 0.5342 - acc: 0.693 - ETA: 3s - loss: 0.5328 - acc: 0.694 - ETA: 3s - loss: 0.5317 - acc: 0.696 - ETA: 2s - loss: 0.5307 - acc: 0.698 - ETA: 2s - loss: 0.5327 - acc: 0.697 - ETA: 1s - loss: 0.5312 - acc: 0.698 - ETA: 0s - loss: 0.5318 - acc: 0.697 - ETA: 0s - loss: 0.5343 - acc: 0.693 - 21s 10ms/step - loss: 0.5353 - acc: 0.6939 - val_loss: 0.4830 - val_acc: 0.7240\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.49889 to 0.48301, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 12/100\n",
      "2076/2076 [==============================] - ETA: 18s - loss: 0.5601 - acc: 0.69 - ETA: 17s - loss: 0.5036 - acc: 0.73 - ETA: 18s - loss: 0.4995 - acc: 0.72 - ETA: 18s - loss: 0.5299 - acc: 0.67 - ETA: 19s - loss: 0.5254 - acc: 0.69 - ETA: 18s - loss: 0.5328 - acc: 0.69 - ETA: 18s - loss: 0.5237 - acc: 0.70 - ETA: 17s - loss: 0.5233 - acc: 0.70 - ETA: 16s - loss: 0.5186 - acc: 0.70 - ETA: 15s - loss: 0.5187 - acc: 0.70 - ETA: 14s - loss: 0.5178 - acc: 0.70 - ETA: 13s - loss: 0.5189 - acc: 0.70 - ETA: 12s - loss: 0.5121 - acc: 0.71 - ETA: 11s - loss: 0.5128 - acc: 0.71 - ETA: 11s - loss: 0.5136 - acc: 0.71 - ETA: 10s - loss: 0.5130 - acc: 0.71 - ETA: 9s - loss: 0.5148 - acc: 0.7142 - ETA: 9s - loss: 0.5133 - acc: 0.716 - ETA: 8s - loss: 0.5149 - acc: 0.712 - ETA: 7s - loss: 0.5158 - acc: 0.711 - ETA: 7s - loss: 0.5177 - acc: 0.710 - ETA: 6s - loss: 0.5177 - acc: 0.709 - ETA: 5s - loss: 0.5162 - acc: 0.711 - ETA: 5s - loss: 0.5181 - acc: 0.707 - ETA: 4s - loss: 0.5210 - acc: 0.703 - ETA: 3s - loss: 0.5195 - acc: 0.704 - ETA: 3s - loss: 0.5210 - acc: 0.702 - ETA: 2s - loss: 0.5207 - acc: 0.702 - ETA: 2s - loss: 0.5210 - acc: 0.701 - ETA: 1s - loss: 0.5200 - acc: 0.702 - ETA: 0s - loss: 0.5214 - acc: 0.703 - ETA: 0s - loss: 0.5228 - acc: 0.700 - 22s 11ms/step - loss: 0.5223 - acc: 0.7021 - val_loss: 0.4664 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.48301 to 0.46639, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 13/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.4535 - acc: 0.77 - ETA: 16s - loss: 0.4525 - acc: 0.75 - ETA: 16s - loss: 0.4659 - acc: 0.74 - ETA: 15s - loss: 0.4665 - acc: 0.73 - ETA: 15s - loss: 0.4701 - acc: 0.74 - ETA: 14s - loss: 0.4899 - acc: 0.72 - ETA: 14s - loss: 0.4954 - acc: 0.72 - ETA: 14s - loss: 0.4983 - acc: 0.71 - ETA: 13s - loss: 0.4993 - acc: 0.71 - ETA: 13s - loss: 0.5045 - acc: 0.70 - ETA: 12s - loss: 0.4977 - acc: 0.72 - ETA: 11s - loss: 0.4939 - acc: 0.72 - ETA: 11s - loss: 0.4984 - acc: 0.72 - ETA: 11s - loss: 0.4922 - acc: 0.72 - ETA: 10s - loss: 0.4949 - acc: 0.72 - ETA: 10s - loss: 0.5005 - acc: 0.71 - ETA: 9s - loss: 0.5048 - acc: 0.7109 - ETA: 8s - loss: 0.5048 - acc: 0.711 - ETA: 8s - loss: 0.5123 - acc: 0.706 - ETA: 7s - loss: 0.5118 - acc: 0.705 - ETA: 7s - loss: 0.5120 - acc: 0.705 - ETA: 6s - loss: 0.5106 - acc: 0.709 - ETA: 5s - loss: 0.5097 - acc: 0.708 - ETA: 5s - loss: 0.5102 - acc: 0.706 - ETA: 4s - loss: 0.5132 - acc: 0.702 - ETA: 4s - loss: 0.5127 - acc: 0.703 - ETA: 3s - loss: 0.5093 - acc: 0.706 - ETA: 2s - loss: 0.5077 - acc: 0.708 - ETA: 2s - loss: 0.5088 - acc: 0.706 - ETA: 1s - loss: 0.5104 - acc: 0.705 - ETA: 0s - loss: 0.5097 - acc: 0.705 - ETA: 0s - loss: 0.5094 - acc: 0.706 - 22s 11ms/step - loss: 0.5089 - acc: 0.7076 - val_loss: 0.4489 - val_acc: 0.7875\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.46639 to 0.44891, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 14/100\n",
      "2076/2076 [==============================] - ETA: 20s - loss: 0.5267 - acc: 0.67 - ETA: 20s - loss: 0.5184 - acc: 0.66 - ETA: 20s - loss: 0.5056 - acc: 0.68 - ETA: 20s - loss: 0.5130 - acc: 0.68 - ETA: 19s - loss: 0.5134 - acc: 0.68 - ETA: 18s - loss: 0.5013 - acc: 0.69 - ETA: 17s - loss: 0.4973 - acc: 0.70 - ETA: 17s - loss: 0.5033 - acc: 0.70 - ETA: 16s - loss: 0.5027 - acc: 0.70 - ETA: 15s - loss: 0.5005 - acc: 0.70 - ETA: 14s - loss: 0.5019 - acc: 0.70 - ETA: 13s - loss: 0.5013 - acc: 0.70 - ETA: 12s - loss: 0.5052 - acc: 0.70 - ETA: 12s - loss: 0.5038 - acc: 0.70 - ETA: 11s - loss: 0.5041 - acc: 0.70 - ETA: 10s - loss: 0.5060 - acc: 0.70 - ETA: 9s - loss: 0.5044 - acc: 0.7068 - ETA: 9s - loss: 0.5022 - acc: 0.710 - ETA: 8s - loss: 0.4957 - acc: 0.715 - ETA: 8s - loss: 0.5000 - acc: 0.713 - ETA: 7s - loss: 0.5011 - acc: 0.713 - ETA: 6s - loss: 0.4989 - acc: 0.713 - ETA: 6s - loss: 0.4975 - acc: 0.715 - ETA: 5s - loss: 0.4942 - acc: 0.720 - ETA: 4s - loss: 0.4919 - acc: 0.721 - ETA: 4s - loss: 0.4919 - acc: 0.720 - ETA: 3s - loss: 0.4897 - acc: 0.723 - ETA: 2s - loss: 0.4880 - acc: 0.724 - ETA: 2s - loss: 0.4902 - acc: 0.722 - ETA: 1s - loss: 0.4878 - acc: 0.726 - ETA: 0s - loss: 0.4906 - acc: 0.722 - ETA: 0s - loss: 0.4910 - acc: 0.721 - 23s 11ms/step - loss: 0.4907 - acc: 0.7204 - val_loss: 0.4318 - val_acc: 0.8212\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.44891 to 0.43179, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 15/100\n",
      "2076/2076 [==============================] - ETA: 18s - loss: 0.4489 - acc: 0.71 - ETA: 19s - loss: 0.4289 - acc: 0.75 - ETA: 17s - loss: 0.4226 - acc: 0.77 - ETA: 17s - loss: 0.4289 - acc: 0.77 - ETA: 16s - loss: 0.4307 - acc: 0.78 - ETA: 15s - loss: 0.4418 - acc: 0.76 - ETA: 15s - loss: 0.4446 - acc: 0.76 - ETA: 16s - loss: 0.4419 - acc: 0.76 - ETA: 16s - loss: 0.4433 - acc: 0.75 - ETA: 15s - loss: 0.4475 - acc: 0.75 - ETA: 14s - loss: 0.4541 - acc: 0.75 - ETA: 13s - loss: 0.4550 - acc: 0.74 - ETA: 13s - loss: 0.4611 - acc: 0.73 - ETA: 12s - loss: 0.4596 - acc: 0.74 - ETA: 11s - loss: 0.4583 - acc: 0.74 - ETA: 10s - loss: 0.4649 - acc: 0.73 - ETA: 10s - loss: 0.4689 - acc: 0.73 - ETA: 9s - loss: 0.4689 - acc: 0.7352 - ETA: 9s - loss: 0.4708 - acc: 0.731 - ETA: 8s - loss: 0.4744 - acc: 0.727 - ETA: 7s - loss: 0.4751 - acc: 0.726 - ETA: 7s - loss: 0.4770 - acc: 0.726 - ETA: 6s - loss: 0.4752 - acc: 0.727 - ETA: 5s - loss: 0.4753 - acc: 0.729 - ETA: 4s - loss: 0.4759 - acc: 0.729 - ETA: 4s - loss: 0.4732 - acc: 0.733 - ETA: 3s - loss: 0.4765 - acc: 0.728 - ETA: 2s - loss: 0.4753 - acc: 0.729 - ETA: 2s - loss: 0.4772 - acc: 0.729 - ETA: 1s - loss: 0.4774 - acc: 0.729 - ETA: 0s - loss: 0.4767 - acc: 0.729 - ETA: 0s - loss: 0.4754 - acc: 0.731 - 23s 11ms/step - loss: 0.4737 - acc: 0.7331 - val_loss: 0.4159 - val_acc: 0.8337\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.43179 to 0.41589, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2076/2076 [==============================] - ETA: 17s - loss: 0.3848 - acc: 0.81 - ETA: 17s - loss: 0.4004 - acc: 0.78 - ETA: 17s - loss: 0.3979 - acc: 0.78 - ETA: 16s - loss: 0.4228 - acc: 0.76 - ETA: 16s - loss: 0.4308 - acc: 0.76 - ETA: 15s - loss: 0.4375 - acc: 0.75 - ETA: 14s - loss: 0.4431 - acc: 0.74 - ETA: 14s - loss: 0.4480 - acc: 0.75 - ETA: 13s - loss: 0.4625 - acc: 0.74 - ETA: 13s - loss: 0.4589 - acc: 0.74 - ETA: 12s - loss: 0.4560 - acc: 0.74 - ETA: 12s - loss: 0.4610 - acc: 0.74 - ETA: 11s - loss: 0.4620 - acc: 0.73 - ETA: 10s - loss: 0.4620 - acc: 0.73 - ETA: 10s - loss: 0.4565 - acc: 0.74 - ETA: 9s - loss: 0.4513 - acc: 0.7495 - ETA: 9s - loss: 0.4494 - acc: 0.749 - ETA: 8s - loss: 0.4486 - acc: 0.749 - ETA: 8s - loss: 0.4482 - acc: 0.750 - ETA: 7s - loss: 0.4470 - acc: 0.752 - ETA: 6s - loss: 0.4484 - acc: 0.749 - ETA: 6s - loss: 0.4495 - acc: 0.746 - ETA: 5s - loss: 0.4524 - acc: 0.742 - ETA: 5s - loss: 0.4537 - acc: 0.742 - ETA: 4s - loss: 0.4570 - acc: 0.739 - ETA: 3s - loss: 0.4575 - acc: 0.738 - ETA: 3s - loss: 0.4566 - acc: 0.740 - ETA: 2s - loss: 0.4572 - acc: 0.741 - ETA: 2s - loss: 0.4565 - acc: 0.742 - ETA: 1s - loss: 0.4568 - acc: 0.743 - ETA: 0s - loss: 0.4567 - acc: 0.742 - ETA: 0s - loss: 0.4579 - acc: 0.743 - 21s 10ms/step - loss: 0.4582 - acc: 0.7425 - val_loss: 0.3992 - val_acc: 0.8558\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.41589 to 0.39923, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 17/100\n",
      "2076/2076 [==============================] - ETA: 18s - loss: 0.5010 - acc: 0.71 - ETA: 17s - loss: 0.4837 - acc: 0.72 - ETA: 17s - loss: 0.4690 - acc: 0.73 - ETA: 16s - loss: 0.4587 - acc: 0.74 - ETA: 16s - loss: 0.4643 - acc: 0.74 - ETA: 15s - loss: 0.4724 - acc: 0.73 - ETA: 14s - loss: 0.4709 - acc: 0.73 - ETA: 14s - loss: 0.4759 - acc: 0.73 - ETA: 13s - loss: 0.4748 - acc: 0.74 - ETA: 12s - loss: 0.4748 - acc: 0.73 - ETA: 12s - loss: 0.4673 - acc: 0.74 - ETA: 11s - loss: 0.4632 - acc: 0.75 - ETA: 11s - loss: 0.4629 - acc: 0.75 - ETA: 10s - loss: 0.4625 - acc: 0.74 - ETA: 10s - loss: 0.4571 - acc: 0.74 - ETA: 9s - loss: 0.4560 - acc: 0.7476 - ETA: 8s - loss: 0.4514 - acc: 0.754 - ETA: 8s - loss: 0.4520 - acc: 0.753 - ETA: 7s - loss: 0.4491 - acc: 0.758 - ETA: 7s - loss: 0.4477 - acc: 0.758 - ETA: 6s - loss: 0.4475 - acc: 0.759 - ETA: 6s - loss: 0.4491 - acc: 0.758 - ETA: 5s - loss: 0.4461 - acc: 0.761 - ETA: 4s - loss: 0.4450 - acc: 0.765 - ETA: 4s - loss: 0.4411 - acc: 0.767 - ETA: 3s - loss: 0.4432 - acc: 0.764 - ETA: 3s - loss: 0.4444 - acc: 0.763 - ETA: 2s - loss: 0.4451 - acc: 0.762 - ETA: 2s - loss: 0.4434 - acc: 0.763 - ETA: 1s - loss: 0.4434 - acc: 0.763 - ETA: 0s - loss: 0.4414 - acc: 0.764 - ETA: 0s - loss: 0.4404 - acc: 0.766 - 22s 11ms/step - loss: 0.4394 - acc: 0.7664 - val_loss: 0.3822 - val_acc: 0.8433\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.39923 to 0.38222, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 18/100\n",
      "2076/2076 [==============================] - ETA: 20s - loss: 0.4037 - acc: 0.77 - ETA: 18s - loss: 0.4562 - acc: 0.76 - ETA: 17s - loss: 0.4843 - acc: 0.73 - ETA: 17s - loss: 0.4803 - acc: 0.74 - ETA: 16s - loss: 0.4789 - acc: 0.75 - ETA: 15s - loss: 0.4799 - acc: 0.74 - ETA: 15s - loss: 0.4655 - acc: 0.75 - ETA: 14s - loss: 0.4633 - acc: 0.75 - ETA: 13s - loss: 0.4603 - acc: 0.75 - ETA: 13s - loss: 0.4680 - acc: 0.75 - ETA: 12s - loss: 0.4612 - acc: 0.75 - ETA: 12s - loss: 0.4607 - acc: 0.75 - ETA: 11s - loss: 0.4619 - acc: 0.75 - ETA: 10s - loss: 0.4564 - acc: 0.76 - ETA: 10s - loss: 0.4536 - acc: 0.77 - ETA: 9s - loss: 0.4474 - acc: 0.7754 - ETA: 9s - loss: 0.4468 - acc: 0.774 - ETA: 8s - loss: 0.4468 - acc: 0.773 - ETA: 7s - loss: 0.4462 - acc: 0.775 - ETA: 7s - loss: 0.4437 - acc: 0.776 - ETA: 6s - loss: 0.4413 - acc: 0.778 - ETA: 6s - loss: 0.4396 - acc: 0.778 - ETA: 5s - loss: 0.4384 - acc: 0.779 - ETA: 5s - loss: 0.4374 - acc: 0.779 - ETA: 4s - loss: 0.4347 - acc: 0.783 - ETA: 3s - loss: 0.4310 - acc: 0.787 - ETA: 3s - loss: 0.4299 - acc: 0.788 - ETA: 2s - loss: 0.4256 - acc: 0.792 - ETA: 2s - loss: 0.4274 - acc: 0.789 - ETA: 1s - loss: 0.4246 - acc: 0.790 - ETA: 0s - loss: 0.4255 - acc: 0.789 - ETA: 0s - loss: 0.4261 - acc: 0.789 - 21s 10ms/step - loss: 0.4253 - acc: 0.7909 - val_loss: 0.3642 - val_acc: 0.8538\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.38222 to 0.36417, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 19/100\n",
      "2076/2076 [==============================] - ETA: 21s - loss: 0.3834 - acc: 0.80 - ETA: 23s - loss: 0.3978 - acc: 0.81 - ETA: 21s - loss: 0.3883 - acc: 0.83 - ETA: 19s - loss: 0.4039 - acc: 0.80 - ETA: 18s - loss: 0.4098 - acc: 0.79 - ETA: 17s - loss: 0.4113 - acc: 0.79 - ETA: 16s - loss: 0.4170 - acc: 0.79 - ETA: 15s - loss: 0.4048 - acc: 0.80 - ETA: 14s - loss: 0.4091 - acc: 0.79 - ETA: 14s - loss: 0.4111 - acc: 0.79 - ETA: 13s - loss: 0.4100 - acc: 0.80 - ETA: 12s - loss: 0.4122 - acc: 0.79 - ETA: 12s - loss: 0.4102 - acc: 0.79 - ETA: 11s - loss: 0.4098 - acc: 0.80 - ETA: 10s - loss: 0.4131 - acc: 0.80 - ETA: 10s - loss: 0.4161 - acc: 0.79 - ETA: 9s - loss: 0.4149 - acc: 0.7978 - ETA: 8s - loss: 0.4119 - acc: 0.799 - ETA: 8s - loss: 0.4108 - acc: 0.801 - ETA: 7s - loss: 0.4133 - acc: 0.799 - ETA: 6s - loss: 0.4125 - acc: 0.801 - ETA: 6s - loss: 0.4162 - acc: 0.798 - ETA: 5s - loss: 0.4176 - acc: 0.796 - ETA: 5s - loss: 0.4151 - acc: 0.797 - ETA: 4s - loss: 0.4130 - acc: 0.799 - ETA: 3s - loss: 0.4135 - acc: 0.799 - ETA: 3s - loss: 0.4128 - acc: 0.800 - ETA: 2s - loss: 0.4126 - acc: 0.802 - ETA: 2s - loss: 0.4116 - acc: 0.803 - ETA: 1s - loss: 0.4107 - acc: 0.805 - ETA: 0s - loss: 0.4105 - acc: 0.804 - ETA: 0s - loss: 0.4093 - acc: 0.805 - 21s 10ms/step - loss: 0.4100 - acc: 0.8025 - val_loss: 0.3457 - val_acc: 0.8731\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.36417 to 0.34568, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 20/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.5211 - acc: 0.75 - ETA: 17s - loss: 0.4646 - acc: 0.78 - ETA: 16s - loss: 0.4244 - acc: 0.81 - ETA: 16s - loss: 0.3968 - acc: 0.83 - ETA: 15s - loss: 0.3999 - acc: 0.82 - ETA: 15s - loss: 0.4019 - acc: 0.81 - ETA: 17s - loss: 0.4029 - acc: 0.81 - ETA: 16s - loss: 0.4008 - acc: 0.81 - ETA: 15s - loss: 0.4106 - acc: 0.80 - ETA: 14s - loss: 0.4006 - acc: 0.81 - ETA: 14s - loss: 0.4054 - acc: 0.81 - ETA: 13s - loss: 0.4077 - acc: 0.80 - ETA: 12s - loss: 0.4003 - acc: 0.81 - ETA: 11s - loss: 0.3997 - acc: 0.81 - ETA: 11s - loss: 0.4021 - acc: 0.81 - ETA: 10s - loss: 0.3997 - acc: 0.81 - ETA: 9s - loss: 0.3984 - acc: 0.8148 - ETA: 9s - loss: 0.4000 - acc: 0.813 - ETA: 8s - loss: 0.4007 - acc: 0.814 - ETA: 7s - loss: 0.3991 - acc: 0.816 - ETA: 7s - loss: 0.4004 - acc: 0.814 - ETA: 6s - loss: 0.4000 - acc: 0.811 - ETA: 5s - loss: 0.3972 - acc: 0.815 - ETA: 5s - loss: 0.3971 - acc: 0.816 - ETA: 4s - loss: 0.3940 - acc: 0.817 - ETA: 3s - loss: 0.3937 - acc: 0.816 - ETA: 3s - loss: 0.3941 - acc: 0.815 - ETA: 2s - loss: 0.3934 - acc: 0.816 - ETA: 2s - loss: 0.3929 - acc: 0.815 - ETA: 1s - loss: 0.3935 - acc: 0.815 - ETA: 0s - loss: 0.3924 - acc: 0.814 - ETA: 0s - loss: 0.3908 - acc: 0.815 - 21s 10ms/step - loss: 0.3906 - acc: 0.8167 - val_loss: 0.3291 - val_acc: 0.8731\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.34568 to 0.32909, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2076/2076 [==============================] - ETA: 18s - loss: 0.2714 - acc: 0.91 - ETA: 18s - loss: 0.3158 - acc: 0.86 - ETA: 17s - loss: 0.3335 - acc: 0.85 - ETA: 16s - loss: 0.3252 - acc: 0.86 - ETA: 16s - loss: 0.3358 - acc: 0.86 - ETA: 15s - loss: 0.3354 - acc: 0.86 - ETA: 15s - loss: 0.3412 - acc: 0.86 - ETA: 14s - loss: 0.3465 - acc: 0.85 - ETA: 13s - loss: 0.3486 - acc: 0.86 - ETA: 13s - loss: 0.3582 - acc: 0.85 - ETA: 12s - loss: 0.3625 - acc: 0.84 - ETA: 12s - loss: 0.3683 - acc: 0.83 - ETA: 11s - loss: 0.3714 - acc: 0.83 - ETA: 10s - loss: 0.3698 - acc: 0.83 - ETA: 10s - loss: 0.3727 - acc: 0.83 - ETA: 9s - loss: 0.3706 - acc: 0.8350 - ETA: 9s - loss: 0.3753 - acc: 0.829 - ETA: 8s - loss: 0.3740 - acc: 0.830 - ETA: 7s - loss: 0.3748 - acc: 0.832 - ETA: 7s - loss: 0.3727 - acc: 0.835 - ETA: 6s - loss: 0.3717 - acc: 0.835 - ETA: 6s - loss: 0.3680 - acc: 0.838 - ETA: 5s - loss: 0.3680 - acc: 0.837 - ETA: 5s - loss: 0.3677 - acc: 0.838 - ETA: 4s - loss: 0.3664 - acc: 0.838 - ETA: 4s - loss: 0.3648 - acc: 0.840 - ETA: 3s - loss: 0.3631 - acc: 0.841 - ETA: 2s - loss: 0.3669 - acc: 0.839 - ETA: 2s - loss: 0.3660 - acc: 0.841 - ETA: 1s - loss: 0.3646 - acc: 0.840 - ETA: 0s - loss: 0.3671 - acc: 0.840 - ETA: 0s - loss: 0.3685 - acc: 0.838 - 22s 11ms/step - loss: 0.3701 - acc: 0.8384 - val_loss: 0.3273 - val_acc: 0.9096\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.32909 to 0.32735, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 22/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.3554 - acc: 0.85 - ETA: 16s - loss: 0.3417 - acc: 0.85 - ETA: 16s - loss: 0.3412 - acc: 0.85 - ETA: 16s - loss: 0.3505 - acc: 0.85 - ETA: 15s - loss: 0.3423 - acc: 0.86 - ETA: 15s - loss: 0.3309 - acc: 0.86 - ETA: 14s - loss: 0.3360 - acc: 0.86 - ETA: 13s - loss: 0.3395 - acc: 0.86 - ETA: 13s - loss: 0.3446 - acc: 0.85 - ETA: 12s - loss: 0.3552 - acc: 0.84 - ETA: 12s - loss: 0.3553 - acc: 0.85 - ETA: 11s - loss: 0.3609 - acc: 0.85 - ETA: 10s - loss: 0.3615 - acc: 0.84 - ETA: 10s - loss: 0.3646 - acc: 0.84 - ETA: 9s - loss: 0.3643 - acc: 0.8479 - ETA: 9s - loss: 0.3627 - acc: 0.847 - ETA: 8s - loss: 0.3558 - acc: 0.852 - ETA: 8s - loss: 0.3588 - acc: 0.849 - ETA: 7s - loss: 0.3550 - acc: 0.852 - ETA: 6s - loss: 0.3547 - acc: 0.851 - ETA: 6s - loss: 0.3558 - acc: 0.847 - ETA: 5s - loss: 0.3556 - acc: 0.846 - ETA: 5s - loss: 0.3552 - acc: 0.846 - ETA: 4s - loss: 0.3531 - acc: 0.848 - ETA: 4s - loss: 0.3537 - acc: 0.846 - ETA: 3s - loss: 0.3529 - acc: 0.847 - ETA: 3s - loss: 0.3528 - acc: 0.846 - ETA: 2s - loss: 0.3534 - acc: 0.847 - ETA: 1s - loss: 0.3520 - acc: 0.847 - ETA: 1s - loss: 0.3526 - acc: 0.847 - ETA: 0s - loss: 0.3513 - acc: 0.849 - ETA: 0s - loss: 0.3491 - acc: 0.849 - 20s 10ms/step - loss: 0.3479 - acc: 0.8509 - val_loss: 0.2933 - val_acc: 0.8865\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.32735 to 0.29332, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 23/100\n",
      "2076/2076 [==============================] - ETA: 19s - loss: 0.3234 - acc: 0.83 - ETA: 17s - loss: 0.3741 - acc: 0.82 - ETA: 17s - loss: 0.3296 - acc: 0.85 - ETA: 16s - loss: 0.3213 - acc: 0.86 - ETA: 16s - loss: 0.3236 - acc: 0.87 - ETA: 15s - loss: 0.3171 - acc: 0.87 - ETA: 14s - loss: 0.3228 - acc: 0.87 - ETA: 14s - loss: 0.3241 - acc: 0.87 - ETA: 13s - loss: 0.3241 - acc: 0.87 - ETA: 13s - loss: 0.3264 - acc: 0.86 - ETA: 12s - loss: 0.3292 - acc: 0.86 - ETA: 12s - loss: 0.3248 - acc: 0.87 - ETA: 11s - loss: 0.3227 - acc: 0.87 - ETA: 11s - loss: 0.3259 - acc: 0.87 - ETA: 10s - loss: 0.3250 - acc: 0.87 - ETA: 9s - loss: 0.3267 - acc: 0.8730 - ETA: 9s - loss: 0.3244 - acc: 0.876 - ETA: 8s - loss: 0.3214 - acc: 0.878 - ETA: 8s - loss: 0.3209 - acc: 0.878 - ETA: 7s - loss: 0.3237 - acc: 0.878 - ETA: 6s - loss: 0.3243 - acc: 0.877 - ETA: 6s - loss: 0.3275 - acc: 0.872 - ETA: 5s - loss: 0.3281 - acc: 0.874 - ETA: 5s - loss: 0.3300 - acc: 0.873 - ETA: 4s - loss: 0.3296 - acc: 0.874 - ETA: 3s - loss: 0.3296 - acc: 0.873 - ETA: 3s - loss: 0.3301 - acc: 0.873 - ETA: 2s - loss: 0.3290 - acc: 0.873 - ETA: 2s - loss: 0.3281 - acc: 0.873 - ETA: 1s - loss: 0.3267 - acc: 0.874 - ETA: 0s - loss: 0.3303 - acc: 0.871 - ETA: 0s - loss: 0.3298 - acc: 0.872 - 21s 10ms/step - loss: 0.3318 - acc: 0.8719 - val_loss: 0.2697 - val_acc: 0.9115\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.29332 to 0.26965, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 24/100\n",
      "2076/2076 [==============================] - ETA: 19s - loss: 0.2910 - acc: 0.92 - ETA: 20s - loss: 0.2729 - acc: 0.91 - ETA: 18s - loss: 0.2650 - acc: 0.91 - ETA: 18s - loss: 0.2740 - acc: 0.90 - ETA: 17s - loss: 0.2921 - acc: 0.90 - ETA: 17s - loss: 0.2920 - acc: 0.90 - ETA: 16s - loss: 0.2936 - acc: 0.90 - ETA: 15s - loss: 0.2920 - acc: 0.90 - ETA: 14s - loss: 0.2949 - acc: 0.90 - ETA: 14s - loss: 0.2990 - acc: 0.89 - ETA: 13s - loss: 0.2992 - acc: 0.89 - ETA: 12s - loss: 0.2957 - acc: 0.89 - ETA: 12s - loss: 0.2978 - acc: 0.89 - ETA: 11s - loss: 0.2939 - acc: 0.89 - ETA: 11s - loss: 0.2973 - acc: 0.89 - ETA: 10s - loss: 0.3028 - acc: 0.88 - ETA: 10s - loss: 0.3023 - acc: 0.88 - ETA: 9s - loss: 0.3050 - acc: 0.8867 - ETA: 8s - loss: 0.3030 - acc: 0.889 - ETA: 8s - loss: 0.3033 - acc: 0.889 - ETA: 7s - loss: 0.3064 - acc: 0.887 - ETA: 6s - loss: 0.3080 - acc: 0.887 - ETA: 6s - loss: 0.3082 - acc: 0.886 - ETA: 5s - loss: 0.3063 - acc: 0.889 - ETA: 4s - loss: 0.3070 - acc: 0.889 - ETA: 4s - loss: 0.3089 - acc: 0.887 - ETA: 3s - loss: 0.3082 - acc: 0.888 - ETA: 2s - loss: 0.3059 - acc: 0.889 - ETA: 2s - loss: 0.3069 - acc: 0.888 - ETA: 1s - loss: 0.3085 - acc: 0.887 - ETA: 0s - loss: 0.3076 - acc: 0.888 - ETA: 0s - loss: 0.3065 - acc: 0.890 - 22s 11ms/step - loss: 0.3060 - acc: 0.8907 - val_loss: 0.2513 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.26965 to 0.25132, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 25/100\n",
      "2076/2076 [==============================] - ETA: 18s - loss: 0.2663 - acc: 0.92 - ETA: 17s - loss: 0.2978 - acc: 0.89 - ETA: 16s - loss: 0.2859 - acc: 0.90 - ETA: 16s - loss: 0.2940 - acc: 0.90 - ETA: 15s - loss: 0.2989 - acc: 0.89 - ETA: 15s - loss: 0.3021 - acc: 0.89 - ETA: 14s - loss: 0.3119 - acc: 0.88 - ETA: 14s - loss: 0.3162 - acc: 0.88 - ETA: 15s - loss: 0.3118 - acc: 0.88 - ETA: 15s - loss: 0.3131 - acc: 0.88 - ETA: 14s - loss: 0.3104 - acc: 0.88 - ETA: 13s - loss: 0.3081 - acc: 0.88 - ETA: 13s - loss: 0.3057 - acc: 0.88 - ETA: 12s - loss: 0.3006 - acc: 0.88 - ETA: 11s - loss: 0.2993 - acc: 0.89 - ETA: 11s - loss: 0.2969 - acc: 0.89 - ETA: 10s - loss: 0.2978 - acc: 0.89 - ETA: 9s - loss: 0.2983 - acc: 0.8915 - ETA: 9s - loss: 0.3002 - acc: 0.893 - ETA: 8s - loss: 0.2998 - acc: 0.893 - ETA: 7s - loss: 0.2997 - acc: 0.894 - ETA: 7s - loss: 0.2976 - acc: 0.893 - ETA: 6s - loss: 0.2969 - acc: 0.893 - ETA: 5s - loss: 0.2954 - acc: 0.893 - ETA: 5s - loss: 0.2941 - acc: 0.895 - ETA: 4s - loss: 0.2951 - acc: 0.897 - ETA: 3s - loss: 0.2963 - acc: 0.895 - ETA: 3s - loss: 0.2948 - acc: 0.895 - ETA: 2s - loss: 0.2926 - acc: 0.898 - ETA: 1s - loss: 0.2926 - acc: 0.897 - ETA: 1s - loss: 0.2917 - acc: 0.897 - ETA: 0s - loss: 0.2918 - acc: 0.897 - 24s 12ms/step - loss: 0.2924 - acc: 0.8967 - val_loss: 0.2324 - val_acc: 0.9250\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.25132 to 0.23237, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2076/2076 [==============================] - ETA: 17s - loss: 0.2603 - acc: 0.91 - ETA: 17s - loss: 0.2445 - acc: 0.91 - ETA: 16s - loss: 0.2588 - acc: 0.92 - ETA: 16s - loss: 0.2530 - acc: 0.92 - ETA: 15s - loss: 0.2615 - acc: 0.91 - ETA: 15s - loss: 0.2718 - acc: 0.91 - ETA: 14s - loss: 0.2699 - acc: 0.91 - ETA: 14s - loss: 0.2722 - acc: 0.90 - ETA: 13s - loss: 0.2675 - acc: 0.91 - ETA: 12s - loss: 0.2706 - acc: 0.90 - ETA: 12s - loss: 0.2800 - acc: 0.90 - ETA: 11s - loss: 0.2780 - acc: 0.90 - ETA: 11s - loss: 0.2793 - acc: 0.90 - ETA: 10s - loss: 0.2782 - acc: 0.90 - ETA: 9s - loss: 0.2776 - acc: 0.9073 - ETA: 9s - loss: 0.2768 - acc: 0.906 - ETA: 8s - loss: 0.2774 - acc: 0.904 - ETA: 8s - loss: 0.2783 - acc: 0.902 - ETA: 7s - loss: 0.2755 - acc: 0.905 - ETA: 7s - loss: 0.2737 - acc: 0.907 - ETA: 6s - loss: 0.2704 - acc: 0.908 - ETA: 6s - loss: 0.2712 - acc: 0.908 - ETA: 5s - loss: 0.2738 - acc: 0.905 - ETA: 4s - loss: 0.2733 - acc: 0.906 - ETA: 4s - loss: 0.2719 - acc: 0.907 - ETA: 3s - loss: 0.2700 - acc: 0.909 - ETA: 3s - loss: 0.2713 - acc: 0.909 - ETA: 2s - loss: 0.2699 - acc: 0.910 - ETA: 1s - loss: 0.2668 - acc: 0.912 - ETA: 1s - loss: 0.2672 - acc: 0.911 - ETA: 0s - loss: 0.2674 - acc: 0.911 - ETA: 0s - loss: 0.2670 - acc: 0.911 - 20s 10ms/step - loss: 0.2667 - acc: 0.9116 - val_loss: 0.2146 - val_acc: 0.9317\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.23237 to 0.21464, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 27/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.2812 - acc: 0.91 - ETA: 16s - loss: 0.2583 - acc: 0.91 - ETA: 16s - loss: 0.2436 - acc: 0.92 - ETA: 16s - loss: 0.2427 - acc: 0.92 - ETA: 15s - loss: 0.2426 - acc: 0.92 - ETA: 15s - loss: 0.2501 - acc: 0.92 - ETA: 14s - loss: 0.2509 - acc: 0.92 - ETA: 13s - loss: 0.2623 - acc: 0.91 - ETA: 13s - loss: 0.2568 - acc: 0.92 - ETA: 12s - loss: 0.2540 - acc: 0.92 - ETA: 12s - loss: 0.2513 - acc: 0.92 - ETA: 11s - loss: 0.2471 - acc: 0.92 - ETA: 11s - loss: 0.2478 - acc: 0.92 - ETA: 10s - loss: 0.2479 - acc: 0.91 - ETA: 10s - loss: 0.2449 - acc: 0.92 - ETA: 9s - loss: 0.2482 - acc: 0.9209 - ETA: 8s - loss: 0.2510 - acc: 0.918 - ETA: 8s - loss: 0.2520 - acc: 0.917 - ETA: 7s - loss: 0.2514 - acc: 0.919 - ETA: 7s - loss: 0.2498 - acc: 0.919 - ETA: 6s - loss: 0.2492 - acc: 0.921 - ETA: 5s - loss: 0.2472 - acc: 0.921 - ETA: 5s - loss: 0.2468 - acc: 0.921 - ETA: 4s - loss: 0.2473 - acc: 0.922 - ETA: 4s - loss: 0.2452 - acc: 0.923 - ETA: 3s - loss: 0.2444 - acc: 0.924 - ETA: 3s - loss: 0.2459 - acc: 0.923 - ETA: 2s - loss: 0.2470 - acc: 0.921 - ETA: 2s - loss: 0.2464 - acc: 0.920 - ETA: 1s - loss: 0.2467 - acc: 0.921 - ETA: 0s - loss: 0.2472 - acc: 0.920 - ETA: 0s - loss: 0.2487 - acc: 0.919 - 21s 10ms/step - loss: 0.2479 - acc: 0.9198 - val_loss: 0.1997 - val_acc: 0.9365\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.21464 to 0.19973, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 28/100\n",
      "2076/2076 [==============================] - ETA: 18s - loss: 0.2323 - acc: 0.96 - ETA: 17s - loss: 0.2294 - acc: 0.94 - ETA: 16s - loss: 0.2196 - acc: 0.94 - ETA: 16s - loss: 0.2336 - acc: 0.93 - ETA: 15s - loss: 0.2343 - acc: 0.93 - ETA: 14s - loss: 0.2362 - acc: 0.93 - ETA: 14s - loss: 0.2501 - acc: 0.92 - ETA: 13s - loss: 0.2448 - acc: 0.92 - ETA: 13s - loss: 0.2402 - acc: 0.92 - ETA: 12s - loss: 0.2350 - acc: 0.93 - ETA: 12s - loss: 0.2410 - acc: 0.92 - ETA: 11s - loss: 0.2467 - acc: 0.92 - ETA: 11s - loss: 0.2479 - acc: 0.92 - ETA: 10s - loss: 0.2449 - acc: 0.92 - ETA: 9s - loss: 0.2462 - acc: 0.9255 - ETA: 9s - loss: 0.2442 - acc: 0.924 - ETA: 8s - loss: 0.2392 - acc: 0.926 - ETA: 8s - loss: 0.2333 - acc: 0.930 - ETA: 7s - loss: 0.2297 - acc: 0.932 - ETA: 7s - loss: 0.2298 - acc: 0.931 - ETA: 6s - loss: 0.2291 - acc: 0.931 - ETA: 5s - loss: 0.2339 - acc: 0.927 - ETA: 5s - loss: 0.2316 - acc: 0.929 - ETA: 4s - loss: 0.2279 - acc: 0.932 - ETA: 4s - loss: 0.2279 - acc: 0.932 - ETA: 3s - loss: 0.2248 - acc: 0.933 - ETA: 3s - loss: 0.2251 - acc: 0.933 - ETA: 2s - loss: 0.2224 - acc: 0.933 - ETA: 1s - loss: 0.2228 - acc: 0.932 - ETA: 1s - loss: 0.2216 - acc: 0.932 - ETA: 0s - loss: 0.2220 - acc: 0.932 - ETA: 0s - loss: 0.2217 - acc: 0.931 - 20s 10ms/step - loss: 0.2222 - acc: 0.9316 - val_loss: 0.1875 - val_acc: 0.9394\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.19973 to 0.18748, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 29/100\n",
      "2076/2076 [==============================] - ETA: 19s - loss: 0.1787 - acc: 0.96 - ETA: 18s - loss: 0.1810 - acc: 0.94 - ETA: 17s - loss: 0.2229 - acc: 0.92 - ETA: 16s - loss: 0.2276 - acc: 0.91 - ETA: 16s - loss: 0.2315 - acc: 0.91 - ETA: 15s - loss: 0.2411 - acc: 0.90 - ETA: 15s - loss: 0.2328 - acc: 0.91 - ETA: 14s - loss: 0.2263 - acc: 0.91 - ETA: 14s - loss: 0.2321 - acc: 0.91 - ETA: 13s - loss: 0.2245 - acc: 0.91 - ETA: 13s - loss: 0.2282 - acc: 0.91 - ETA: 12s - loss: 0.2233 - acc: 0.92 - ETA: 11s - loss: 0.2195 - acc: 0.92 - ETA: 11s - loss: 0.2249 - acc: 0.92 - ETA: 10s - loss: 0.2250 - acc: 0.92 - ETA: 9s - loss: 0.2276 - acc: 0.9229 - ETA: 9s - loss: 0.2244 - acc: 0.924 - ETA: 8s - loss: 0.2237 - acc: 0.924 - ETA: 7s - loss: 0.2231 - acc: 0.923 - ETA: 7s - loss: 0.2225 - acc: 0.923 - ETA: 6s - loss: 0.2237 - acc: 0.923 - ETA: 6s - loss: 0.2232 - acc: 0.924 - ETA: 5s - loss: 0.2229 - acc: 0.924 - ETA: 5s - loss: 0.2216 - acc: 0.926 - ETA: 4s - loss: 0.2184 - acc: 0.929 - ETA: 3s - loss: 0.2179 - acc: 0.929 - ETA: 3s - loss: 0.2184 - acc: 0.928 - ETA: 2s - loss: 0.2166 - acc: 0.930 - ETA: 2s - loss: 0.2151 - acc: 0.931 - ETA: 1s - loss: 0.2139 - acc: 0.932 - ETA: 0s - loss: 0.2126 - acc: 0.932 - ETA: 0s - loss: 0.2104 - acc: 0.933 - 21s 10ms/step - loss: 0.2113 - acc: 0.9326 - val_loss: 0.1624 - val_acc: 0.9481\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.18748 to 0.16239, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 30/100\n",
      "2076/2076 [==============================] - ETA: 22s - loss: 0.2123 - acc: 0.96 - ETA: 22s - loss: 0.2447 - acc: 0.92 - ETA: 21s - loss: 0.2270 - acc: 0.93 - ETA: 20s - loss: 0.2175 - acc: 0.94 - ETA: 19s - loss: 0.2112 - acc: 0.94 - ETA: 18s - loss: 0.2132 - acc: 0.94 - ETA: 18s - loss: 0.2183 - acc: 0.94 - ETA: 17s - loss: 0.2131 - acc: 0.94 - ETA: 16s - loss: 0.2118 - acc: 0.94 - ETA: 16s - loss: 0.2146 - acc: 0.93 - ETA: 16s - loss: 0.2122 - acc: 0.94 - ETA: 15s - loss: 0.2122 - acc: 0.94 - ETA: 14s - loss: 0.2161 - acc: 0.93 - ETA: 13s - loss: 0.2122 - acc: 0.94 - ETA: 12s - loss: 0.2097 - acc: 0.94 - ETA: 12s - loss: 0.2064 - acc: 0.94 - ETA: 11s - loss: 0.2027 - acc: 0.94 - ETA: 10s - loss: 0.2013 - acc: 0.94 - ETA: 10s - loss: 0.1998 - acc: 0.94 - ETA: 9s - loss: 0.1991 - acc: 0.9480 - ETA: 8s - loss: 0.1959 - acc: 0.949 - ETA: 7s - loss: 0.1969 - acc: 0.948 - ETA: 7s - loss: 0.1968 - acc: 0.947 - ETA: 6s - loss: 0.1947 - acc: 0.947 - ETA: 5s - loss: 0.1954 - acc: 0.947 - ETA: 4s - loss: 0.1966 - acc: 0.945 - ETA: 3s - loss: 0.1976 - acc: 0.945 - ETA: 3s - loss: 0.1966 - acc: 0.945 - ETA: 2s - loss: 0.1950 - acc: 0.946 - ETA: 1s - loss: 0.1941 - acc: 0.946 - ETA: 1s - loss: 0.1943 - acc: 0.947 - ETA: 0s - loss: 0.1927 - acc: 0.947 - 24s 12ms/step - loss: 0.1933 - acc: 0.9473 - val_loss: 0.1516 - val_acc: 0.9548\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.16239 to 0.15160, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2076/2076 [==============================] - ETA: 17s - loss: 0.1386 - acc: 0.96 - ETA: 17s - loss: 0.1517 - acc: 0.96 - ETA: 16s - loss: 0.1620 - acc: 0.96 - ETA: 16s - loss: 0.1548 - acc: 0.97 - ETA: 15s - loss: 0.1598 - acc: 0.97 - ETA: 15s - loss: 0.1754 - acc: 0.96 - ETA: 14s - loss: 0.1700 - acc: 0.96 - ETA: 14s - loss: 0.1662 - acc: 0.96 - ETA: 13s - loss: 0.1651 - acc: 0.96 - ETA: 13s - loss: 0.1662 - acc: 0.96 - ETA: 12s - loss: 0.1694 - acc: 0.95 - ETA: 11s - loss: 0.1679 - acc: 0.96 - ETA: 11s - loss: 0.1673 - acc: 0.95 - ETA: 10s - loss: 0.1646 - acc: 0.96 - ETA: 10s - loss: 0.1646 - acc: 0.96 - ETA: 9s - loss: 0.1672 - acc: 0.9600 - ETA: 9s - loss: 0.1658 - acc: 0.960 - ETA: 8s - loss: 0.1702 - acc: 0.959 - ETA: 7s - loss: 0.1680 - acc: 0.961 - ETA: 7s - loss: 0.1721 - acc: 0.957 - ETA: 6s - loss: 0.1745 - acc: 0.955 - ETA: 6s - loss: 0.1721 - acc: 0.957 - ETA: 5s - loss: 0.1713 - acc: 0.958 - ETA: 4s - loss: 0.1707 - acc: 0.959 - ETA: 4s - loss: 0.1703 - acc: 0.959 - ETA: 3s - loss: 0.1689 - acc: 0.960 - ETA: 3s - loss: 0.1688 - acc: 0.959 - ETA: 2s - loss: 0.1688 - acc: 0.958 - ETA: 2s - loss: 0.1707 - acc: 0.956 - ETA: 1s - loss: 0.1735 - acc: 0.955 - ETA: 0s - loss: 0.1732 - acc: 0.955 - ETA: 0s - loss: 0.1748 - acc: 0.954 - 20s 10ms/step - loss: 0.1744 - acc: 0.9547 - val_loss: 0.1362 - val_acc: 0.9606\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.15160 to 0.13624, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 32/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.2162 - acc: 0.87 - ETA: 16s - loss: 0.2092 - acc: 0.91 - ETA: 16s - loss: 0.1891 - acc: 0.92 - ETA: 16s - loss: 0.1907 - acc: 0.92 - ETA: 15s - loss: 0.1830 - acc: 0.93 - ETA: 15s - loss: 0.1826 - acc: 0.93 - ETA: 14s - loss: 0.1779 - acc: 0.94 - ETA: 13s - loss: 0.1715 - acc: 0.94 - ETA: 13s - loss: 0.1735 - acc: 0.94 - ETA: 12s - loss: 0.1757 - acc: 0.95 - ETA: 12s - loss: 0.1766 - acc: 0.94 - ETA: 11s - loss: 0.1732 - acc: 0.95 - ETA: 10s - loss: 0.1675 - acc: 0.95 - ETA: 10s - loss: 0.1679 - acc: 0.95 - ETA: 9s - loss: 0.1719 - acc: 0.9526 - ETA: 9s - loss: 0.1709 - acc: 0.951 - ETA: 8s - loss: 0.1708 - acc: 0.950 - ETA: 8s - loss: 0.1704 - acc: 0.950 - ETA: 7s - loss: 0.1705 - acc: 0.951 - ETA: 7s - loss: 0.1685 - acc: 0.952 - ETA: 6s - loss: 0.1695 - acc: 0.952 - ETA: 5s - loss: 0.1681 - acc: 0.953 - ETA: 5s - loss: 0.1678 - acc: 0.952 - ETA: 4s - loss: 0.1686 - acc: 0.952 - ETA: 4s - loss: 0.1680 - acc: 0.952 - ETA: 3s - loss: 0.1657 - acc: 0.953 - ETA: 3s - loss: 0.1642 - acc: 0.954 - ETA: 2s - loss: 0.1646 - acc: 0.954 - ETA: 1s - loss: 0.1640 - acc: 0.955 - ETA: 1s - loss: 0.1635 - acc: 0.956 - ETA: 0s - loss: 0.1618 - acc: 0.956 - ETA: 0s - loss: 0.1629 - acc: 0.956 - 21s 10ms/step - loss: 0.1633 - acc: 0.9562 - val_loss: 0.1243 - val_acc: 0.9654\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.13624 to 0.12435, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 33/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.1174 - acc: 0.97 - ETA: 18s - loss: 0.1387 - acc: 0.97 - ETA: 17s - loss: 0.1515 - acc: 0.96 - ETA: 17s - loss: 0.1438 - acc: 0.97 - ETA: 17s - loss: 0.1482 - acc: 0.96 - ETA: 16s - loss: 0.1402 - acc: 0.97 - ETA: 15s - loss: 0.1423 - acc: 0.97 - ETA: 14s - loss: 0.1420 - acc: 0.96 - ETA: 14s - loss: 0.1541 - acc: 0.96 - ETA: 13s - loss: 0.1523 - acc: 0.96 - ETA: 12s - loss: 0.1546 - acc: 0.96 - ETA: 12s - loss: 0.1549 - acc: 0.96 - ETA: 11s - loss: 0.1545 - acc: 0.96 - ETA: 10s - loss: 0.1532 - acc: 0.96 - ETA: 10s - loss: 0.1509 - acc: 0.96 - ETA: 9s - loss: 0.1463 - acc: 0.9668 - ETA: 9s - loss: 0.1461 - acc: 0.967 - ETA: 8s - loss: 0.1505 - acc: 0.962 - ETA: 8s - loss: 0.1512 - acc: 0.961 - ETA: 7s - loss: 0.1506 - acc: 0.961 - ETA: 7s - loss: 0.1518 - acc: 0.961 - ETA: 6s - loss: 0.1507 - acc: 0.961 - ETA: 5s - loss: 0.1476 - acc: 0.962 - ETA: 5s - loss: 0.1457 - acc: 0.962 - ETA: 4s - loss: 0.1481 - acc: 0.960 - ETA: 3s - loss: 0.1482 - acc: 0.960 - ETA: 3s - loss: 0.1492 - acc: 0.960 - ETA: 2s - loss: 0.1494 - acc: 0.959 - ETA: 2s - loss: 0.1477 - acc: 0.960 - ETA: 1s - loss: 0.1470 - acc: 0.960 - ETA: 0s - loss: 0.1466 - acc: 0.960 - ETA: 0s - loss: 0.1455 - acc: 0.961 - 22s 10ms/step - loss: 0.1455 - acc: 0.9612 - val_loss: 0.1262 - val_acc: 0.9644\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.12435\n",
      "Epoch 34/100\n",
      "2076/2076 [==============================] - ETA: 18s - loss: 0.0852 - acc: 1.00 - ETA: 18s - loss: 0.1077 - acc: 0.98 - ETA: 19s - loss: 0.1074 - acc: 0.98 - ETA: 19s - loss: 0.1151 - acc: 0.98 - ETA: 18s - loss: 0.1149 - acc: 0.98 - ETA: 17s - loss: 0.1154 - acc: 0.98 - ETA: 16s - loss: 0.1224 - acc: 0.98 - ETA: 15s - loss: 0.1235 - acc: 0.97 - ETA: 15s - loss: 0.1244 - acc: 0.98 - ETA: 14s - loss: 0.1256 - acc: 0.97 - ETA: 13s - loss: 0.1262 - acc: 0.97 - ETA: 12s - loss: 0.1279 - acc: 0.97 - ETA: 12s - loss: 0.1299 - acc: 0.97 - ETA: 11s - loss: 0.1312 - acc: 0.97 - ETA: 11s - loss: 0.1326 - acc: 0.97 - ETA: 10s - loss: 0.1350 - acc: 0.96 - ETA: 9s - loss: 0.1399 - acc: 0.9683 - ETA: 9s - loss: 0.1387 - acc: 0.968 - ETA: 8s - loss: 0.1376 - acc: 0.970 - ETA: 7s - loss: 0.1379 - acc: 0.969 - ETA: 7s - loss: 0.1369 - acc: 0.969 - ETA: 6s - loss: 0.1367 - acc: 0.969 - ETA: 5s - loss: 0.1368 - acc: 0.969 - ETA: 5s - loss: 0.1362 - acc: 0.970 - ETA: 4s - loss: 0.1357 - acc: 0.970 - ETA: 4s - loss: 0.1366 - acc: 0.969 - ETA: 3s - loss: 0.1360 - acc: 0.969 - ETA: 2s - loss: 0.1360 - acc: 0.968 - ETA: 2s - loss: 0.1354 - acc: 0.969 - ETA: 1s - loss: 0.1345 - acc: 0.970 - ETA: 0s - loss: 0.1344 - acc: 0.970 - ETA: 0s - loss: 0.1330 - acc: 0.970 - 22s 10ms/step - loss: 0.1326 - acc: 0.9711 - val_loss: 0.1131 - val_acc: 0.9673\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.12435 to 0.11311, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 35/100\n",
      "2076/2076 [==============================] - ETA: 30s - loss: 0.1380 - acc: 0.97 - ETA: 27s - loss: 0.1470 - acc: 0.96 - ETA: 24s - loss: 0.1420 - acc: 0.97 - ETA: 22s - loss: 0.1479 - acc: 0.97 - ETA: 20s - loss: 0.1444 - acc: 0.97 - ETA: 19s - loss: 0.1433 - acc: 0.97 - ETA: 18s - loss: 0.1389 - acc: 0.97 - ETA: 17s - loss: 0.1295 - acc: 0.97 - ETA: 16s - loss: 0.1256 - acc: 0.98 - ETA: 15s - loss: 0.1313 - acc: 0.97 - ETA: 14s - loss: 0.1328 - acc: 0.97 - ETA: 14s - loss: 0.1297 - acc: 0.97 - ETA: 13s - loss: 0.1276 - acc: 0.97 - ETA: 13s - loss: 0.1249 - acc: 0.97 - ETA: 13s - loss: 0.1251 - acc: 0.97 - ETA: 12s - loss: 0.1243 - acc: 0.97 - ETA: 11s - loss: 0.1238 - acc: 0.97 - ETA: 10s - loss: 0.1225 - acc: 0.97 - ETA: 9s - loss: 0.1219 - acc: 0.9749 - ETA: 9s - loss: 0.1185 - acc: 0.975 - ETA: 8s - loss: 0.1190 - acc: 0.975 - ETA: 7s - loss: 0.1200 - acc: 0.974 - ETA: 6s - loss: 0.1211 - acc: 0.972 - ETA: 5s - loss: 0.1197 - acc: 0.973 - ETA: 5s - loss: 0.1193 - acc: 0.973 - ETA: 4s - loss: 0.1195 - acc: 0.973 - ETA: 3s - loss: 0.1187 - acc: 0.974 - ETA: 3s - loss: 0.1186 - acc: 0.972 - ETA: 2s - loss: 0.1179 - acc: 0.973 - ETA: 1s - loss: 0.1198 - acc: 0.972 - ETA: 1s - loss: 0.1193 - acc: 0.971 - ETA: 0s - loss: 0.1185 - acc: 0.972 - 26s 12ms/step - loss: 0.1179 - acc: 0.9725 - val_loss: 0.0979 - val_acc: 0.9788\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.11311 to 0.09788, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2076/2076 [==============================] - ETA: 20s - loss: 0.1024 - acc: 0.98 - ETA: 19s - loss: 0.0820 - acc: 0.99 - ETA: 18s - loss: 0.0839 - acc: 0.98 - ETA: 18s - loss: 0.0829 - acc: 0.99 - ETA: 17s - loss: 0.0875 - acc: 0.99 - ETA: 16s - loss: 0.0833 - acc: 0.99 - ETA: 15s - loss: 0.0973 - acc: 0.98 - ETA: 15s - loss: 0.1001 - acc: 0.98 - ETA: 14s - loss: 0.0985 - acc: 0.98 - ETA: 13s - loss: 0.1000 - acc: 0.98 - ETA: 13s - loss: 0.1028 - acc: 0.98 - ETA: 12s - loss: 0.1055 - acc: 0.97 - ETA: 11s - loss: 0.1069 - acc: 0.97 - ETA: 11s - loss: 0.1058 - acc: 0.97 - ETA: 10s - loss: 0.1032 - acc: 0.98 - ETA: 10s - loss: 0.1024 - acc: 0.98 - ETA: 9s - loss: 0.1006 - acc: 0.9821 - ETA: 8s - loss: 0.1015 - acc: 0.982 - ETA: 8s - loss: 0.0999 - acc: 0.983 - ETA: 7s - loss: 0.1022 - acc: 0.982 - ETA: 6s - loss: 0.1073 - acc: 0.979 - ETA: 6s - loss: 0.1079 - acc: 0.978 - ETA: 5s - loss: 0.1081 - acc: 0.978 - ETA: 5s - loss: 0.1103 - acc: 0.976 - ETA: 4s - loss: 0.1098 - acc: 0.976 - ETA: 3s - loss: 0.1100 - acc: 0.976 - ETA: 3s - loss: 0.1108 - acc: 0.975 - ETA: 2s - loss: 0.1116 - acc: 0.974 - ETA: 2s - loss: 0.1112 - acc: 0.975 - ETA: 1s - loss: 0.1100 - acc: 0.976 - ETA: 0s - loss: 0.1091 - acc: 0.976 - ETA: 0s - loss: 0.1079 - acc: 0.977 - 21s 10ms/step - loss: 0.1076 - acc: 0.9774 - val_loss: 0.0929 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.09788 to 0.09287, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 37/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.1264 - acc: 0.98 - ETA: 17s - loss: 0.1232 - acc: 0.97 - ETA: 16s - loss: 0.1229 - acc: 0.97 - ETA: 15s - loss: 0.1167 - acc: 0.97 - ETA: 15s - loss: 0.1170 - acc: 0.97 - ETA: 14s - loss: 0.1129 - acc: 0.97 - ETA: 14s - loss: 0.1062 - acc: 0.98 - ETA: 13s - loss: 0.1026 - acc: 0.98 - ETA: 12s - loss: 0.1033 - acc: 0.98 - ETA: 12s - loss: 0.1009 - acc: 0.98 - ETA: 11s - loss: 0.1006 - acc: 0.98 - ETA: 11s - loss: 0.1043 - acc: 0.97 - ETA: 10s - loss: 0.1020 - acc: 0.97 - ETA: 10s - loss: 0.1022 - acc: 0.97 - ETA: 9s - loss: 0.1011 - acc: 0.9781 - ETA: 9s - loss: 0.1018 - acc: 0.978 - ETA: 8s - loss: 0.1032 - acc: 0.978 - ETA: 8s - loss: 0.1011 - acc: 0.980 - ETA: 7s - loss: 0.0993 - acc: 0.981 - ETA: 6s - loss: 0.0999 - acc: 0.981 - ETA: 6s - loss: 0.1007 - acc: 0.981 - ETA: 5s - loss: 0.0994 - acc: 0.981 - ETA: 5s - loss: 0.0996 - acc: 0.980 - ETA: 4s - loss: 0.1005 - acc: 0.979 - ETA: 4s - loss: 0.1003 - acc: 0.980 - ETA: 3s - loss: 0.1023 - acc: 0.979 - ETA: 3s - loss: 0.1027 - acc: 0.979 - ETA: 2s - loss: 0.1013 - acc: 0.979 - ETA: 1s - loss: 0.1015 - acc: 0.979 - ETA: 1s - loss: 0.1012 - acc: 0.979 - ETA: 0s - loss: 0.1027 - acc: 0.979 - ETA: 0s - loss: 0.1050 - acc: 0.978 - 21s 10ms/step - loss: 0.1042 - acc: 0.9783 - val_loss: 0.0866 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.09287 to 0.08657, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 38/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.1082 - acc: 0.98 - ETA: 16s - loss: 0.0896 - acc: 0.98 - ETA: 16s - loss: 0.0846 - acc: 0.98 - ETA: 16s - loss: 0.0934 - acc: 0.98 - ETA: 15s - loss: 0.0853 - acc: 0.98 - ETA: 14s - loss: 0.0854 - acc: 0.98 - ETA: 14s - loss: 0.0851 - acc: 0.98 - ETA: 13s - loss: 0.0862 - acc: 0.98 - ETA: 12s - loss: 0.0864 - acc: 0.98 - ETA: 12s - loss: 0.0886 - acc: 0.98 - ETA: 11s - loss: 0.0892 - acc: 0.98 - ETA: 11s - loss: 0.0911 - acc: 0.98 - ETA: 10s - loss: 0.0894 - acc: 0.98 - ETA: 10s - loss: 0.0892 - acc: 0.98 - ETA: 9s - loss: 0.0896 - acc: 0.9807 - ETA: 9s - loss: 0.0867 - acc: 0.981 - ETA: 8s - loss: 0.0862 - acc: 0.982 - ETA: 8s - loss: 0.0855 - acc: 0.982 - ETA: 7s - loss: 0.0881 - acc: 0.980 - ETA: 6s - loss: 0.0865 - acc: 0.980 - ETA: 6s - loss: 0.0854 - acc: 0.981 - ETA: 5s - loss: 0.0865 - acc: 0.981 - ETA: 5s - loss: 0.0861 - acc: 0.981 - ETA: 4s - loss: 0.0857 - acc: 0.982 - ETA: 4s - loss: 0.0858 - acc: 0.983 - ETA: 3s - loss: 0.0866 - acc: 0.982 - ETA: 3s - loss: 0.0872 - acc: 0.982 - ETA: 2s - loss: 0.0883 - acc: 0.981 - ETA: 1s - loss: 0.0911 - acc: 0.980 - ETA: 1s - loss: 0.0929 - acc: 0.979 - ETA: 0s - loss: 0.0926 - acc: 0.979 - ETA: 0s - loss: 0.0920 - acc: 0.979 - 20s 9ms/step - loss: 0.0925 - acc: 0.9793 - val_loss: 0.0824 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.08657 to 0.08239, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 39/100\n",
      "2076/2076 [==============================] - ETA: 18s - loss: 0.0820 - acc: 0.99 - ETA: 18s - loss: 0.0881 - acc: 0.98 - ETA: 17s - loss: 0.0922 - acc: 0.98 - ETA: 16s - loss: 0.0871 - acc: 0.98 - ETA: 15s - loss: 0.0941 - acc: 0.97 - ETA: 15s - loss: 0.0915 - acc: 0.97 - ETA: 14s - loss: 0.0892 - acc: 0.98 - ETA: 14s - loss: 0.0855 - acc: 0.98 - ETA: 13s - loss: 0.0859 - acc: 0.98 - ETA: 12s - loss: 0.0877 - acc: 0.98 - ETA: 12s - loss: 0.0894 - acc: 0.98 - ETA: 11s - loss: 0.0886 - acc: 0.98 - ETA: 11s - loss: 0.0852 - acc: 0.98 - ETA: 10s - loss: 0.0844 - acc: 0.99 - ETA: 9s - loss: 0.0837 - acc: 0.9896 - ETA: 9s - loss: 0.0849 - acc: 0.987 - ETA: 8s - loss: 0.0829 - acc: 0.988 - ETA: 8s - loss: 0.0823 - acc: 0.989 - ETA: 7s - loss: 0.0836 - acc: 0.988 - ETA: 7s - loss: 0.0816 - acc: 0.989 - ETA: 6s - loss: 0.0819 - acc: 0.989 - ETA: 5s - loss: 0.0822 - acc: 0.989 - ETA: 5s - loss: 0.0854 - acc: 0.986 - ETA: 4s - loss: 0.0849 - acc: 0.987 - ETA: 4s - loss: 0.0839 - acc: 0.987 - ETA: 3s - loss: 0.0841 - acc: 0.987 - ETA: 3s - loss: 0.0849 - acc: 0.986 - ETA: 2s - loss: 0.0854 - acc: 0.986 - ETA: 1s - loss: 0.0845 - acc: 0.986 - ETA: 1s - loss: 0.0855 - acc: 0.985 - ETA: 0s - loss: 0.0863 - acc: 0.984 - ETA: 0s - loss: 0.0864 - acc: 0.984 - 20s 10ms/step - loss: 0.0861 - acc: 0.9846 - val_loss: 0.0765 - val_acc: 0.9865\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.08239 to 0.07654, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 40/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.1110 - acc: 0.98 - ETA: 16s - loss: 0.0914 - acc: 0.99 - ETA: 16s - loss: 0.0813 - acc: 0.99 - ETA: 15s - loss: 0.0887 - acc: 0.98 - ETA: 15s - loss: 0.0934 - acc: 0.98 - ETA: 16s - loss: 0.0895 - acc: 0.98 - ETA: 15s - loss: 0.0878 - acc: 0.97 - ETA: 14s - loss: 0.0873 - acc: 0.98 - ETA: 13s - loss: 0.0868 - acc: 0.98 - ETA: 13s - loss: 0.0836 - acc: 0.98 - ETA: 12s - loss: 0.0810 - acc: 0.98 - ETA: 11s - loss: 0.0841 - acc: 0.98 - ETA: 11s - loss: 0.0828 - acc: 0.98 - ETA: 10s - loss: 0.0827 - acc: 0.98 - ETA: 10s - loss: 0.0820 - acc: 0.98 - ETA: 9s - loss: 0.0828 - acc: 0.9897 - ETA: 8s - loss: 0.0817 - acc: 0.989 - ETA: 8s - loss: 0.0805 - acc: 0.989 - ETA: 8s - loss: 0.0788 - acc: 0.990 - ETA: 7s - loss: 0.0773 - acc: 0.990 - ETA: 6s - loss: 0.0776 - acc: 0.990 - ETA: 6s - loss: 0.0778 - acc: 0.990 - ETA: 5s - loss: 0.0765 - acc: 0.991 - ETA: 5s - loss: 0.0762 - acc: 0.990 - ETA: 4s - loss: 0.0766 - acc: 0.990 - ETA: 4s - loss: 0.0754 - acc: 0.990 - ETA: 3s - loss: 0.0755 - acc: 0.989 - ETA: 2s - loss: 0.0740 - acc: 0.990 - ETA: 2s - loss: 0.0732 - acc: 0.990 - ETA: 1s - loss: 0.0739 - acc: 0.989 - ETA: 0s - loss: 0.0753 - acc: 0.987 - ETA: 0s - loss: 0.0765 - acc: 0.987 - 23s 11ms/step - loss: 0.0759 - acc: 0.9877 - val_loss: 0.0728 - val_acc: 0.9846\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.07654 to 0.07279, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2076/2076 [==============================] - ETA: 20s - loss: 0.0739 - acc: 0.99 - ETA: 19s - loss: 0.0690 - acc: 0.99 - ETA: 18s - loss: 0.0664 - acc: 0.99 - ETA: 17s - loss: 0.0622 - acc: 0.99 - ETA: 16s - loss: 0.0693 - acc: 0.99 - ETA: 16s - loss: 0.0696 - acc: 0.99 - ETA: 16s - loss: 0.0660 - acc: 0.99 - ETA: 16s - loss: 0.0654 - acc: 0.99 - ETA: 16s - loss: 0.0649 - acc: 0.99 - ETA: 15s - loss: 0.0643 - acc: 0.99 - ETA: 14s - loss: 0.0682 - acc: 0.99 - ETA: 13s - loss: 0.0686 - acc: 0.98 - ETA: 13s - loss: 0.0714 - acc: 0.98 - ETA: 12s - loss: 0.0731 - acc: 0.98 - ETA: 11s - loss: 0.0717 - acc: 0.98 - ETA: 10s - loss: 0.0702 - acc: 0.98 - ETA: 9s - loss: 0.0714 - acc: 0.9867 - ETA: 9s - loss: 0.0707 - acc: 0.986 - ETA: 8s - loss: 0.0711 - acc: 0.985 - ETA: 7s - loss: 0.0700 - acc: 0.986 - ETA: 7s - loss: 0.0727 - acc: 0.985 - ETA: 6s - loss: 0.0765 - acc: 0.984 - ETA: 5s - loss: 0.0752 - acc: 0.985 - ETA: 5s - loss: 0.0760 - acc: 0.984 - ETA: 4s - loss: 0.0755 - acc: 0.984 - ETA: 3s - loss: 0.0757 - acc: 0.984 - ETA: 3s - loss: 0.0765 - acc: 0.984 - ETA: 2s - loss: 0.0751 - acc: 0.985 - ETA: 2s - loss: 0.0751 - acc: 0.985 - ETA: 1s - loss: 0.0743 - acc: 0.985 - ETA: 0s - loss: 0.0735 - acc: 0.986 - ETA: 0s - loss: 0.0730 - acc: 0.986 - 21s 10ms/step - loss: 0.0735 - acc: 0.9868 - val_loss: 0.0693 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.07279 to 0.06931, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 42/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.0491 - acc: 0.98 - ETA: 17s - loss: 0.0797 - acc: 0.97 - ETA: 17s - loss: 0.0713 - acc: 0.98 - ETA: 16s - loss: 0.0712 - acc: 0.98 - ETA: 15s - loss: 0.0762 - acc: 0.98 - ETA: 15s - loss: 0.0717 - acc: 0.99 - ETA: 14s - loss: 0.0692 - acc: 0.99 - ETA: 14s - loss: 0.0683 - acc: 0.99 - ETA: 13s - loss: 0.0651 - acc: 0.99 - ETA: 12s - loss: 0.0636 - acc: 0.99 - ETA: 12s - loss: 0.0646 - acc: 0.99 - ETA: 11s - loss: 0.0663 - acc: 0.98 - ETA: 11s - loss: 0.0680 - acc: 0.98 - ETA: 10s - loss: 0.0668 - acc: 0.98 - ETA: 9s - loss: 0.0674 - acc: 0.9891 - ETA: 9s - loss: 0.0667 - acc: 0.989 - ETA: 8s - loss: 0.0670 - acc: 0.989 - ETA: 8s - loss: 0.0678 - acc: 0.989 - ETA: 7s - loss: 0.0675 - acc: 0.990 - ETA: 7s - loss: 0.0663 - acc: 0.990 - ETA: 6s - loss: 0.0660 - acc: 0.990 - ETA: 6s - loss: 0.0656 - acc: 0.990 - ETA: 5s - loss: 0.0663 - acc: 0.990 - ETA: 4s - loss: 0.0656 - acc: 0.990 - ETA: 4s - loss: 0.0656 - acc: 0.990 - ETA: 3s - loss: 0.0662 - acc: 0.990 - ETA: 3s - loss: 0.0666 - acc: 0.990 - ETA: 2s - loss: 0.0663 - acc: 0.990 - ETA: 1s - loss: 0.0659 - acc: 0.990 - ETA: 1s - loss: 0.0653 - acc: 0.991 - ETA: 0s - loss: 0.0655 - acc: 0.990 - ETA: 0s - loss: 0.0648 - acc: 0.991 - 22s 10ms/step - loss: 0.0651 - acc: 0.9908 - val_loss: 0.0784 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.06931\n",
      "Epoch 43/100\n",
      "2076/2076 [==============================] - ETA: 20s - loss: 0.0695 - acc: 0.98 - ETA: 22s - loss: 0.0619 - acc: 0.98 - ETA: 22s - loss: 0.0946 - acc: 0.97 - ETA: 22s - loss: 0.0870 - acc: 0.97 - ETA: 21s - loss: 0.0831 - acc: 0.98 - ETA: 19s - loss: 0.0781 - acc: 0.98 - ETA: 18s - loss: 0.0757 - acc: 0.98 - ETA: 17s - loss: 0.0729 - acc: 0.98 - ETA: 16s - loss: 0.0714 - acc: 0.98 - ETA: 15s - loss: 0.0693 - acc: 0.98 - ETA: 15s - loss: 0.0698 - acc: 0.98 - ETA: 14s - loss: 0.0672 - acc: 0.98 - ETA: 13s - loss: 0.0671 - acc: 0.98 - ETA: 12s - loss: 0.0652 - acc: 0.98 - ETA: 12s - loss: 0.0646 - acc: 0.98 - ETA: 11s - loss: 0.0644 - acc: 0.98 - ETA: 11s - loss: 0.0632 - acc: 0.98 - ETA: 10s - loss: 0.0633 - acc: 0.98 - ETA: 9s - loss: 0.0637 - acc: 0.9889 - ETA: 8s - loss: 0.0621 - acc: 0.989 - ETA: 7s - loss: 0.0608 - acc: 0.990 - ETA: 7s - loss: 0.0604 - acc: 0.989 - ETA: 6s - loss: 0.0606 - acc: 0.990 - ETA: 5s - loss: 0.0602 - acc: 0.989 - ETA: 5s - loss: 0.0601 - acc: 0.990 - ETA: 4s - loss: 0.0608 - acc: 0.989 - ETA: 3s - loss: 0.0607 - acc: 0.989 - ETA: 3s - loss: 0.0608 - acc: 0.988 - ETA: 2s - loss: 0.0608 - acc: 0.989 - ETA: 1s - loss: 0.0600 - acc: 0.989 - ETA: 0s - loss: 0.0593 - acc: 0.989 - ETA: 0s - loss: 0.0592 - acc: 0.989 - 25s 12ms/step - loss: 0.0596 - acc: 0.9894 - val_loss: 0.0774 - val_acc: 0.9779\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.06931\n",
      "Epoch 44/100\n",
      "2076/2076 [==============================] - ETA: 20s - loss: 0.0831 - acc: 0.98 - ETA: 20s - loss: 0.0780 - acc: 0.98 - ETA: 19s - loss: 0.0641 - acc: 0.99 - ETA: 19s - loss: 0.0648 - acc: 0.99 - ETA: 18s - loss: 0.0605 - acc: 0.99 - ETA: 17s - loss: 0.0563 - acc: 0.99 - ETA: 16s - loss: 0.0582 - acc: 0.99 - ETA: 15s - loss: 0.0562 - acc: 0.99 - ETA: 14s - loss: 0.0590 - acc: 0.99 - ETA: 14s - loss: 0.0581 - acc: 0.99 - ETA: 13s - loss: 0.0573 - acc: 0.99 - ETA: 12s - loss: 0.0565 - acc: 0.99 - ETA: 12s - loss: 0.0581 - acc: 0.99 - ETA: 11s - loss: 0.0630 - acc: 0.99 - ETA: 11s - loss: 0.0618 - acc: 0.99 - ETA: 11s - loss: 0.0611 - acc: 0.99 - ETA: 10s - loss: 0.0612 - acc: 0.99 - ETA: 10s - loss: 0.0606 - acc: 0.99 - ETA: 9s - loss: 0.0601 - acc: 0.9930 - ETA: 8s - loss: 0.0592 - acc: 0.993 - ETA: 8s - loss: 0.0588 - acc: 0.993 - ETA: 7s - loss: 0.0585 - acc: 0.994 - ETA: 6s - loss: 0.0572 - acc: 0.994 - ETA: 5s - loss: 0.0569 - acc: 0.993 - ETA: 5s - loss: 0.0565 - acc: 0.993 - ETA: 4s - loss: 0.0563 - acc: 0.993 - ETA: 3s - loss: 0.0563 - acc: 0.993 - ETA: 3s - loss: 0.0553 - acc: 0.993 - ETA: 2s - loss: 0.0548 - acc: 0.994 - ETA: 1s - loss: 0.0564 - acc: 0.993 - ETA: 0s - loss: 0.0556 - acc: 0.993 - ETA: 0s - loss: 0.0551 - acc: 0.993 - 23s 11ms/step - loss: 0.0548 - acc: 0.9937 - val_loss: 0.0700 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.06931\n",
      "Epoch 45/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.0475 - acc: 1.00 - ETA: 17s - loss: 0.0455 - acc: 1.00 - ETA: 16s - loss: 0.0612 - acc: 0.99 - ETA: 15s - loss: 0.0603 - acc: 0.99 - ETA: 15s - loss: 0.0555 - acc: 0.99 - ETA: 14s - loss: 0.0569 - acc: 0.99 - ETA: 14s - loss: 0.0558 - acc: 0.99 - ETA: 13s - loss: 0.0558 - acc: 0.99 - ETA: 13s - loss: 0.0550 - acc: 0.99 - ETA: 12s - loss: 0.0536 - acc: 0.99 - ETA: 12s - loss: 0.0516 - acc: 0.99 - ETA: 11s - loss: 0.0502 - acc: 0.99 - ETA: 11s - loss: 0.0493 - acc: 0.99 - ETA: 10s - loss: 0.0483 - acc: 0.99 - ETA: 10s - loss: 0.0482 - acc: 0.99 - ETA: 9s - loss: 0.0499 - acc: 0.9946 - ETA: 8s - loss: 0.0500 - acc: 0.994 - ETA: 8s - loss: 0.0488 - acc: 0.995 - ETA: 7s - loss: 0.0483 - acc: 0.995 - ETA: 7s - loss: 0.0477 - acc: 0.995 - ETA: 7s - loss: 0.0468 - acc: 0.995 - ETA: 6s - loss: 0.0466 - acc: 0.995 - ETA: 6s - loss: 0.0476 - acc: 0.994 - ETA: 5s - loss: 0.0473 - acc: 0.995 - ETA: 4s - loss: 0.0478 - acc: 0.994 - ETA: 4s - loss: 0.0474 - acc: 0.994 - ETA: 3s - loss: 0.0469 - acc: 0.995 - ETA: 2s - loss: 0.0466 - acc: 0.995 - ETA: 2s - loss: 0.0475 - acc: 0.994 - ETA: 1s - loss: 0.0477 - acc: 0.995 - ETA: 1s - loss: 0.0481 - acc: 0.994 - ETA: 0s - loss: 0.0486 - acc: 0.994 - 25s 12ms/step - loss: 0.0484 - acc: 0.9945 - val_loss: 0.0590 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.06931 to 0.05899, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2076/2076 [==============================] - ETA: 18s - loss: 0.0528 - acc: 1.00 - ETA: 18s - loss: 0.0452 - acc: 1.00 - ETA: 18s - loss: 0.0461 - acc: 1.00 - ETA: 18s - loss: 0.0442 - acc: 1.00 - ETA: 17s - loss: 0.0423 - acc: 1.00 - ETA: 16s - loss: 0.0417 - acc: 1.00 - ETA: 15s - loss: 0.0434 - acc: 1.00 - ETA: 15s - loss: 0.0429 - acc: 1.00 - ETA: 14s - loss: 0.0409 - acc: 1.00 - ETA: 13s - loss: 0.0402 - acc: 1.00 - ETA: 13s - loss: 0.0415 - acc: 0.99 - ETA: 12s - loss: 0.0419 - acc: 0.99 - ETA: 11s - loss: 0.0422 - acc: 0.99 - ETA: 11s - loss: 0.0420 - acc: 0.99 - ETA: 10s - loss: 0.0414 - acc: 0.99 - ETA: 10s - loss: 0.0443 - acc: 0.99 - ETA: 9s - loss: 0.0478 - acc: 0.9945 - ETA: 8s - loss: 0.0510 - acc: 0.991 - ETA: 8s - loss: 0.0511 - acc: 0.991 - ETA: 7s - loss: 0.0513 - acc: 0.991 - ETA: 6s - loss: 0.0504 - acc: 0.991 - ETA: 6s - loss: 0.0502 - acc: 0.991 - ETA: 5s - loss: 0.0502 - acc: 0.991 - ETA: 5s - loss: 0.0495 - acc: 0.992 - ETA: 4s - loss: 0.0489 - acc: 0.992 - ETA: 4s - loss: 0.0488 - acc: 0.992 - ETA: 3s - loss: 0.0482 - acc: 0.992 - ETA: 2s - loss: 0.0478 - acc: 0.992 - ETA: 2s - loss: 0.0477 - acc: 0.992 - ETA: 1s - loss: 0.0472 - acc: 0.992 - ETA: 0s - loss: 0.0475 - acc: 0.992 - ETA: 0s - loss: 0.0468 - acc: 0.992 - 22s 11ms/step - loss: 0.0465 - acc: 0.9925 - val_loss: 0.0700 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.05899\n",
      "Epoch 47/100\n",
      "2076/2076 [==============================] - ETA: 18s - loss: 0.0307 - acc: 1.00 - ETA: 17s - loss: 0.0284 - acc: 0.99 - ETA: 16s - loss: 0.0315 - acc: 0.99 - ETA: 16s - loss: 0.0442 - acc: 0.98 - ETA: 15s - loss: 0.0421 - acc: 0.98 - ETA: 15s - loss: 0.0417 - acc: 0.99 - ETA: 15s - loss: 0.0431 - acc: 0.99 - ETA: 14s - loss: 0.0450 - acc: 0.99 - ETA: 13s - loss: 0.0423 - acc: 0.99 - ETA: 13s - loss: 0.0420 - acc: 0.99 - ETA: 12s - loss: 0.0405 - acc: 0.99 - ETA: 11s - loss: 0.0402 - acc: 0.99 - ETA: 11s - loss: 0.0398 - acc: 0.99 - ETA: 10s - loss: 0.0420 - acc: 0.99 - ETA: 10s - loss: 0.0436 - acc: 0.99 - ETA: 9s - loss: 0.0429 - acc: 0.9932 - ETA: 8s - loss: 0.0423 - acc: 0.993 - ETA: 8s - loss: 0.0423 - acc: 0.993 - ETA: 7s - loss: 0.0424 - acc: 0.993 - ETA: 7s - loss: 0.0419 - acc: 0.994 - ETA: 6s - loss: 0.0422 - acc: 0.994 - ETA: 5s - loss: 0.0416 - acc: 0.994 - ETA: 5s - loss: 0.0413 - acc: 0.994 - ETA: 4s - loss: 0.0437 - acc: 0.994 - ETA: 4s - loss: 0.0433 - acc: 0.994 - ETA: 3s - loss: 0.0433 - acc: 0.994 - ETA: 3s - loss: 0.0431 - acc: 0.994 - ETA: 2s - loss: 0.0428 - acc: 0.995 - ETA: 1s - loss: 0.0429 - acc: 0.994 - ETA: 1s - loss: 0.0429 - acc: 0.995 - ETA: 0s - loss: 0.0427 - acc: 0.995 - ETA: 0s - loss: 0.0421 - acc: 0.995 - 20s 10ms/step - loss: 0.0419 - acc: 0.9954 - val_loss: 0.0569 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.05899 to 0.05692, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 48/100\n",
      "2076/2076 [==============================] - ETA: 31s - loss: 0.0440 - acc: 1.00 - ETA: 26s - loss: 0.0373 - acc: 0.99 - ETA: 23s - loss: 0.0412 - acc: 0.98 - ETA: 21s - loss: 0.0428 - acc: 0.98 - ETA: 19s - loss: 0.0439 - acc: 0.98 - ETA: 18s - loss: 0.0449 - acc: 0.98 - ETA: 17s - loss: 0.0429 - acc: 0.99 - ETA: 16s - loss: 0.0429 - acc: 0.99 - ETA: 15s - loss: 0.0405 - acc: 0.99 - ETA: 14s - loss: 0.0430 - acc: 0.99 - ETA: 13s - loss: 0.0419 - acc: 0.99 - ETA: 12s - loss: 0.0421 - acc: 0.99 - ETA: 12s - loss: 0.0409 - acc: 0.99 - ETA: 11s - loss: 0.0412 - acc: 0.99 - ETA: 10s - loss: 0.0407 - acc: 0.99 - ETA: 10s - loss: 0.0401 - acc: 0.99 - ETA: 9s - loss: 0.0391 - acc: 0.9949 - ETA: 8s - loss: 0.0419 - acc: 0.994 - ETA: 8s - loss: 0.0417 - acc: 0.994 - ETA: 7s - loss: 0.0415 - acc: 0.994 - ETA: 6s - loss: 0.0408 - acc: 0.995 - ETA: 6s - loss: 0.0411 - acc: 0.995 - ETA: 5s - loss: 0.0413 - acc: 0.994 - ETA: 5s - loss: 0.0408 - acc: 0.995 - ETA: 4s - loss: 0.0405 - acc: 0.995 - ETA: 3s - loss: 0.0403 - acc: 0.995 - ETA: 3s - loss: 0.0400 - acc: 0.995 - ETA: 2s - loss: 0.0398 - acc: 0.995 - ETA: 2s - loss: 0.0390 - acc: 0.996 - ETA: 1s - loss: 0.0398 - acc: 0.995 - ETA: 0s - loss: 0.0390 - acc: 0.995 - ETA: 0s - loss: 0.0389 - acc: 0.995 - 21s 10ms/step - loss: 0.0386 - acc: 0.9959 - val_loss: 0.0622 - val_acc: 0.9875\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.05692\n",
      "Epoch 49/100\n",
      "2076/2076 [==============================] - ETA: 18s - loss: 0.0537 - acc: 0.98 - ETA: 17s - loss: 0.0494 - acc: 0.98 - ETA: 17s - loss: 0.0472 - acc: 0.98 - ETA: 16s - loss: 0.0532 - acc: 0.99 - ETA: 16s - loss: 0.0495 - acc: 0.99 - ETA: 15s - loss: 0.0458 - acc: 0.99 - ETA: 15s - loss: 0.0459 - acc: 0.99 - ETA: 14s - loss: 0.0482 - acc: 0.99 - ETA: 13s - loss: 0.0465 - acc: 0.99 - ETA: 13s - loss: 0.0461 - acc: 0.99 - ETA: 12s - loss: 0.0463 - acc: 0.99 - ETA: 12s - loss: 0.0450 - acc: 0.99 - ETA: 11s - loss: 0.0436 - acc: 0.99 - ETA: 10s - loss: 0.0432 - acc: 0.99 - ETA: 10s - loss: 0.0424 - acc: 0.99 - ETA: 9s - loss: 0.0421 - acc: 0.9922 - ETA: 9s - loss: 0.0414 - acc: 0.991 - ETA: 8s - loss: 0.0410 - acc: 0.992 - ETA: 8s - loss: 0.0402 - acc: 0.992 - ETA: 7s - loss: 0.0394 - acc: 0.993 - ETA: 6s - loss: 0.0390 - acc: 0.993 - ETA: 6s - loss: 0.0387 - acc: 0.993 - ETA: 5s - loss: 0.0378 - acc: 0.993 - ETA: 5s - loss: 0.0375 - acc: 0.994 - ETA: 4s - loss: 0.0370 - acc: 0.994 - ETA: 3s - loss: 0.0365 - acc: 0.994 - ETA: 3s - loss: 0.0361 - acc: 0.994 - ETA: 2s - loss: 0.0357 - acc: 0.995 - ETA: 2s - loss: 0.0352 - acc: 0.995 - ETA: 1s - loss: 0.0349 - acc: 0.995 - ETA: 0s - loss: 0.0346 - acc: 0.995 - ETA: 0s - loss: 0.0352 - acc: 0.995 - 21s 10ms/step - loss: 0.0355 - acc: 0.9947 - val_loss: 0.0531 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.05692 to 0.05310, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 50/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.0139 - acc: 1.00 - ETA: 17s - loss: 0.0249 - acc: 1.00 - ETA: 16s - loss: 0.0331 - acc: 0.99 - ETA: 16s - loss: 0.0336 - acc: 0.99 - ETA: 16s - loss: 0.0355 - acc: 0.99 - ETA: 15s - loss: 0.0349 - acc: 0.99 - ETA: 14s - loss: 0.0354 - acc: 0.99 - ETA: 14s - loss: 0.0370 - acc: 0.99 - ETA: 14s - loss: 0.0376 - acc: 0.99 - ETA: 13s - loss: 0.0377 - acc: 0.99 - ETA: 12s - loss: 0.0370 - acc: 0.99 - ETA: 12s - loss: 0.0355 - acc: 0.99 - ETA: 12s - loss: 0.0351 - acc: 0.99 - ETA: 11s - loss: 0.0340 - acc: 0.99 - ETA: 10s - loss: 0.0348 - acc: 0.99 - ETA: 10s - loss: 0.0346 - acc: 0.99 - ETA: 9s - loss: 0.0341 - acc: 0.9972 - ETA: 9s - loss: 0.0346 - acc: 0.997 - ETA: 8s - loss: 0.0339 - acc: 0.997 - ETA: 7s - loss: 0.0337 - acc: 0.997 - ETA: 7s - loss: 0.0348 - acc: 0.996 - ETA: 6s - loss: 0.0348 - acc: 0.996 - ETA: 5s - loss: 0.0352 - acc: 0.996 - ETA: 5s - loss: 0.0347 - acc: 0.996 - ETA: 4s - loss: 0.0357 - acc: 0.996 - ETA: 3s - loss: 0.0359 - acc: 0.996 - ETA: 3s - loss: 0.0360 - acc: 0.996 - ETA: 2s - loss: 0.0359 - acc: 0.996 - ETA: 2s - loss: 0.0354 - acc: 0.996 - ETA: 1s - loss: 0.0363 - acc: 0.996 - ETA: 0s - loss: 0.0361 - acc: 0.996 - ETA: 0s - loss: 0.0363 - acc: 0.995 - 22s 11ms/step - loss: 0.0359 - acc: 0.9959 - val_loss: 0.0577 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.05310\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2076/2076 [==============================] - ETA: 19s - loss: 0.0249 - acc: 1.00 - ETA: 18s - loss: 0.0186 - acc: 1.00 - ETA: 17s - loss: 0.0182 - acc: 1.00 - ETA: 17s - loss: 0.0194 - acc: 1.00 - ETA: 16s - loss: 0.0242 - acc: 0.99 - ETA: 15s - loss: 0.0282 - acc: 0.99 - ETA: 15s - loss: 0.0283 - acc: 0.99 - ETA: 14s - loss: 0.0289 - acc: 0.99 - ETA: 14s - loss: 0.0287 - acc: 0.99 - ETA: 13s - loss: 0.0288 - acc: 0.99 - ETA: 12s - loss: 0.0292 - acc: 0.99 - ETA: 12s - loss: 0.0291 - acc: 0.99 - ETA: 11s - loss: 0.0289 - acc: 0.99 - ETA: 11s - loss: 0.0310 - acc: 0.99 - ETA: 10s - loss: 0.0315 - acc: 0.99 - ETA: 10s - loss: 0.0321 - acc: 0.99 - ETA: 9s - loss: 0.0319 - acc: 0.9945 - ETA: 8s - loss: 0.0313 - acc: 0.994 - ETA: 8s - loss: 0.0316 - acc: 0.995 - ETA: 7s - loss: 0.0334 - acc: 0.994 - ETA: 7s - loss: 0.0323 - acc: 0.994 - ETA: 6s - loss: 0.0320 - acc: 0.995 - ETA: 5s - loss: 0.0325 - acc: 0.994 - ETA: 5s - loss: 0.0318 - acc: 0.994 - ETA: 4s - loss: 0.0318 - acc: 0.995 - ETA: 4s - loss: 0.0315 - acc: 0.995 - ETA: 3s - loss: 0.0314 - acc: 0.995 - ETA: 2s - loss: 0.0314 - acc: 0.995 - ETA: 2s - loss: 0.0320 - acc: 0.995 - ETA: 1s - loss: 0.0317 - acc: 0.995 - ETA: 0s - loss: 0.0315 - acc: 0.996 - ETA: 0s - loss: 0.0311 - acc: 0.996 - 22s 11ms/step - loss: 0.0309 - acc: 0.9961 - val_loss: 0.0525 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.05310 to 0.05247, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 52/100\n",
      "2076/2076 [==============================] - ETA: 21s - loss: 0.0233 - acc: 1.00 - ETA: 21s - loss: 0.0217 - acc: 1.00 - ETA: 20s - loss: 0.0221 - acc: 1.00 - ETA: 19s - loss: 0.0198 - acc: 1.00 - ETA: 19s - loss: 0.0228 - acc: 0.99 - ETA: 18s - loss: 0.0223 - acc: 0.99 - ETA: 17s - loss: 0.0224 - acc: 0.99 - ETA: 16s - loss: 0.0237 - acc: 0.99 - ETA: 15s - loss: 0.0249 - acc: 0.99 - ETA: 15s - loss: 0.0258 - acc: 0.99 - ETA: 14s - loss: 0.0263 - acc: 0.99 - ETA: 13s - loss: 0.0258 - acc: 0.99 - ETA: 13s - loss: 0.0277 - acc: 0.99 - ETA: 12s - loss: 0.0274 - acc: 0.99 - ETA: 11s - loss: 0.0276 - acc: 0.99 - ETA: 11s - loss: 0.0290 - acc: 0.99 - ETA: 10s - loss: 0.0304 - acc: 0.99 - ETA: 9s - loss: 0.0297 - acc: 0.9961 - ETA: 9s - loss: 0.0294 - acc: 0.996 - ETA: 8s - loss: 0.0296 - acc: 0.995 - ETA: 7s - loss: 0.0299 - acc: 0.995 - ETA: 7s - loss: 0.0294 - acc: 0.995 - ETA: 6s - loss: 0.0292 - acc: 0.995 - ETA: 5s - loss: 0.0288 - acc: 0.995 - ETA: 5s - loss: 0.0291 - acc: 0.995 - ETA: 4s - loss: 0.0300 - acc: 0.995 - ETA: 3s - loss: 0.0295 - acc: 0.995 - ETA: 3s - loss: 0.0296 - acc: 0.995 - ETA: 2s - loss: 0.0299 - acc: 0.995 - ETA: 1s - loss: 0.0293 - acc: 0.995 - ETA: 0s - loss: 0.0288 - acc: 0.996 - ETA: 0s - loss: 0.0289 - acc: 0.996 - 24s 12ms/step - loss: 0.0288 - acc: 0.9961 - val_loss: 0.0534 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.05247\n",
      "Epoch 53/100\n",
      "2076/2076 [==============================] - ETA: 20s - loss: 0.0312 - acc: 1.00 - ETA: 19s - loss: 0.0282 - acc: 0.99 - ETA: 19s - loss: 0.0306 - acc: 0.99 - ETA: 19s - loss: 0.0282 - acc: 0.99 - ETA: 18s - loss: 0.0264 - acc: 0.99 - ETA: 18s - loss: 0.0264 - acc: 0.99 - ETA: 17s - loss: 0.0248 - acc: 0.99 - ETA: 16s - loss: 0.0239 - acc: 0.99 - ETA: 17s - loss: 0.0230 - acc: 0.99 - ETA: 16s - loss: 0.0227 - acc: 0.99 - ETA: 15s - loss: 0.0241 - acc: 0.99 - ETA: 14s - loss: 0.0243 - acc: 0.99 - ETA: 13s - loss: 0.0251 - acc: 0.99 - ETA: 12s - loss: 0.0256 - acc: 0.99 - ETA: 11s - loss: 0.0257 - acc: 0.99 - ETA: 11s - loss: 0.0262 - acc: 0.99 - ETA: 10s - loss: 0.0270 - acc: 0.99 - ETA: 9s - loss: 0.0264 - acc: 0.9974 - ETA: 8s - loss: 0.0261 - acc: 0.997 - ETA: 8s - loss: 0.0257 - acc: 0.997 - ETA: 7s - loss: 0.0261 - acc: 0.997 - ETA: 6s - loss: 0.0280 - acc: 0.996 - ETA: 6s - loss: 0.0279 - acc: 0.996 - ETA: 5s - loss: 0.0277 - acc: 0.997 - ETA: 4s - loss: 0.0276 - acc: 0.997 - ETA: 4s - loss: 0.0279 - acc: 0.997 - ETA: 3s - loss: 0.0286 - acc: 0.996 - ETA: 2s - loss: 0.0296 - acc: 0.996 - ETA: 2s - loss: 0.0302 - acc: 0.995 - ETA: 1s - loss: 0.0299 - acc: 0.995 - ETA: 0s - loss: 0.0301 - acc: 0.996 - ETA: 0s - loss: 0.0298 - acc: 0.996 - 22s 10ms/step - loss: 0.0296 - acc: 0.9961 - val_loss: 0.0525 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.05247\n",
      "Epoch 54/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.0160 - acc: 1.00 - ETA: 17s - loss: 0.0232 - acc: 1.00 - ETA: 17s - loss: 0.0202 - acc: 1.00 - ETA: 17s - loss: 0.0207 - acc: 1.00 - ETA: 18s - loss: 0.0236 - acc: 0.99 - ETA: 17s - loss: 0.0272 - acc: 0.99 - ETA: 17s - loss: 0.0334 - acc: 0.99 - ETA: 16s - loss: 0.0323 - acc: 0.99 - ETA: 15s - loss: 0.0307 - acc: 0.99 - ETA: 14s - loss: 0.0299 - acc: 0.99 - ETA: 14s - loss: 0.0287 - acc: 0.99 - ETA: 13s - loss: 0.0276 - acc: 0.99 - ETA: 12s - loss: 0.0282 - acc: 0.99 - ETA: 12s - loss: 0.0284 - acc: 0.99 - ETA: 11s - loss: 0.0290 - acc: 0.99 - ETA: 10s - loss: 0.0281 - acc: 0.99 - ETA: 9s - loss: 0.0277 - acc: 0.9977 - ETA: 9s - loss: 0.0272 - acc: 0.997 - ETA: 8s - loss: 0.0274 - acc: 0.997 - ETA: 7s - loss: 0.0276 - acc: 0.998 - ETA: 7s - loss: 0.0278 - acc: 0.998 - ETA: 6s - loss: 0.0283 - acc: 0.997 - ETA: 5s - loss: 0.0285 - acc: 0.997 - ETA: 5s - loss: 0.0285 - acc: 0.997 - ETA: 4s - loss: 0.0285 - acc: 0.997 - ETA: 4s - loss: 0.0289 - acc: 0.997 - ETA: 3s - loss: 0.0287 - acc: 0.997 - ETA: 2s - loss: 0.0287 - acc: 0.997 - ETA: 2s - loss: 0.0284 - acc: 0.997 - ETA: 1s - loss: 0.0285 - acc: 0.997 - ETA: 0s - loss: 0.0290 - acc: 0.996 - ETA: 0s - loss: 0.0292 - acc: 0.996 - 22s 10ms/step - loss: 0.0291 - acc: 0.9969 - val_loss: 0.0540 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.05247\n",
      "Epoch 55/100\n",
      "2076/2076 [==============================] - ETA: 20s - loss: 0.0312 - acc: 1.00 - ETA: 18s - loss: 0.0247 - acc: 1.00 - ETA: 17s - loss: 0.0209 - acc: 1.00 - ETA: 16s - loss: 0.0197 - acc: 1.00 - ETA: 16s - loss: 0.0220 - acc: 0.99 - ETA: 15s - loss: 0.0217 - acc: 0.99 - ETA: 15s - loss: 0.0222 - acc: 0.99 - ETA: 14s - loss: 0.0220 - acc: 0.99 - ETA: 13s - loss: 0.0231 - acc: 0.99 - ETA: 13s - loss: 0.0247 - acc: 0.99 - ETA: 12s - loss: 0.0257 - acc: 0.99 - ETA: 12s - loss: 0.0262 - acc: 0.99 - ETA: 11s - loss: 0.0261 - acc: 0.99 - ETA: 10s - loss: 0.0256 - acc: 0.99 - ETA: 10s - loss: 0.0249 - acc: 0.99 - ETA: 9s - loss: 0.0241 - acc: 0.9976 - ETA: 9s - loss: 0.0245 - acc: 0.997 - ETA: 8s - loss: 0.0243 - acc: 0.997 - ETA: 7s - loss: 0.0244 - acc: 0.997 - ETA: 7s - loss: 0.0239 - acc: 0.997 - ETA: 6s - loss: 0.0240 - acc: 0.997 - ETA: 6s - loss: 0.0236 - acc: 0.997 - ETA: 5s - loss: 0.0261 - acc: 0.997 - ETA: 5s - loss: 0.0270 - acc: 0.997 - ETA: 4s - loss: 0.0266 - acc: 0.997 - ETA: 3s - loss: 0.0264 - acc: 0.997 - ETA: 3s - loss: 0.0259 - acc: 0.997 - ETA: 2s - loss: 0.0259 - acc: 0.997 - ETA: 2s - loss: 0.0256 - acc: 0.997 - ETA: 1s - loss: 0.0256 - acc: 0.997 - ETA: 0s - loss: 0.0253 - acc: 0.997 - ETA: 0s - loss: 0.0252 - acc: 0.997 - 21s 10ms/step - loss: 0.0252 - acc: 0.9976 - val_loss: 0.0546 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.05247\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2076/2076 [==============================] - ETA: 17s - loss: 0.0520 - acc: 0.97 - ETA: 18s - loss: 0.0369 - acc: 0.98 - ETA: 18s - loss: 0.0292 - acc: 0.99 - ETA: 17s - loss: 0.0307 - acc: 0.99 - ETA: 18s - loss: 0.0296 - acc: 0.99 - ETA: 20s - loss: 0.0264 - acc: 0.99 - ETA: 19s - loss: 0.0280 - acc: 0.99 - ETA: 17s - loss: 0.0274 - acc: 0.99 - ETA: 16s - loss: 0.0261 - acc: 0.99 - ETA: 15s - loss: 0.0262 - acc: 0.99 - ETA: 14s - loss: 0.0257 - acc: 0.99 - ETA: 13s - loss: 0.0256 - acc: 0.99 - ETA: 13s - loss: 0.0261 - acc: 0.99 - ETA: 12s - loss: 0.0256 - acc: 0.99 - ETA: 11s - loss: 0.0246 - acc: 0.99 - ETA: 11s - loss: 0.0237 - acc: 0.99 - ETA: 10s - loss: 0.0231 - acc: 0.99 - ETA: 9s - loss: 0.0231 - acc: 0.9983 - ETA: 8s - loss: 0.0226 - acc: 0.998 - ETA: 8s - loss: 0.0221 - acc: 0.998 - ETA: 7s - loss: 0.0218 - acc: 0.998 - ETA: 6s - loss: 0.0216 - acc: 0.998 - ETA: 6s - loss: 0.0220 - acc: 0.998 - ETA: 5s - loss: 0.0220 - acc: 0.998 - ETA: 4s - loss: 0.0226 - acc: 0.997 - ETA: 4s - loss: 0.0229 - acc: 0.997 - ETA: 3s - loss: 0.0229 - acc: 0.998 - ETA: 2s - loss: 0.0224 - acc: 0.998 - ETA: 2s - loss: 0.0223 - acc: 0.998 - ETA: 1s - loss: 0.0221 - acc: 0.998 - ETA: 0s - loss: 0.0227 - acc: 0.998 - ETA: 0s - loss: 0.0226 - acc: 0.998 - 23s 11ms/step - loss: 0.0238 - acc: 0.9976 - val_loss: 0.0740 - val_acc: 0.9788\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.05247\n",
      "Epoch 57/100\n",
      "2076/2076 [==============================] - ETA: 23s - loss: 0.0242 - acc: 1.00 - ETA: 27s - loss: 0.0284 - acc: 1.00 - ETA: 25s - loss: 0.0251 - acc: 1.00 - ETA: 24s - loss: 0.0290 - acc: 0.99 - ETA: 24s - loss: 0.0291 - acc: 0.99 - ETA: 24s - loss: 0.0353 - acc: 0.99 - ETA: 22s - loss: 0.0323 - acc: 0.99 - ETA: 20s - loss: 0.0300 - acc: 0.99 - ETA: 19s - loss: 0.0295 - acc: 0.99 - ETA: 18s - loss: 0.0290 - acc: 0.99 - ETA: 17s - loss: 0.0275 - acc: 0.99 - ETA: 15s - loss: 0.0288 - acc: 0.99 - ETA: 14s - loss: 0.0276 - acc: 0.99 - ETA: 13s - loss: 0.0266 - acc: 0.99 - ETA: 12s - loss: 0.0258 - acc: 0.99 - ETA: 12s - loss: 0.0254 - acc: 0.99 - ETA: 11s - loss: 0.0249 - acc: 0.99 - ETA: 10s - loss: 0.0245 - acc: 0.99 - ETA: 9s - loss: 0.0241 - acc: 0.9975 - ETA: 9s - loss: 0.0239 - acc: 0.997 - ETA: 8s - loss: 0.0237 - acc: 0.997 - ETA: 7s - loss: 0.0236 - acc: 0.997 - ETA: 7s - loss: 0.0234 - acc: 0.998 - ETA: 6s - loss: 0.0243 - acc: 0.997 - ETA: 5s - loss: 0.0250 - acc: 0.996 - ETA: 4s - loss: 0.0248 - acc: 0.997 - ETA: 3s - loss: 0.0246 - acc: 0.997 - ETA: 3s - loss: 0.0246 - acc: 0.997 - ETA: 2s - loss: 0.0249 - acc: 0.997 - ETA: 1s - loss: 0.0250 - acc: 0.996 - ETA: 1s - loss: 0.0245 - acc: 0.997 - ETA: 0s - loss: 0.0245 - acc: 0.997 - 26s 13ms/step - loss: 0.0245 - acc: 0.9971 - val_loss: 0.0515 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.05247 to 0.05154, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 58/100\n",
      "2076/2076 [==============================] - ETA: 18s - loss: 0.0133 - acc: 1.00 - ETA: 18s - loss: 0.0412 - acc: 0.99 - ETA: 17s - loss: 0.0335 - acc: 0.99 - ETA: 16s - loss: 0.0317 - acc: 0.99 - ETA: 16s - loss: 0.0286 - acc: 0.99 - ETA: 15s - loss: 0.0257 - acc: 0.99 - ETA: 14s - loss: 0.0246 - acc: 0.99 - ETA: 14s - loss: 0.0241 - acc: 0.99 - ETA: 13s - loss: 0.0231 - acc: 0.99 - ETA: 12s - loss: 0.0233 - acc: 0.99 - ETA: 12s - loss: 0.0227 - acc: 0.99 - ETA: 11s - loss: 0.0221 - acc: 0.99 - ETA: 11s - loss: 0.0221 - acc: 0.99 - ETA: 10s - loss: 0.0216 - acc: 0.99 - ETA: 10s - loss: 0.0214 - acc: 0.99 - ETA: 9s - loss: 0.0209 - acc: 0.9990 - ETA: 8s - loss: 0.0219 - acc: 0.998 - ETA: 8s - loss: 0.0236 - acc: 0.996 - ETA: 7s - loss: 0.0248 - acc: 0.995 - ETA: 7s - loss: 0.0243 - acc: 0.996 - ETA: 6s - loss: 0.0248 - acc: 0.995 - ETA: 6s - loss: 0.0255 - acc: 0.995 - ETA: 5s - loss: 0.0254 - acc: 0.995 - ETA: 5s - loss: 0.0249 - acc: 0.996 - ETA: 4s - loss: 0.0244 - acc: 0.996 - ETA: 3s - loss: 0.0246 - acc: 0.996 - ETA: 3s - loss: 0.0242 - acc: 0.996 - ETA: 2s - loss: 0.0248 - acc: 0.996 - ETA: 2s - loss: 0.0250 - acc: 0.996 - ETA: 1s - loss: 0.0250 - acc: 0.996 - ETA: 0s - loss: 0.0248 - acc: 0.996 - ETA: 0s - loss: 0.0250 - acc: 0.995 - 22s 10ms/step - loss: 0.0248 - acc: 0.9959 - val_loss: 0.0592 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.05154\n",
      "Epoch 59/100\n",
      "2076/2076 [==============================] - ETA: 20s - loss: 0.0130 - acc: 1.00 - ETA: 19s - loss: 0.0208 - acc: 0.99 - ETA: 18s - loss: 0.0204 - acc: 0.99 - ETA: 18s - loss: 0.0188 - acc: 0.99 - ETA: 21s - loss: 0.0198 - acc: 0.99 - ETA: 21s - loss: 0.0200 - acc: 0.99 - ETA: 20s - loss: 0.0198 - acc: 0.99 - ETA: 19s - loss: 0.0196 - acc: 0.99 - ETA: 18s - loss: 0.0206 - acc: 0.99 - ETA: 17s - loss: 0.0222 - acc: 0.99 - ETA: 16s - loss: 0.0218 - acc: 0.99 - ETA: 15s - loss: 0.0228 - acc: 0.99 - ETA: 14s - loss: 0.0228 - acc: 0.99 - ETA: 13s - loss: 0.0224 - acc: 0.99 - ETA: 12s - loss: 0.0235 - acc: 0.99 - ETA: 12s - loss: 0.0238 - acc: 0.99 - ETA: 11s - loss: 0.0230 - acc: 0.99 - ETA: 10s - loss: 0.0223 - acc: 0.99 - ETA: 9s - loss: 0.0219 - acc: 0.9984 - ETA: 8s - loss: 0.0217 - acc: 0.998 - ETA: 8s - loss: 0.0214 - acc: 0.998 - ETA: 7s - loss: 0.0210 - acc: 0.998 - ETA: 6s - loss: 0.0211 - acc: 0.998 - ETA: 5s - loss: 0.0211 - acc: 0.998 - ETA: 5s - loss: 0.0218 - acc: 0.998 - ETA: 4s - loss: 0.0222 - acc: 0.997 - ETA: 3s - loss: 0.0225 - acc: 0.997 - ETA: 3s - loss: 0.0222 - acc: 0.997 - ETA: 2s - loss: 0.0220 - acc: 0.997 - ETA: 1s - loss: 0.0218 - acc: 0.997 - ETA: 1s - loss: 0.0215 - acc: 0.997 - ETA: 0s - loss: 0.0216 - acc: 0.997 - 24s 12ms/step - loss: 0.0217 - acc: 0.9978 - val_loss: 0.0518 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.05154\n",
      "Epoch 60/100\n",
      "2076/2076 [==============================] - ETA: 20s - loss: 0.0206 - acc: 1.00 - ETA: 19s - loss: 0.0173 - acc: 1.00 - ETA: 19s - loss: 0.0152 - acc: 1.00 - ETA: 18s - loss: 0.0209 - acc: 0.99 - ETA: 18s - loss: 0.0189 - acc: 0.99 - ETA: 19s - loss: 0.0197 - acc: 0.99 - ETA: 19s - loss: 0.0193 - acc: 0.99 - ETA: 19s - loss: 0.0190 - acc: 0.99 - ETA: 18s - loss: 0.0192 - acc: 0.99 - ETA: 18s - loss: 0.0194 - acc: 0.99 - ETA: 17s - loss: 0.0195 - acc: 0.99 - ETA: 16s - loss: 0.0226 - acc: 0.99 - ETA: 15s - loss: 0.0217 - acc: 0.99 - ETA: 14s - loss: 0.0216 - acc: 0.99 - ETA: 13s - loss: 0.0214 - acc: 0.99 - ETA: 12s - loss: 0.0208 - acc: 0.99 - ETA: 12s - loss: 0.0204 - acc: 0.99 - ETA: 11s - loss: 0.0205 - acc: 0.99 - ETA: 10s - loss: 0.0203 - acc: 0.99 - ETA: 9s - loss: 0.0207 - acc: 0.9984 - ETA: 9s - loss: 0.0220 - acc: 0.997 - ETA: 8s - loss: 0.0225 - acc: 0.997 - ETA: 7s - loss: 0.0227 - acc: 0.997 - ETA: 6s - loss: 0.0225 - acc: 0.997 - ETA: 6s - loss: 0.0232 - acc: 0.997 - ETA: 5s - loss: 0.0232 - acc: 0.997 - ETA: 4s - loss: 0.0231 - acc: 0.997 - ETA: 3s - loss: 0.0231 - acc: 0.997 - ETA: 2s - loss: 0.0228 - acc: 0.997 - ETA: 1s - loss: 0.0224 - acc: 0.997 - ETA: 1s - loss: 0.0220 - acc: 0.997 - ETA: 0s - loss: 0.0217 - acc: 0.997 - 30s 14ms/step - loss: 0.0216 - acc: 0.9978 - val_loss: 0.0562 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.05154\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2076/2076 [==============================] - ETA: 22s - loss: 0.0133 - acc: 1.00 - ETA: 24s - loss: 0.0152 - acc: 1.00 - ETA: 22s - loss: 0.0144 - acc: 1.00 - ETA: 21s - loss: 0.0213 - acc: 0.99 - ETA: 22s - loss: 0.0200 - acc: 0.99 - ETA: 20s - loss: 0.0199 - acc: 0.99 - ETA: 19s - loss: 0.0192 - acc: 0.99 - ETA: 19s - loss: 0.0211 - acc: 0.99 - ETA: 18s - loss: 0.0225 - acc: 0.99 - ETA: 17s - loss: 0.0237 - acc: 0.99 - ETA: 16s - loss: 0.0223 - acc: 0.99 - ETA: 16s - loss: 0.0224 - acc: 0.99 - ETA: 15s - loss: 0.0226 - acc: 0.99 - ETA: 14s - loss: 0.0219 - acc: 0.99 - ETA: 13s - loss: 0.0214 - acc: 0.99 - ETA: 13s - loss: 0.0213 - acc: 0.99 - ETA: 12s - loss: 0.0207 - acc: 0.99 - ETA: 11s - loss: 0.0211 - acc: 0.99 - ETA: 10s - loss: 0.0213 - acc: 0.99 - ETA: 9s - loss: 0.0210 - acc: 0.9969 - ETA: 9s - loss: 0.0208 - acc: 0.997 - ETA: 8s - loss: 0.0203 - acc: 0.997 - ETA: 7s - loss: 0.0205 - acc: 0.997 - ETA: 6s - loss: 0.0204 - acc: 0.997 - ETA: 5s - loss: 0.0203 - acc: 0.997 - ETA: 5s - loss: 0.0203 - acc: 0.997 - ETA: 4s - loss: 0.0201 - acc: 0.997 - ETA: 3s - loss: 0.0198 - acc: 0.997 - ETA: 2s - loss: 0.0197 - acc: 0.997 - ETA: 1s - loss: 0.0197 - acc: 0.997 - ETA: 1s - loss: 0.0200 - acc: 0.997 - ETA: 0s - loss: 0.0198 - acc: 0.997 - 29s 14ms/step - loss: 0.0199 - acc: 0.9978 - val_loss: 0.0502 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.05154 to 0.05023, saving model to weight_InceptionResnetV2.hdf5\n",
      "Epoch 62/100\n",
      "2076/2076 [==============================] - ETA: 23s - loss: 0.0202 - acc: 1.00 - ETA: 23s - loss: 0.0186 - acc: 1.00 - ETA: 22s - loss: 0.0183 - acc: 1.00 - ETA: 23s - loss: 0.0181 - acc: 1.00 - ETA: 22s - loss: 0.0167 - acc: 1.00 - ETA: 21s - loss: 0.0174 - acc: 1.00 - ETA: 20s - loss: 0.0185 - acc: 1.00 - ETA: 19s - loss: 0.0178 - acc: 1.00 - ETA: 18s - loss: 0.0177 - acc: 1.00 - ETA: 17s - loss: 0.0179 - acc: 1.00 - ETA: 17s - loss: 0.0179 - acc: 1.00 - ETA: 16s - loss: 0.0176 - acc: 1.00 - ETA: 16s - loss: 0.0168 - acc: 1.00 - ETA: 15s - loss: 0.0177 - acc: 0.99 - ETA: 14s - loss: 0.0173 - acc: 0.99 - ETA: 13s - loss: 0.0168 - acc: 0.99 - ETA: 13s - loss: 0.0167 - acc: 0.99 - ETA: 12s - loss: 0.0165 - acc: 0.99 - ETA: 11s - loss: 0.0164 - acc: 0.99 - ETA: 11s - loss: 0.0162 - acc: 0.99 - ETA: 10s - loss: 0.0162 - acc: 0.99 - ETA: 9s - loss: 0.0161 - acc: 0.9993 - ETA: 8s - loss: 0.0166 - acc: 0.998 - ETA: 7s - loss: 0.0170 - acc: 0.998 - ETA: 6s - loss: 0.0174 - acc: 0.998 - ETA: 5s - loss: 0.0173 - acc: 0.998 - ETA: 4s - loss: 0.0175 - acc: 0.998 - ETA: 3s - loss: 0.0175 - acc: 0.998 - ETA: 2s - loss: 0.0174 - acc: 0.998 - ETA: 2s - loss: 0.0176 - acc: 0.998 - ETA: 1s - loss: 0.0173 - acc: 0.998 - ETA: 0s - loss: 0.0175 - acc: 0.998 - 31s 15ms/step - loss: 0.0175 - acc: 0.9986 - val_loss: 0.0512 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.05023\n",
      "Epoch 63/100\n",
      "2076/2076 [==============================] - ETA: 21s - loss: 0.0171 - acc: 1.00 - ETA: 20s - loss: 0.0161 - acc: 1.00 - ETA: 19s - loss: 0.0146 - acc: 1.00 - ETA: 19s - loss: 0.0142 - acc: 1.00 - ETA: 19s - loss: 0.0135 - acc: 1.00 - ETA: 19s - loss: 0.0145 - acc: 1.00 - ETA: 18s - loss: 0.0135 - acc: 1.00 - ETA: 18s - loss: 0.0132 - acc: 1.00 - ETA: 17s - loss: 0.0139 - acc: 1.00 - ETA: 16s - loss: 0.0135 - acc: 1.00 - ETA: 16s - loss: 0.0137 - acc: 1.00 - ETA: 15s - loss: 0.0142 - acc: 1.00 - ETA: 15s - loss: 0.0148 - acc: 0.99 - ETA: 14s - loss: 0.0144 - acc: 0.99 - ETA: 14s - loss: 0.0172 - acc: 0.99 - ETA: 13s - loss: 0.0169 - acc: 0.99 - ETA: 12s - loss: 0.0172 - acc: 0.99 - ETA: 11s - loss: 0.0170 - acc: 0.99 - ETA: 11s - loss: 0.0169 - acc: 0.99 - ETA: 10s - loss: 0.0176 - acc: 0.99 - ETA: 9s - loss: 0.0184 - acc: 0.9978 - ETA: 8s - loss: 0.0203 - acc: 0.997 - ETA: 7s - loss: 0.0199 - acc: 0.997 - ETA: 6s - loss: 0.0200 - acc: 0.997 - ETA: 6s - loss: 0.0199 - acc: 0.997 - ETA: 5s - loss: 0.0198 - acc: 0.997 - ETA: 4s - loss: 0.0199 - acc: 0.997 - ETA: 3s - loss: 0.0196 - acc: 0.997 - ETA: 2s - loss: 0.0196 - acc: 0.997 - ETA: 1s - loss: 0.0194 - acc: 0.997 - ETA: 1s - loss: 0.0197 - acc: 0.997 - ETA: 0s - loss: 0.0197 - acc: 0.997 - 28s 13ms/step - loss: 0.0195 - acc: 0.9974 - val_loss: 0.0566 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.05023\n",
      "Epoch 64/100\n",
      "2076/2076 [==============================] - ETA: 22s - loss: 0.0221 - acc: 1.00 - ETA: 21s - loss: 0.0200 - acc: 1.00 - ETA: 20s - loss: 0.0168 - acc: 1.00 - ETA: 20s - loss: 0.0184 - acc: 0.99 - ETA: 19s - loss: 0.0182 - acc: 0.99 - ETA: 18s - loss: 0.0182 - acc: 0.99 - ETA: 17s - loss: 0.0178 - acc: 0.99 - ETA: 16s - loss: 0.0186 - acc: 0.99 - ETA: 16s - loss: 0.0175 - acc: 0.99 - ETA: 15s - loss: 0.0212 - acc: 0.99 - ETA: 14s - loss: 0.0217 - acc: 0.99 - ETA: 13s - loss: 0.0215 - acc: 0.99 - ETA: 13s - loss: 0.0223 - acc: 0.99 - ETA: 13s - loss: 0.0220 - acc: 0.99 - ETA: 12s - loss: 0.0219 - acc: 0.99 - ETA: 11s - loss: 0.0221 - acc: 0.99 - ETA: 10s - loss: 0.0219 - acc: 0.99 - ETA: 10s - loss: 0.0215 - acc: 0.99 - ETA: 9s - loss: 0.0210 - acc: 0.9967 - ETA: 8s - loss: 0.0213 - acc: 0.996 - ETA: 7s - loss: 0.0205 - acc: 0.996 - ETA: 7s - loss: 0.0204 - acc: 0.996 - ETA: 6s - loss: 0.0203 - acc: 0.996 - ETA: 5s - loss: 0.0199 - acc: 0.997 - ETA: 4s - loss: 0.0197 - acc: 0.997 - ETA: 4s - loss: 0.0198 - acc: 0.997 - ETA: 3s - loss: 0.0199 - acc: 0.997 - ETA: 2s - loss: 0.0197 - acc: 0.997 - ETA: 2s - loss: 0.0193 - acc: 0.997 - ETA: 1s - loss: 0.0190 - acc: 0.997 - ETA: 0s - loss: 0.0191 - acc: 0.997 - ETA: 0s - loss: 0.0189 - acc: 0.997 - 23s 11ms/step - loss: 0.0188 - acc: 0.9978 - val_loss: 0.0544 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.05023\n",
      "Epoch 65/100\n",
      "2076/2076 [==============================] - ETA: 25s - loss: 0.0229 - acc: 1.00 - ETA: 22s - loss: 0.0192 - acc: 1.00 - ETA: 21s - loss: 0.0197 - acc: 1.00 - ETA: 20s - loss: 0.0171 - acc: 1.00 - ETA: 19s - loss: 0.0168 - acc: 1.00 - ETA: 17s - loss: 0.0155 - acc: 1.00 - ETA: 16s - loss: 0.0160 - acc: 1.00 - ETA: 16s - loss: 0.0151 - acc: 1.00 - ETA: 15s - loss: 0.0145 - acc: 1.00 - ETA: 14s - loss: 0.0143 - acc: 1.00 - ETA: 13s - loss: 0.0143 - acc: 1.00 - ETA: 12s - loss: 0.0145 - acc: 1.00 - ETA: 12s - loss: 0.0144 - acc: 1.00 - ETA: 12s - loss: 0.0145 - acc: 1.00 - ETA: 11s - loss: 0.0141 - acc: 1.00 - ETA: 11s - loss: 0.0148 - acc: 0.99 - ETA: 10s - loss: 0.0149 - acc: 0.99 - ETA: 9s - loss: 0.0150 - acc: 0.9996 - ETA: 8s - loss: 0.0162 - acc: 0.998 - ETA: 8s - loss: 0.0165 - acc: 0.998 - ETA: 7s - loss: 0.0162 - acc: 0.998 - ETA: 6s - loss: 0.0160 - acc: 0.998 - ETA: 6s - loss: 0.0161 - acc: 0.998 - ETA: 5s - loss: 0.0161 - acc: 0.998 - ETA: 4s - loss: 0.0159 - acc: 0.998 - ETA: 4s - loss: 0.0159 - acc: 0.998 - ETA: 3s - loss: 0.0156 - acc: 0.998 - ETA: 2s - loss: 0.0161 - acc: 0.998 - ETA: 2s - loss: 0.0159 - acc: 0.998 - ETA: 1s - loss: 0.0157 - acc: 0.998 - ETA: 0s - loss: 0.0159 - acc: 0.998 - ETA: 0s - loss: 0.0160 - acc: 0.998 - 22s 11ms/step - loss: 0.0158 - acc: 0.9983 - val_loss: 0.0529 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.05023\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2076/2076 [==============================] - ETA: 20s - loss: 0.0113 - acc: 1.00 - ETA: 18s - loss: 0.0126 - acc: 1.00 - ETA: 18s - loss: 0.0191 - acc: 0.99 - ETA: 17s - loss: 0.0173 - acc: 0.99 - ETA: 17s - loss: 0.0158 - acc: 0.99 - ETA: 17s - loss: 0.0159 - acc: 0.99 - ETA: 17s - loss: 0.0153 - acc: 0.99 - ETA: 16s - loss: 0.0147 - acc: 0.99 - ETA: 15s - loss: 0.0144 - acc: 0.99 - ETA: 14s - loss: 0.0147 - acc: 0.99 - ETA: 14s - loss: 0.0162 - acc: 0.99 - ETA: 13s - loss: 0.0168 - acc: 0.99 - ETA: 12s - loss: 0.0169 - acc: 0.99 - ETA: 11s - loss: 0.0171 - acc: 0.99 - ETA: 11s - loss: 0.0167 - acc: 0.99 - ETA: 10s - loss: 0.0161 - acc: 0.99 - ETA: 9s - loss: 0.0162 - acc: 0.9986 - ETA: 9s - loss: 0.0161 - acc: 0.998 - ETA: 8s - loss: 0.0161 - acc: 0.998 - ETA: 7s - loss: 0.0161 - acc: 0.998 - ETA: 7s - loss: 0.0160 - acc: 0.998 - ETA: 6s - loss: 0.0157 - acc: 0.998 - ETA: 6s - loss: 0.0157 - acc: 0.999 - ETA: 5s - loss: 0.0155 - acc: 0.999 - ETA: 5s - loss: 0.0154 - acc: 0.999 - ETA: 4s - loss: 0.0153 - acc: 0.999 - ETA: 3s - loss: 0.0151 - acc: 0.999 - ETA: 3s - loss: 0.0153 - acc: 0.999 - ETA: 2s - loss: 0.0155 - acc: 0.998 - ETA: 1s - loss: 0.0153 - acc: 0.999 - ETA: 1s - loss: 0.0153 - acc: 0.999 - ETA: 0s - loss: 0.0156 - acc: 0.999 - 25s 12ms/step - loss: 0.0156 - acc: 0.9990 - val_loss: 0.0549 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.05023\n",
      "Epoch 67/100\n",
      "2076/2076 [==============================] - ETA: 18s - loss: 0.0171 - acc: 1.00 - ETA: 17s - loss: 0.0195 - acc: 1.00 - ETA: 17s - loss: 0.0164 - acc: 1.00 - ETA: 16s - loss: 0.0145 - acc: 1.00 - ETA: 16s - loss: 0.0128 - acc: 1.00 - ETA: 15s - loss: 0.0117 - acc: 1.00 - ETA: 15s - loss: 0.0119 - acc: 1.00 - ETA: 14s - loss: 0.0120 - acc: 1.00 - ETA: 13s - loss: 0.0116 - acc: 1.00 - ETA: 13s - loss: 0.0125 - acc: 1.00 - ETA: 12s - loss: 0.0140 - acc: 1.00 - ETA: 12s - loss: 0.0140 - acc: 1.00 - ETA: 11s - loss: 0.0140 - acc: 1.00 - ETA: 10s - loss: 0.0136 - acc: 1.00 - ETA: 10s - loss: 0.0138 - acc: 0.99 - ETA: 9s - loss: 0.0137 - acc: 0.9995 - ETA: 9s - loss: 0.0133 - acc: 0.999 - ETA: 8s - loss: 0.0135 - acc: 0.999 - ETA: 7s - loss: 0.0130 - acc: 0.999 - ETA: 7s - loss: 0.0133 - acc: 0.999 - ETA: 6s - loss: 0.0135 - acc: 0.999 - ETA: 6s - loss: 0.0136 - acc: 0.999 - ETA: 5s - loss: 0.0141 - acc: 0.999 - ETA: 4s - loss: 0.0139 - acc: 0.999 - ETA: 4s - loss: 0.0140 - acc: 0.999 - ETA: 3s - loss: 0.0140 - acc: 0.999 - ETA: 3s - loss: 0.0141 - acc: 0.999 - ETA: 2s - loss: 0.0144 - acc: 0.999 - ETA: 2s - loss: 0.0144 - acc: 0.999 - ETA: 1s - loss: 0.0140 - acc: 0.999 - ETA: 0s - loss: 0.0144 - acc: 0.999 - ETA: 0s - loss: 0.0145 - acc: 0.999 - 21s 10ms/step - loss: 0.0147 - acc: 0.9995 - val_loss: 0.0521 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.05023\n",
      "Epoch 68/100\n",
      "2076/2076 [==============================] - ETA: 22s - loss: 0.0209 - acc: 0.99 - ETA: 31s - loss: 0.0158 - acc: 0.99 - ETA: 27s - loss: 0.0129 - acc: 0.99 - ETA: 24s - loss: 0.0129 - acc: 0.99 - ETA: 22s - loss: 0.0126 - acc: 0.99 - ETA: 20s - loss: 0.0130 - acc: 0.99 - ETA: 19s - loss: 0.0122 - acc: 0.99 - ETA: 19s - loss: 0.0123 - acc: 0.99 - ETA: 17s - loss: 0.0128 - acc: 0.99 - ETA: 16s - loss: 0.0126 - acc: 0.99 - ETA: 15s - loss: 0.0130 - acc: 0.99 - ETA: 14s - loss: 0.0128 - acc: 0.99 - ETA: 13s - loss: 0.0132 - acc: 0.99 - ETA: 13s - loss: 0.0129 - acc: 0.99 - ETA: 12s - loss: 0.0130 - acc: 0.99 - ETA: 12s - loss: 0.0132 - acc: 0.99 - ETA: 11s - loss: 0.0132 - acc: 0.99 - ETA: 10s - loss: 0.0131 - acc: 0.99 - ETA: 9s - loss: 0.0131 - acc: 0.9996 - ETA: 8s - loss: 0.0129 - acc: 0.999 - ETA: 8s - loss: 0.0134 - acc: 0.999 - ETA: 7s - loss: 0.0131 - acc: 0.999 - ETA: 6s - loss: 0.0127 - acc: 0.999 - ETA: 5s - loss: 0.0126 - acc: 0.999 - ETA: 5s - loss: 0.0122 - acc: 0.999 - ETA: 4s - loss: 0.0122 - acc: 0.999 - ETA: 3s - loss: 0.0119 - acc: 0.999 - ETA: 2s - loss: 0.0119 - acc: 0.999 - ETA: 2s - loss: 0.0122 - acc: 0.999 - ETA: 1s - loss: 0.0121 - acc: 0.999 - ETA: 0s - loss: 0.0121 - acc: 0.999 - ETA: 0s - loss: 0.0121 - acc: 0.999 - 23s 11ms/step - loss: 0.0126 - acc: 0.9990 - val_loss: 0.0555 - val_acc: 0.9875\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.05023\n",
      "Epoch 69/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.0135 - acc: 1.00 - ETA: 17s - loss: 0.0167 - acc: 1.00 - ETA: 16s - loss: 0.0142 - acc: 1.00 - ETA: 16s - loss: 0.0199 - acc: 0.99 - ETA: 15s - loss: 0.0200 - acc: 0.99 - ETA: 15s - loss: 0.0188 - acc: 0.99 - ETA: 14s - loss: 0.0244 - acc: 0.99 - ETA: 14s - loss: 0.0222 - acc: 0.99 - ETA: 13s - loss: 0.0225 - acc: 0.99 - ETA: 13s - loss: 0.0225 - acc: 0.99 - ETA: 12s - loss: 0.0218 - acc: 0.99 - ETA: 12s - loss: 0.0204 - acc: 0.99 - ETA: 11s - loss: 0.0209 - acc: 0.99 - ETA: 11s - loss: 0.0214 - acc: 0.99 - ETA: 10s - loss: 0.0208 - acc: 0.99 - ETA: 9s - loss: 0.0206 - acc: 0.9966 - ETA: 9s - loss: 0.0202 - acc: 0.996 - ETA: 8s - loss: 0.0196 - acc: 0.997 - ETA: 8s - loss: 0.0190 - acc: 0.997 - ETA: 7s - loss: 0.0182 - acc: 0.997 - ETA: 6s - loss: 0.0179 - acc: 0.997 - ETA: 6s - loss: 0.0174 - acc: 0.997 - ETA: 5s - loss: 0.0172 - acc: 0.997 - ETA: 5s - loss: 0.0169 - acc: 0.997 - ETA: 4s - loss: 0.0167 - acc: 0.997 - ETA: 3s - loss: 0.0163 - acc: 0.997 - ETA: 3s - loss: 0.0162 - acc: 0.998 - ETA: 2s - loss: 0.0159 - acc: 0.998 - ETA: 2s - loss: 0.0160 - acc: 0.998 - ETA: 1s - loss: 0.0156 - acc: 0.998 - ETA: 0s - loss: 0.0158 - acc: 0.998 - ETA: 0s - loss: 0.0159 - acc: 0.998 - 22s 11ms/step - loss: 0.0157 - acc: 0.9983 - val_loss: 0.0660 - val_acc: 0.9846\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.05023\n",
      "Epoch 70/100\n",
      "2076/2076 [==============================] - ETA: 22s - loss: 0.0045 - acc: 1.00 - ETA: 20s - loss: 0.0074 - acc: 1.00 - ETA: 20s - loss: 0.0114 - acc: 1.00 - ETA: 19s - loss: 0.0113 - acc: 1.00 - ETA: 18s - loss: 0.0113 - acc: 1.00 - ETA: 17s - loss: 0.0103 - acc: 1.00 - ETA: 17s - loss: 0.0113 - acc: 1.00 - ETA: 16s - loss: 0.0116 - acc: 1.00 - ETA: 16s - loss: 0.0124 - acc: 1.00 - ETA: 15s - loss: 0.0123 - acc: 1.00 - ETA: 14s - loss: 0.0118 - acc: 1.00 - ETA: 14s - loss: 0.0118 - acc: 1.00 - ETA: 13s - loss: 0.0123 - acc: 1.00 - ETA: 13s - loss: 0.0122 - acc: 1.00 - ETA: 12s - loss: 0.0129 - acc: 1.00 - ETA: 11s - loss: 0.0139 - acc: 0.99 - ETA: 10s - loss: 0.0144 - acc: 0.99 - ETA: 10s - loss: 0.0147 - acc: 0.99 - ETA: 9s - loss: 0.0148 - acc: 0.9992 - ETA: 8s - loss: 0.0164 - acc: 0.997 - ETA: 8s - loss: 0.0162 - acc: 0.997 - ETA: 7s - loss: 0.0161 - acc: 0.997 - ETA: 6s - loss: 0.0159 - acc: 0.998 - ETA: 5s - loss: 0.0159 - acc: 0.998 - ETA: 5s - loss: 0.0157 - acc: 0.998 - ETA: 4s - loss: 0.0155 - acc: 0.998 - ETA: 3s - loss: 0.0154 - acc: 0.998 - ETA: 3s - loss: 0.0152 - acc: 0.998 - ETA: 2s - loss: 0.0154 - acc: 0.998 - ETA: 1s - loss: 0.0155 - acc: 0.998 - ETA: 1s - loss: 0.0160 - acc: 0.997 - ETA: 0s - loss: 0.0158 - acc: 0.997 - 25s 12ms/step - loss: 0.0157 - acc: 0.9978 - val_loss: 0.0516 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.05023\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2076/2076 [==============================] - ETA: 21s - loss: 0.0077 - acc: 1.00 - ETA: 20s - loss: 0.0127 - acc: 0.99 - ETA: 19s - loss: 0.0186 - acc: 0.99 - ETA: 19s - loss: 0.0184 - acc: 0.99 - ETA: 18s - loss: 0.0169 - acc: 0.99 - ETA: 17s - loss: 0.0158 - acc: 0.99 - ETA: 16s - loss: 0.0162 - acc: 0.99 - ETA: 16s - loss: 0.0149 - acc: 0.99 - ETA: 15s - loss: 0.0144 - acc: 0.99 - ETA: 14s - loss: 0.0143 - acc: 0.99 - ETA: 14s - loss: 0.0143 - acc: 0.99 - ETA: 13s - loss: 0.0146 - acc: 0.99 - ETA: 12s - loss: 0.0151 - acc: 0.99 - ETA: 12s - loss: 0.0150 - acc: 0.99 - ETA: 11s - loss: 0.0145 - acc: 0.99 - ETA: 10s - loss: 0.0141 - acc: 0.99 - ETA: 10s - loss: 0.0138 - acc: 0.99 - ETA: 9s - loss: 0.0134 - acc: 0.9987 - ETA: 8s - loss: 0.0143 - acc: 0.997 - ETA: 8s - loss: 0.0148 - acc: 0.998 - ETA: 7s - loss: 0.0151 - acc: 0.998 - ETA: 6s - loss: 0.0153 - acc: 0.998 - ETA: 6s - loss: 0.0149 - acc: 0.998 - ETA: 5s - loss: 0.0149 - acc: 0.998 - ETA: 4s - loss: 0.0148 - acc: 0.998 - ETA: 4s - loss: 0.0148 - acc: 0.998 - ETA: 3s - loss: 0.0146 - acc: 0.998 - ETA: 2s - loss: 0.0146 - acc: 0.998 - ETA: 2s - loss: 0.0144 - acc: 0.998 - ETA: 1s - loss: 0.0142 - acc: 0.998 - ETA: 0s - loss: 0.0146 - acc: 0.998 - ETA: 0s - loss: 0.0145 - acc: 0.998 - 22s 11ms/step - loss: 0.0145 - acc: 0.9986 - val_loss: 0.0532 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.05023\n",
      "Epoch 72/100\n",
      "2076/2076 [==============================] - ETA: 18s - loss: 0.0048 - acc: 1.00 - ETA: 18s - loss: 0.0115 - acc: 1.00 - ETA: 17s - loss: 0.0116 - acc: 1.00 - ETA: 16s - loss: 0.0121 - acc: 1.00 - ETA: 15s - loss: 0.0130 - acc: 1.00 - ETA: 15s - loss: 0.0127 - acc: 1.00 - ETA: 14s - loss: 0.0134 - acc: 1.00 - ETA: 14s - loss: 0.0126 - acc: 1.00 - ETA: 13s - loss: 0.0130 - acc: 1.00 - ETA: 13s - loss: 0.0134 - acc: 1.00 - ETA: 12s - loss: 0.0130 - acc: 1.00 - ETA: 11s - loss: 0.0125 - acc: 1.00 - ETA: 11s - loss: 0.0123 - acc: 1.00 - ETA: 10s - loss: 0.0131 - acc: 0.99 - ETA: 10s - loss: 0.0129 - acc: 0.99 - ETA: 9s - loss: 0.0126 - acc: 0.9995 - ETA: 8s - loss: 0.0124 - acc: 0.999 - ETA: 8s - loss: 0.0128 - acc: 0.999 - ETA: 7s - loss: 0.0127 - acc: 0.999 - ETA: 7s - loss: 0.0123 - acc: 0.999 - ETA: 6s - loss: 0.0121 - acc: 0.999 - ETA: 5s - loss: 0.0120 - acc: 0.999 - ETA: 5s - loss: 0.0127 - acc: 0.999 - ETA: 4s - loss: 0.0126 - acc: 0.999 - ETA: 4s - loss: 0.0132 - acc: 0.998 - ETA: 3s - loss: 0.0131 - acc: 0.998 - ETA: 3s - loss: 0.0130 - acc: 0.998 - ETA: 2s - loss: 0.0134 - acc: 0.998 - ETA: 1s - loss: 0.0133 - acc: 0.998 - ETA: 1s - loss: 0.0130 - acc: 0.998 - ETA: 0s - loss: 0.0129 - acc: 0.998 - ETA: 0s - loss: 0.0130 - acc: 0.998 - 20s 10ms/step - loss: 0.0129 - acc: 0.9986 - val_loss: 0.0584 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.05023\n",
      "Epoch 73/100\n",
      "2076/2076 [==============================] - ETA: 19s - loss: 0.0230 - acc: 0.98 - ETA: 17s - loss: 0.0251 - acc: 0.98 - ETA: 17s - loss: 0.0189 - acc: 0.98 - ETA: 16s - loss: 0.0149 - acc: 0.99 - ETA: 16s - loss: 0.0144 - acc: 0.99 - ETA: 15s - loss: 0.0149 - acc: 0.99 - ETA: 15s - loss: 0.0145 - acc: 0.99 - ETA: 14s - loss: 0.0136 - acc: 0.99 - ETA: 13s - loss: 0.0129 - acc: 0.99 - ETA: 12s - loss: 0.0126 - acc: 0.99 - ETA: 12s - loss: 0.0123 - acc: 0.99 - ETA: 11s - loss: 0.0118 - acc: 0.99 - ETA: 11s - loss: 0.0126 - acc: 0.99 - ETA: 10s - loss: 0.0125 - acc: 0.99 - ETA: 10s - loss: 0.0124 - acc: 0.99 - ETA: 9s - loss: 0.0124 - acc: 0.9980 - ETA: 8s - loss: 0.0125 - acc: 0.998 - ETA: 8s - loss: 0.0125 - acc: 0.998 - ETA: 7s - loss: 0.0126 - acc: 0.998 - ETA: 7s - loss: 0.0127 - acc: 0.998 - ETA: 6s - loss: 0.0134 - acc: 0.998 - ETA: 6s - loss: 0.0131 - acc: 0.998 - ETA: 5s - loss: 0.0129 - acc: 0.998 - ETA: 5s - loss: 0.0133 - acc: 0.998 - ETA: 4s - loss: 0.0131 - acc: 0.998 - ETA: 3s - loss: 0.0129 - acc: 0.998 - ETA: 3s - loss: 0.0128 - acc: 0.998 - ETA: 2s - loss: 0.0126 - acc: 0.998 - ETA: 2s - loss: 0.0126 - acc: 0.998 - ETA: 1s - loss: 0.0125 - acc: 0.998 - ETA: 0s - loss: 0.0123 - acc: 0.998 - ETA: 0s - loss: 0.0122 - acc: 0.998 - 21s 10ms/step - loss: 0.0123 - acc: 0.9988 - val_loss: 0.0543 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.05023\n",
      "Epoch 74/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.0061 - acc: 1.00 - ETA: 16s - loss: 0.0084 - acc: 1.00 - ETA: 16s - loss: 0.0112 - acc: 1.00 - ETA: 16s - loss: 0.0110 - acc: 1.00 - ETA: 15s - loss: 0.0107 - acc: 1.00 - ETA: 15s - loss: 0.0109 - acc: 1.00 - ETA: 14s - loss: 0.0102 - acc: 1.00 - ETA: 14s - loss: 0.0111 - acc: 1.00 - ETA: 13s - loss: 0.0105 - acc: 1.00 - ETA: 12s - loss: 0.0105 - acc: 1.00 - ETA: 12s - loss: 0.0112 - acc: 1.00 - ETA: 11s - loss: 0.0113 - acc: 1.00 - ETA: 11s - loss: 0.0120 - acc: 1.00 - ETA: 10s - loss: 0.0120 - acc: 1.00 - ETA: 10s - loss: 0.0117 - acc: 1.00 - ETA: 9s - loss: 0.0115 - acc: 1.0000 - ETA: 8s - loss: 0.0117 - acc: 1.000 - ETA: 8s - loss: 0.0117 - acc: 1.000 - ETA: 7s - loss: 0.0117 - acc: 1.000 - ETA: 7s - loss: 0.0115 - acc: 1.000 - ETA: 6s - loss: 0.0116 - acc: 1.000 - ETA: 6s - loss: 0.0115 - acc: 1.000 - ETA: 5s - loss: 0.0112 - acc: 1.000 - ETA: 5s - loss: 0.0111 - acc: 1.000 - ETA: 4s - loss: 0.0117 - acc: 0.999 - ETA: 3s - loss: 0.0116 - acc: 0.999 - ETA: 3s - loss: 0.0114 - acc: 0.999 - ETA: 2s - loss: 0.0112 - acc: 0.999 - ETA: 2s - loss: 0.0113 - acc: 0.999 - ETA: 1s - loss: 0.0111 - acc: 0.999 - ETA: 0s - loss: 0.0112 - acc: 0.999 - ETA: 0s - loss: 0.0111 - acc: 0.999 - 21s 10ms/step - loss: 0.0111 - acc: 0.9998 - val_loss: 0.0539 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.05023\n",
      "Epoch 75/100\n",
      "2076/2076 [==============================] - ETA: 18s - loss: 0.0080 - acc: 1.00 - ETA: 17s - loss: 0.0103 - acc: 1.00 - ETA: 17s - loss: 0.0146 - acc: 0.99 - ETA: 16s - loss: 0.0155 - acc: 0.99 - ETA: 16s - loss: 0.0138 - acc: 0.99 - ETA: 15s - loss: 0.0134 - acc: 0.99 - ETA: 15s - loss: 0.0134 - acc: 0.99 - ETA: 14s - loss: 0.0130 - acc: 0.99 - ETA: 14s - loss: 0.0125 - acc: 0.99 - ETA: 13s - loss: 0.0118 - acc: 0.99 - ETA: 12s - loss: 0.0116 - acc: 0.99 - ETA: 12s - loss: 0.0114 - acc: 0.99 - ETA: 11s - loss: 0.0113 - acc: 0.99 - ETA: 10s - loss: 0.0109 - acc: 0.99 - ETA: 10s - loss: 0.0110 - acc: 0.99 - ETA: 9s - loss: 0.0117 - acc: 0.9995 - ETA: 9s - loss: 0.0113 - acc: 0.999 - ETA: 8s - loss: 0.0110 - acc: 0.999 - ETA: 7s - loss: 0.0109 - acc: 0.999 - ETA: 7s - loss: 0.0112 - acc: 0.999 - ETA: 6s - loss: 0.0119 - acc: 0.998 - ETA: 6s - loss: 0.0116 - acc: 0.998 - ETA: 5s - loss: 0.0114 - acc: 0.999 - ETA: 4s - loss: 0.0118 - acc: 0.999 - ETA: 4s - loss: 0.0116 - acc: 0.999 - ETA: 3s - loss: 0.0115 - acc: 0.999 - ETA: 3s - loss: 0.0115 - acc: 0.999 - ETA: 2s - loss: 0.0116 - acc: 0.999 - ETA: 2s - loss: 0.0118 - acc: 0.999 - ETA: 1s - loss: 0.0116 - acc: 0.999 - ETA: 0s - loss: 0.0118 - acc: 0.999 - ETA: 0s - loss: 0.0117 - acc: 0.999 - 22s 11ms/step - loss: 0.0116 - acc: 0.9993 - val_loss: 0.0527 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.05023\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2076/2076 [==============================] - ETA: 20s - loss: 0.0133 - acc: 1.00 - ETA: 19s - loss: 0.0139 - acc: 1.00 - ETA: 20s - loss: 0.0139 - acc: 1.00 - ETA: 19s - loss: 0.0134 - acc: 1.00 - ETA: 18s - loss: 0.0119 - acc: 1.00 - ETA: 18s - loss: 0.0119 - acc: 1.00 - ETA: 17s - loss: 0.0120 - acc: 1.00 - ETA: 17s - loss: 0.0129 - acc: 0.99 - ETA: 16s - loss: 0.0127 - acc: 0.99 - ETA: 16s - loss: 0.0125 - acc: 0.99 - ETA: 15s - loss: 0.0119 - acc: 0.99 - ETA: 14s - loss: 0.0115 - acc: 0.99 - ETA: 13s - loss: 0.0112 - acc: 0.99 - ETA: 13s - loss: 0.0115 - acc: 0.99 - ETA: 13s - loss: 0.0116 - acc: 0.99 - ETA: 12s - loss: 0.0112 - acc: 0.99 - ETA: 11s - loss: 0.0109 - acc: 0.99 - ETA: 11s - loss: 0.0111 - acc: 0.99 - ETA: 10s - loss: 0.0112 - acc: 0.99 - ETA: 9s - loss: 0.0111 - acc: 0.9996 - ETA: 8s - loss: 0.0115 - acc: 0.999 - ETA: 7s - loss: 0.0115 - acc: 0.999 - ETA: 6s - loss: 0.0112 - acc: 0.999 - ETA: 6s - loss: 0.0110 - acc: 0.999 - ETA: 5s - loss: 0.0110 - acc: 0.999 - ETA: 4s - loss: 0.0111 - acc: 0.999 - ETA: 3s - loss: 0.0110 - acc: 0.999 - ETA: 3s - loss: 0.0108 - acc: 0.999 - ETA: 2s - loss: 0.0105 - acc: 0.999 - ETA: 1s - loss: 0.0106 - acc: 0.999 - ETA: 1s - loss: 0.0105 - acc: 0.999 - ETA: 0s - loss: 0.0104 - acc: 0.999 - 24s 12ms/step - loss: 0.0104 - acc: 0.9998 - val_loss: 0.0568 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.05023\n",
      "Epoch 77/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.0061 - acc: 1.00 - ETA: 17s - loss: 0.0060 - acc: 1.00 - ETA: 17s - loss: 0.0078 - acc: 1.00 - ETA: 16s - loss: 0.0100 - acc: 0.99 - ETA: 15s - loss: 0.0098 - acc: 0.99 - ETA: 15s - loss: 0.0096 - acc: 0.99 - ETA: 14s - loss: 0.0099 - acc: 0.99 - ETA: 14s - loss: 0.0099 - acc: 0.99 - ETA: 13s - loss: 0.0098 - acc: 0.99 - ETA: 12s - loss: 0.0093 - acc: 0.99 - ETA: 12s - loss: 0.0097 - acc: 0.99 - ETA: 11s - loss: 0.0100 - acc: 0.99 - ETA: 11s - loss: 0.0099 - acc: 0.99 - ETA: 10s - loss: 0.0102 - acc: 0.99 - ETA: 10s - loss: 0.0099 - acc: 0.99 - ETA: 9s - loss: 0.0097 - acc: 0.9995 - ETA: 9s - loss: 0.0107 - acc: 0.998 - ETA: 8s - loss: 0.0105 - acc: 0.998 - ETA: 8s - loss: 0.0105 - acc: 0.998 - ETA: 7s - loss: 0.0110 - acc: 0.998 - ETA: 6s - loss: 0.0108 - acc: 0.998 - ETA: 6s - loss: 0.0105 - acc: 0.998 - ETA: 5s - loss: 0.0102 - acc: 0.998 - ETA: 5s - loss: 0.0101 - acc: 0.998 - ETA: 4s - loss: 0.0100 - acc: 0.998 - ETA: 3s - loss: 0.0100 - acc: 0.998 - ETA: 3s - loss: 0.0103 - acc: 0.998 - ETA: 2s - loss: 0.0101 - acc: 0.998 - ETA: 2s - loss: 0.0099 - acc: 0.998 - ETA: 1s - loss: 0.0100 - acc: 0.999 - ETA: 0s - loss: 0.0101 - acc: 0.999 - ETA: 0s - loss: 0.0099 - acc: 0.999 - 21s 10ms/step - loss: 0.0099 - acc: 0.9990 - val_loss: 0.0512 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.05023\n",
      "Epoch 78/100\n",
      "2076/2076 [==============================] - ETA: 20s - loss: 0.0313 - acc: 0.98 - ETA: 19s - loss: 0.0195 - acc: 0.99 - ETA: 18s - loss: 0.0206 - acc: 0.98 - ETA: 17s - loss: 0.0171 - acc: 0.99 - ETA: 16s - loss: 0.0155 - acc: 0.99 - ETA: 15s - loss: 0.0143 - acc: 0.99 - ETA: 14s - loss: 0.0135 - acc: 0.99 - ETA: 14s - loss: 0.0134 - acc: 0.99 - ETA: 13s - loss: 0.0126 - acc: 0.99 - ETA: 12s - loss: 0.0118 - acc: 0.99 - ETA: 12s - loss: 0.0112 - acc: 0.99 - ETA: 11s - loss: 0.0111 - acc: 0.99 - ETA: 11s - loss: 0.0106 - acc: 0.99 - ETA: 10s - loss: 0.0103 - acc: 0.99 - ETA: 10s - loss: 0.0099 - acc: 0.99 - ETA: 9s - loss: 0.0097 - acc: 0.9980 - ETA: 8s - loss: 0.0097 - acc: 0.998 - ETA: 8s - loss: 0.0095 - acc: 0.998 - ETA: 7s - loss: 0.0096 - acc: 0.998 - ETA: 7s - loss: 0.0097 - acc: 0.998 - ETA: 6s - loss: 0.0094 - acc: 0.998 - ETA: 6s - loss: 0.0094 - acc: 0.998 - ETA: 5s - loss: 0.0094 - acc: 0.998 - ETA: 4s - loss: 0.0096 - acc: 0.998 - ETA: 4s - loss: 0.0097 - acc: 0.998 - ETA: 3s - loss: 0.0098 - acc: 0.998 - ETA: 3s - loss: 0.0096 - acc: 0.998 - ETA: 2s - loss: 0.0094 - acc: 0.998 - ETA: 2s - loss: 0.0093 - acc: 0.998 - ETA: 1s - loss: 0.0092 - acc: 0.999 - ETA: 0s - loss: 0.0091 - acc: 0.999 - ETA: 0s - loss: 0.0091 - acc: 0.999 - 21s 10ms/step - loss: 0.0091 - acc: 0.9990 - val_loss: 0.0554 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.05023\n",
      "Epoch 79/100\n",
      "2076/2076 [==============================] - ETA: 19s - loss: 0.0048 - acc: 1.00 - ETA: 17s - loss: 0.0059 - acc: 1.00 - ETA: 16s - loss: 0.0073 - acc: 1.00 - ETA: 16s - loss: 0.0078 - acc: 1.00 - ETA: 15s - loss: 0.0075 - acc: 1.00 - ETA: 14s - loss: 0.0073 - acc: 1.00 - ETA: 14s - loss: 0.0080 - acc: 1.00 - ETA: 14s - loss: 0.0080 - acc: 1.00 - ETA: 13s - loss: 0.0083 - acc: 1.00 - ETA: 14s - loss: 0.0084 - acc: 1.00 - ETA: 14s - loss: 0.0084 - acc: 1.00 - ETA: 13s - loss: 0.0082 - acc: 1.00 - ETA: 13s - loss: 0.0083 - acc: 1.00 - ETA: 12s - loss: 0.0087 - acc: 1.00 - ETA: 11s - loss: 0.0090 - acc: 1.00 - ETA: 10s - loss: 0.0089 - acc: 1.00 - ETA: 10s - loss: 0.0092 - acc: 1.00 - ETA: 9s - loss: 0.0091 - acc: 1.0000 - ETA: 8s - loss: 0.0089 - acc: 1.000 - ETA: 7s - loss: 0.0088 - acc: 1.000 - ETA: 7s - loss: 0.0086 - acc: 1.000 - ETA: 6s - loss: 0.0092 - acc: 0.999 - ETA: 5s - loss: 0.0090 - acc: 0.999 - ETA: 5s - loss: 0.0089 - acc: 0.999 - ETA: 4s - loss: 0.0089 - acc: 0.999 - ETA: 4s - loss: 0.0089 - acc: 0.999 - ETA: 3s - loss: 0.0089 - acc: 0.999 - ETA: 2s - loss: 0.0088 - acc: 0.999 - ETA: 2s - loss: 0.0086 - acc: 0.999 - ETA: 1s - loss: 0.0085 - acc: 0.999 - ETA: 0s - loss: 0.0083 - acc: 0.999 - ETA: 0s - loss: 0.0083 - acc: 0.999 - 22s 10ms/step - loss: 0.0085 - acc: 0.9998 - val_loss: 0.0581 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.05023\n",
      "Epoch 80/100\n",
      "2076/2076 [==============================] - ETA: 19s - loss: 0.0184 - acc: 1.00 - ETA: 18s - loss: 0.0168 - acc: 0.99 - ETA: 17s - loss: 0.0175 - acc: 0.99 - ETA: 16s - loss: 0.0147 - acc: 0.99 - ETA: 15s - loss: 0.0133 - acc: 0.99 - ETA: 15s - loss: 0.0121 - acc: 0.99 - ETA: 14s - loss: 0.0112 - acc: 0.99 - ETA: 14s - loss: 0.0108 - acc: 0.99 - ETA: 13s - loss: 0.0107 - acc: 0.99 - ETA: 13s - loss: 0.0117 - acc: 0.99 - ETA: 12s - loss: 0.0114 - acc: 0.99 - ETA: 12s - loss: 0.0117 - acc: 0.99 - ETA: 11s - loss: 0.0136 - acc: 0.99 - ETA: 11s - loss: 0.0136 - acc: 0.99 - ETA: 10s - loss: 0.0130 - acc: 0.99 - ETA: 9s - loss: 0.0126 - acc: 0.9976 - ETA: 9s - loss: 0.0124 - acc: 0.997 - ETA: 8s - loss: 0.0119 - acc: 0.997 - ETA: 7s - loss: 0.0116 - acc: 0.997 - ETA: 7s - loss: 0.0111 - acc: 0.998 - ETA: 6s - loss: 0.0110 - acc: 0.998 - ETA: 6s - loss: 0.0107 - acc: 0.998 - ETA: 5s - loss: 0.0105 - acc: 0.998 - ETA: 4s - loss: 0.0103 - acc: 0.998 - ETA: 4s - loss: 0.0101 - acc: 0.998 - ETA: 3s - loss: 0.0100 - acc: 0.998 - ETA: 3s - loss: 0.0099 - acc: 0.998 - ETA: 2s - loss: 0.0098 - acc: 0.998 - ETA: 2s - loss: 0.0098 - acc: 0.998 - ETA: 1s - loss: 0.0098 - acc: 0.998 - ETA: 0s - loss: 0.0096 - acc: 0.998 - ETA: 0s - loss: 0.0096 - acc: 0.998 - 21s 10ms/step - loss: 0.0095 - acc: 0.9988 - val_loss: 0.0523 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.05023\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2076/2076 [==============================] - ETA: 19s - loss: 0.0057 - acc: 1.00 - ETA: 18s - loss: 0.0061 - acc: 1.00 - ETA: 17s - loss: 0.0059 - acc: 1.00 - ETA: 16s - loss: 0.0054 - acc: 1.00 - ETA: 15s - loss: 0.0054 - acc: 1.00 - ETA: 15s - loss: 0.0051 - acc: 1.00 - ETA: 14s - loss: 0.0054 - acc: 1.00 - ETA: 14s - loss: 0.0066 - acc: 1.00 - ETA: 13s - loss: 0.0063 - acc: 1.00 - ETA: 13s - loss: 0.0063 - acc: 1.00 - ETA: 12s - loss: 0.0063 - acc: 1.00 - ETA: 12s - loss: 0.0062 - acc: 1.00 - ETA: 11s - loss: 0.0061 - acc: 1.00 - ETA: 11s - loss: 0.0066 - acc: 1.00 - ETA: 10s - loss: 0.0080 - acc: 0.99 - ETA: 9s - loss: 0.0082 - acc: 0.9990 - ETA: 9s - loss: 0.0082 - acc: 0.999 - ETA: 8s - loss: 0.0079 - acc: 0.999 - ETA: 8s - loss: 0.0080 - acc: 0.999 - ETA: 7s - loss: 0.0079 - acc: 0.999 - ETA: 6s - loss: 0.0079 - acc: 0.999 - ETA: 6s - loss: 0.0078 - acc: 0.999 - ETA: 5s - loss: 0.0079 - acc: 0.999 - ETA: 5s - loss: 0.0080 - acc: 0.999 - ETA: 4s - loss: 0.0078 - acc: 0.999 - ETA: 3s - loss: 0.0079 - acc: 0.999 - ETA: 3s - loss: 0.0080 - acc: 0.999 - ETA: 2s - loss: 0.0082 - acc: 0.999 - ETA: 2s - loss: 0.0082 - acc: 0.999 - ETA: 1s - loss: 0.0084 - acc: 0.999 - ETA: 0s - loss: 0.0085 - acc: 0.999 - ETA: 0s - loss: 0.0085 - acc: 0.999 - 21s 10ms/step - loss: 0.0086 - acc: 0.9995 - val_loss: 0.0525 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.05023\n",
      "Epoch 82/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.0030 - acc: 1.00 - ETA: 18s - loss: 0.0091 - acc: 0.99 - ETA: 17s - loss: 0.0086 - acc: 0.99 - ETA: 17s - loss: 0.0080 - acc: 0.99 - ETA: 16s - loss: 0.0084 - acc: 0.99 - ETA: 17s - loss: 0.0087 - acc: 0.99 - ETA: 17s - loss: 0.0087 - acc: 0.99 - ETA: 16s - loss: 0.0087 - acc: 0.99 - ETA: 15s - loss: 0.0082 - acc: 0.99 - ETA: 15s - loss: 0.0079 - acc: 0.99 - ETA: 14s - loss: 0.0076 - acc: 0.99 - ETA: 13s - loss: 0.0075 - acc: 0.99 - ETA: 12s - loss: 0.0073 - acc: 0.99 - ETA: 11s - loss: 0.0073 - acc: 0.99 - ETA: 11s - loss: 0.0071 - acc: 0.99 - ETA: 10s - loss: 0.0081 - acc: 0.99 - ETA: 9s - loss: 0.0082 - acc: 0.9991 - ETA: 9s - loss: 0.0082 - acc: 0.999 - ETA: 8s - loss: 0.0081 - acc: 0.999 - ETA: 7s - loss: 0.0080 - acc: 0.999 - ETA: 7s - loss: 0.0078 - acc: 0.999 - ETA: 6s - loss: 0.0078 - acc: 0.999 - ETA: 6s - loss: 0.0077 - acc: 0.999 - ETA: 5s - loss: 0.0079 - acc: 0.999 - ETA: 4s - loss: 0.0078 - acc: 0.999 - ETA: 4s - loss: 0.0076 - acc: 0.999 - ETA: 3s - loss: 0.0076 - acc: 0.999 - ETA: 2s - loss: 0.0076 - acc: 0.999 - ETA: 2s - loss: 0.0076 - acc: 0.999 - ETA: 1s - loss: 0.0077 - acc: 0.999 - ETA: 0s - loss: 0.0075 - acc: 0.999 - ETA: 0s - loss: 0.0075 - acc: 0.999 - 23s 11ms/step - loss: 0.0079 - acc: 0.9993 - val_loss: 0.0532 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.05023\n",
      "Epoch 83/100\n",
      "2076/2076 [==============================] - ETA: 18s - loss: 0.0057 - acc: 1.00 - ETA: 17s - loss: 0.0089 - acc: 1.00 - ETA: 17s - loss: 0.0099 - acc: 1.00 - ETA: 16s - loss: 0.0083 - acc: 1.00 - ETA: 16s - loss: 0.0075 - acc: 1.00 - ETA: 15s - loss: 0.0089 - acc: 1.00 - ETA: 15s - loss: 0.0084 - acc: 1.00 - ETA: 14s - loss: 0.0083 - acc: 1.00 - ETA: 14s - loss: 0.0078 - acc: 1.00 - ETA: 13s - loss: 0.0074 - acc: 1.00 - ETA: 13s - loss: 0.0088 - acc: 1.00 - ETA: 12s - loss: 0.0085 - acc: 1.00 - ETA: 11s - loss: 0.0081 - acc: 1.00 - ETA: 11s - loss: 0.0078 - acc: 1.00 - ETA: 10s - loss: 0.0080 - acc: 1.00 - ETA: 9s - loss: 0.0080 - acc: 1.0000 - ETA: 9s - loss: 0.0080 - acc: 1.000 - ETA: 8s - loss: 0.0079 - acc: 1.000 - ETA: 8s - loss: 0.0077 - acc: 1.000 - ETA: 7s - loss: 0.0077 - acc: 1.000 - ETA: 7s - loss: 0.0078 - acc: 1.000 - ETA: 6s - loss: 0.0083 - acc: 0.999 - ETA: 6s - loss: 0.0081 - acc: 0.999 - ETA: 5s - loss: 0.0080 - acc: 0.999 - ETA: 4s - loss: 0.0079 - acc: 0.999 - ETA: 4s - loss: 0.0080 - acc: 0.999 - ETA: 3s - loss: 0.0080 - acc: 0.999 - ETA: 3s - loss: 0.0081 - acc: 0.999 - ETA: 2s - loss: 0.0081 - acc: 0.999 - ETA: 1s - loss: 0.0081 - acc: 0.999 - ETA: 0s - loss: 0.0081 - acc: 0.999 - ETA: 0s - loss: 0.0081 - acc: 0.999 - 24s 12ms/step - loss: 0.0082 - acc: 0.9998 - val_loss: 0.0543 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.05023\n",
      "Epoch 84/100\n",
      "2076/2076 [==============================] - ETA: 26s - loss: 0.0101 - acc: 1.00 - ETA: 28s - loss: 0.0152 - acc: 0.99 - ETA: 25s - loss: 0.0121 - acc: 0.99 - ETA: 22s - loss: 0.0097 - acc: 0.99 - ETA: 20s - loss: 0.0083 - acc: 0.99 - ETA: 19s - loss: 0.0082 - acc: 0.99 - ETA: 19s - loss: 0.0081 - acc: 0.99 - ETA: 18s - loss: 0.0094 - acc: 0.99 - ETA: 17s - loss: 0.0090 - acc: 0.99 - ETA: 16s - loss: 0.0086 - acc: 0.99 - ETA: 15s - loss: 0.0111 - acc: 0.99 - ETA: 14s - loss: 0.0106 - acc: 0.99 - ETA: 13s - loss: 0.0102 - acc: 0.99 - ETA: 13s - loss: 0.0097 - acc: 0.99 - ETA: 12s - loss: 0.0094 - acc: 0.99 - ETA: 11s - loss: 0.0094 - acc: 0.99 - ETA: 10s - loss: 0.0094 - acc: 0.99 - ETA: 9s - loss: 0.0094 - acc: 0.9983 - ETA: 9s - loss: 0.0094 - acc: 0.998 - ETA: 8s - loss: 0.0095 - acc: 0.998 - ETA: 7s - loss: 0.0097 - acc: 0.998 - ETA: 6s - loss: 0.0098 - acc: 0.998 - ETA: 6s - loss: 0.0133 - acc: 0.998 - ETA: 5s - loss: 0.0140 - acc: 0.997 - ETA: 4s - loss: 0.0137 - acc: 0.997 - ETA: 4s - loss: 0.0139 - acc: 0.997 - ETA: 3s - loss: 0.0138 - acc: 0.997 - ETA: 2s - loss: 0.0136 - acc: 0.997 - ETA: 2s - loss: 0.0132 - acc: 0.997 - ETA: 1s - loss: 0.0132 - acc: 0.997 - ETA: 0s - loss: 0.0131 - acc: 0.998 - ETA: 0s - loss: 0.0129 - acc: 0.998 - 23s 11ms/step - loss: 0.0130 - acc: 0.9981 - val_loss: 0.0547 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.05023\n",
      "Epoch 85/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.0093 - acc: 1.00 - ETA: 17s - loss: 0.0068 - acc: 1.00 - ETA: 16s - loss: 0.0066 - acc: 1.00 - ETA: 15s - loss: 0.0075 - acc: 1.00 - ETA: 15s - loss: 0.0080 - acc: 1.00 - ETA: 14s - loss: 0.0070 - acc: 1.00 - ETA: 14s - loss: 0.0070 - acc: 1.00 - ETA: 13s - loss: 0.0066 - acc: 1.00 - ETA: 13s - loss: 0.0068 - acc: 1.00 - ETA: 12s - loss: 0.0073 - acc: 1.00 - ETA: 12s - loss: 0.0070 - acc: 1.00 - ETA: 11s - loss: 0.0073 - acc: 1.00 - ETA: 11s - loss: 0.0072 - acc: 1.00 - ETA: 10s - loss: 0.0073 - acc: 1.00 - ETA: 9s - loss: 0.0073 - acc: 1.0000 - ETA: 9s - loss: 0.0073 - acc: 1.000 - ETA: 8s - loss: 0.0072 - acc: 1.000 - ETA: 8s - loss: 0.0071 - acc: 1.000 - ETA: 7s - loss: 0.0071 - acc: 1.000 - ETA: 7s - loss: 0.0071 - acc: 1.000 - ETA: 6s - loss: 0.0073 - acc: 1.000 - ETA: 5s - loss: 0.0071 - acc: 1.000 - ETA: 5s - loss: 0.0077 - acc: 0.999 - ETA: 4s - loss: 0.0079 - acc: 0.999 - ETA: 4s - loss: 0.0079 - acc: 0.999 - ETA: 3s - loss: 0.0080 - acc: 0.999 - ETA: 3s - loss: 0.0079 - acc: 0.999 - ETA: 2s - loss: 0.0079 - acc: 0.999 - ETA: 2s - loss: 0.0078 - acc: 0.999 - ETA: 1s - loss: 0.0078 - acc: 0.999 - ETA: 0s - loss: 0.0077 - acc: 0.999 - ETA: 0s - loss: 0.0076 - acc: 0.999 - 21s 10ms/step - loss: 0.0077 - acc: 0.9998 - val_loss: 0.0539 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.05023\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2076/2076 [==============================] - ETA: 18s - loss: 0.0054 - acc: 1.00 - ETA: 20s - loss: 0.0047 - acc: 1.00 - ETA: 20s - loss: 0.0075 - acc: 1.00 - ETA: 19s - loss: 0.0079 - acc: 1.00 - ETA: 18s - loss: 0.0073 - acc: 1.00 - ETA: 17s - loss: 0.0073 - acc: 1.00 - ETA: 16s - loss: 0.0075 - acc: 1.00 - ETA: 15s - loss: 0.0080 - acc: 1.00 - ETA: 15s - loss: 0.0075 - acc: 1.00 - ETA: 14s - loss: 0.0071 - acc: 1.00 - ETA: 13s - loss: 0.0074 - acc: 1.00 - ETA: 13s - loss: 0.0071 - acc: 1.00 - ETA: 12s - loss: 0.0069 - acc: 1.00 - ETA: 11s - loss: 0.0066 - acc: 1.00 - ETA: 10s - loss: 0.0066 - acc: 1.00 - ETA: 10s - loss: 0.0065 - acc: 1.00 - ETA: 9s - loss: 0.0064 - acc: 1.0000 - ETA: 8s - loss: 0.0069 - acc: 1.000 - ETA: 8s - loss: 0.0071 - acc: 1.000 - ETA: 7s - loss: 0.0070 - acc: 1.000 - ETA: 7s - loss: 0.0076 - acc: 1.000 - ETA: 6s - loss: 0.0075 - acc: 1.000 - ETA: 5s - loss: 0.0074 - acc: 1.000 - ETA: 5s - loss: 0.0077 - acc: 0.999 - ETA: 4s - loss: 0.0075 - acc: 0.999 - ETA: 3s - loss: 0.0074 - acc: 0.999 - ETA: 3s - loss: 0.0073 - acc: 0.999 - ETA: 2s - loss: 0.0072 - acc: 0.999 - ETA: 2s - loss: 0.0074 - acc: 0.999 - ETA: 1s - loss: 0.0073 - acc: 0.999 - ETA: 0s - loss: 0.0072 - acc: 0.999 - ETA: 0s - loss: 0.0071 - acc: 0.999 - 21s 10ms/step - loss: 0.0071 - acc: 0.9998 - val_loss: 0.0549 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.05023\n",
      "Epoch 87/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.0166 - acc: 1.00 - ETA: 17s - loss: 0.0109 - acc: 1.00 - ETA: 16s - loss: 0.0088 - acc: 1.00 - ETA: 16s - loss: 0.0094 - acc: 1.00 - ETA: 15s - loss: 0.0087 - acc: 1.00 - ETA: 14s - loss: 0.0087 - acc: 1.00 - ETA: 14s - loss: 0.0079 - acc: 1.00 - ETA: 14s - loss: 0.0072 - acc: 1.00 - ETA: 13s - loss: 0.0072 - acc: 1.00 - ETA: 12s - loss: 0.0069 - acc: 1.00 - ETA: 12s - loss: 0.0082 - acc: 0.99 - ETA: 11s - loss: 0.0082 - acc: 0.99 - ETA: 11s - loss: 0.0078 - acc: 0.99 - ETA: 10s - loss: 0.0084 - acc: 0.99 - ETA: 10s - loss: 0.0081 - acc: 0.99 - ETA: 9s - loss: 0.0080 - acc: 0.9985 - ETA: 8s - loss: 0.0080 - acc: 0.998 - ETA: 8s - loss: 0.0081 - acc: 0.998 - ETA: 7s - loss: 0.0083 - acc: 0.998 - ETA: 7s - loss: 0.0082 - acc: 0.998 - ETA: 6s - loss: 0.0080 - acc: 0.998 - ETA: 5s - loss: 0.0080 - acc: 0.998 - ETA: 5s - loss: 0.0081 - acc: 0.999 - ETA: 4s - loss: 0.0080 - acc: 0.999 - ETA: 4s - loss: 0.0086 - acc: 0.998 - ETA: 3s - loss: 0.0085 - acc: 0.998 - ETA: 3s - loss: 0.0086 - acc: 0.998 - ETA: 2s - loss: 0.0086 - acc: 0.998 - ETA: 2s - loss: 0.0092 - acc: 0.998 - ETA: 1s - loss: 0.0091 - acc: 0.998 - ETA: 0s - loss: 0.0091 - acc: 0.998 - ETA: 0s - loss: 0.0089 - acc: 0.998 - 21s 10ms/step - loss: 0.0090 - acc: 0.9986 - val_loss: 0.0533 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.05023\n",
      "Epoch 88/100\n",
      "2076/2076 [==============================] - ETA: 18s - loss: 0.0049 - acc: 1.00 - ETA: 17s - loss: 0.0056 - acc: 1.00 - ETA: 16s - loss: 0.0074 - acc: 1.00 - ETA: 16s - loss: 0.0078 - acc: 1.00 - ETA: 15s - loss: 0.0090 - acc: 1.00 - ETA: 14s - loss: 0.0087 - acc: 1.00 - ETA: 14s - loss: 0.0079 - acc: 1.00 - ETA: 13s - loss: 0.0081 - acc: 1.00 - ETA: 13s - loss: 0.0083 - acc: 1.00 - ETA: 12s - loss: 0.0084 - acc: 1.00 - ETA: 12s - loss: 0.0083 - acc: 1.00 - ETA: 11s - loss: 0.0084 - acc: 1.00 - ETA: 11s - loss: 0.0082 - acc: 1.00 - ETA: 10s - loss: 0.0097 - acc: 0.99 - ETA: 9s - loss: 0.0096 - acc: 0.9990 - ETA: 9s - loss: 0.0096 - acc: 0.999 - ETA: 8s - loss: 0.0095 - acc: 0.999 - ETA: 8s - loss: 0.0093 - acc: 0.999 - ETA: 7s - loss: 0.0091 - acc: 0.999 - ETA: 7s - loss: 0.0088 - acc: 0.999 - ETA: 6s - loss: 0.0086 - acc: 0.999 - ETA: 5s - loss: 0.0084 - acc: 0.999 - ETA: 5s - loss: 0.0082 - acc: 0.999 - ETA: 4s - loss: 0.0081 - acc: 0.999 - ETA: 4s - loss: 0.0079 - acc: 0.999 - ETA: 3s - loss: 0.0077 - acc: 0.999 - ETA: 3s - loss: 0.0077 - acc: 0.999 - ETA: 2s - loss: 0.0077 - acc: 0.999 - ETA: 1s - loss: 0.0077 - acc: 0.999 - ETA: 1s - loss: 0.0078 - acc: 0.999 - ETA: 0s - loss: 0.0076 - acc: 0.999 - ETA: 0s - loss: 0.0075 - acc: 0.999 - 20s 10ms/step - loss: 0.0076 - acc: 0.9995 - val_loss: 0.0533 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.05023\n",
      "Epoch 89/100\n",
      "2076/2076 [==============================] - ETA: 19s - loss: 0.0071 - acc: 1.00 - ETA: 18s - loss: 0.0044 - acc: 1.00 - ETA: 17s - loss: 0.0045 - acc: 1.00 - ETA: 17s - loss: 0.0046 - acc: 1.00 - ETA: 16s - loss: 0.0055 - acc: 1.00 - ETA: 15s - loss: 0.0059 - acc: 1.00 - ETA: 15s - loss: 0.0061 - acc: 1.00 - ETA: 14s - loss: 0.0064 - acc: 1.00 - ETA: 14s - loss: 0.0071 - acc: 1.00 - ETA: 13s - loss: 0.0067 - acc: 1.00 - ETA: 12s - loss: 0.0064 - acc: 1.00 - ETA: 12s - loss: 0.0066 - acc: 1.00 - ETA: 11s - loss: 0.0063 - acc: 1.00 - ETA: 11s - loss: 0.0066 - acc: 1.00 - ETA: 10s - loss: 0.0064 - acc: 1.00 - ETA: 9s - loss: 0.0067 - acc: 1.0000 - ETA: 9s - loss: 0.0069 - acc: 1.000 - ETA: 8s - loss: 0.0069 - acc: 1.000 - ETA: 8s - loss: 0.0068 - acc: 1.000 - ETA: 7s - loss: 0.0067 - acc: 1.000 - ETA: 6s - loss: 0.0065 - acc: 1.000 - ETA: 6s - loss: 0.0068 - acc: 1.000 - ETA: 5s - loss: 0.0068 - acc: 1.000 - ETA: 5s - loss: 0.0068 - acc: 1.000 - ETA: 4s - loss: 0.0069 - acc: 1.000 - ETA: 3s - loss: 0.0069 - acc: 1.000 - ETA: 3s - loss: 0.0067 - acc: 1.000 - ETA: 2s - loss: 0.0066 - acc: 1.000 - ETA: 2s - loss: 0.0068 - acc: 1.000 - ETA: 1s - loss: 0.0067 - acc: 1.000 - ETA: 0s - loss: 0.0067 - acc: 1.000 - ETA: 0s - loss: 0.0067 - acc: 1.000 - 22s 10ms/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.0552 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.05023\n",
      "Epoch 90/100\n",
      "2076/2076 [==============================] - ETA: 18s - loss: 0.0052 - acc: 1.00 - ETA: 17s - loss: 0.0061 - acc: 1.00 - ETA: 17s - loss: 0.0079 - acc: 1.00 - ETA: 16s - loss: 0.0077 - acc: 1.00 - ETA: 16s - loss: 0.0069 - acc: 1.00 - ETA: 15s - loss: 0.0064 - acc: 1.00 - ETA: 15s - loss: 0.0060 - acc: 1.00 - ETA: 14s - loss: 0.0057 - acc: 1.00 - ETA: 13s - loss: 0.0060 - acc: 1.00 - ETA: 13s - loss: 0.0065 - acc: 1.00 - ETA: 12s - loss: 0.0065 - acc: 1.00 - ETA: 11s - loss: 0.0071 - acc: 1.00 - ETA: 11s - loss: 0.0071 - acc: 1.00 - ETA: 10s - loss: 0.0072 - acc: 1.00 - ETA: 10s - loss: 0.0070 - acc: 1.00 - ETA: 9s - loss: 0.0072 - acc: 1.0000 - ETA: 9s - loss: 0.0071 - acc: 1.000 - ETA: 8s - loss: 0.0070 - acc: 1.000 - ETA: 8s - loss: 0.0068 - acc: 1.000 - ETA: 7s - loss: 0.0067 - acc: 1.000 - ETA: 6s - loss: 0.0066 - acc: 1.000 - ETA: 6s - loss: 0.0064 - acc: 1.000 - ETA: 5s - loss: 0.0063 - acc: 1.000 - ETA: 5s - loss: 0.0063 - acc: 1.000 - ETA: 4s - loss: 0.0064 - acc: 1.000 - ETA: 4s - loss: 0.0066 - acc: 1.000 - ETA: 3s - loss: 0.0065 - acc: 1.000 - ETA: 2s - loss: 0.0065 - acc: 1.000 - ETA: 2s - loss: 0.0066 - acc: 1.000 - ETA: 1s - loss: 0.0067 - acc: 1.000 - ETA: 0s - loss: 0.0069 - acc: 1.000 - ETA: 0s - loss: 0.0069 - acc: 1.000 - 22s 11ms/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.0598 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.05023\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2076/2076 [==============================] - ETA: 18s - loss: 0.0054 - acc: 1.00 - ETA: 17s - loss: 0.0068 - acc: 1.00 - ETA: 17s - loss: 0.0059 - acc: 1.00 - ETA: 16s - loss: 0.0050 - acc: 1.00 - ETA: 15s - loss: 0.0057 - acc: 1.00 - ETA: 15s - loss: 0.0064 - acc: 1.00 - ETA: 14s - loss: 0.0065 - acc: 1.00 - ETA: 14s - loss: 0.0089 - acc: 0.99 - ETA: 14s - loss: 0.0086 - acc: 0.99 - ETA: 13s - loss: 0.0084 - acc: 0.99 - ETA: 13s - loss: 0.0086 - acc: 0.99 - ETA: 12s - loss: 0.0082 - acc: 0.99 - ETA: 12s - loss: 0.0079 - acc: 0.99 - ETA: 11s - loss: 0.0077 - acc: 0.99 - ETA: 10s - loss: 0.0075 - acc: 0.99 - ETA: 10s - loss: 0.0073 - acc: 0.99 - ETA: 9s - loss: 0.0072 - acc: 0.9991 - ETA: 8s - loss: 0.0071 - acc: 0.999 - ETA: 8s - loss: 0.0068 - acc: 0.999 - ETA: 7s - loss: 0.0066 - acc: 0.999 - ETA: 7s - loss: 0.0067 - acc: 0.999 - ETA: 6s - loss: 0.0067 - acc: 0.999 - ETA: 5s - loss: 0.0068 - acc: 0.999 - ETA: 5s - loss: 0.0066 - acc: 0.999 - ETA: 4s - loss: 0.0071 - acc: 0.999 - ETA: 3s - loss: 0.0073 - acc: 0.999 - ETA: 3s - loss: 0.0073 - acc: 0.999 - ETA: 2s - loss: 0.0073 - acc: 0.999 - ETA: 2s - loss: 0.0073 - acc: 0.999 - ETA: 1s - loss: 0.0076 - acc: 0.999 - ETA: 0s - loss: 0.0078 - acc: 0.999 - ETA: 0s - loss: 0.0078 - acc: 0.999 - 21s 10ms/step - loss: 0.0077 - acc: 0.9993 - val_loss: 0.0567 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.05023\n",
      "Epoch 92/100\n",
      "2076/2076 [==============================] - ETA: 19s - loss: 0.0080 - acc: 1.00 - ETA: 17s - loss: 0.0058 - acc: 1.00 - ETA: 16s - loss: 0.0056 - acc: 1.00 - ETA: 16s - loss: 0.0066 - acc: 1.00 - ETA: 15s - loss: 0.0065 - acc: 1.00 - ETA: 14s - loss: 0.0057 - acc: 1.00 - ETA: 14s - loss: 0.0059 - acc: 1.00 - ETA: 13s - loss: 0.0064 - acc: 1.00 - ETA: 13s - loss: 0.0061 - acc: 1.00 - ETA: 12s - loss: 0.0059 - acc: 1.00 - ETA: 12s - loss: 0.0060 - acc: 1.00 - ETA: 11s - loss: 0.0060 - acc: 1.00 - ETA: 10s - loss: 0.0062 - acc: 1.00 - ETA: 10s - loss: 0.0072 - acc: 0.99 - ETA: 9s - loss: 0.0072 - acc: 0.9995 - ETA: 9s - loss: 0.0080 - acc: 0.999 - ETA: 8s - loss: 0.0076 - acc: 0.999 - ETA: 8s - loss: 0.0073 - acc: 0.999 - ETA: 7s - loss: 0.0073 - acc: 0.999 - ETA: 7s - loss: 0.0071 - acc: 0.999 - ETA: 6s - loss: 0.0071 - acc: 0.999 - ETA: 5s - loss: 0.0073 - acc: 0.999 - ETA: 5s - loss: 0.0072 - acc: 0.999 - ETA: 4s - loss: 0.0070 - acc: 0.999 - ETA: 4s - loss: 0.0073 - acc: 0.999 - ETA: 3s - loss: 0.0073 - acc: 0.999 - ETA: 3s - loss: 0.0072 - acc: 0.999 - ETA: 2s - loss: 0.0071 - acc: 0.999 - ETA: 1s - loss: 0.0070 - acc: 0.999 - ETA: 1s - loss: 0.0068 - acc: 0.999 - ETA: 0s - loss: 0.0069 - acc: 0.999 - ETA: 0s - loss: 0.0068 - acc: 0.999 - 20s 10ms/step - loss: 0.0068 - acc: 0.9995 - val_loss: 0.0567 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.05023\n",
      "Epoch 93/100\n",
      "2076/2076 [==============================] - ETA: 19s - loss: 0.0086 - acc: 1.00 - ETA: 18s - loss: 0.0076 - acc: 1.00 - ETA: 17s - loss: 0.0071 - acc: 1.00 - ETA: 17s - loss: 0.0067 - acc: 1.00 - ETA: 17s - loss: 0.0061 - acc: 1.00 - ETA: 16s - loss: 0.0071 - acc: 1.00 - ETA: 15s - loss: 0.0066 - acc: 1.00 - ETA: 14s - loss: 0.0065 - acc: 1.00 - ETA: 14s - loss: 0.0063 - acc: 1.00 - ETA: 13s - loss: 0.0060 - acc: 1.00 - ETA: 12s - loss: 0.0060 - acc: 1.00 - ETA: 12s - loss: 0.0060 - acc: 1.00 - ETA: 11s - loss: 0.0059 - acc: 1.00 - ETA: 10s - loss: 0.0057 - acc: 1.00 - ETA: 10s - loss: 0.0059 - acc: 1.00 - ETA: 9s - loss: 0.0059 - acc: 1.0000 - ETA: 9s - loss: 0.0059 - acc: 1.000 - ETA: 8s - loss: 0.0057 - acc: 1.000 - ETA: 7s - loss: 0.0057 - acc: 1.000 - ETA: 7s - loss: 0.0056 - acc: 1.000 - ETA: 6s - loss: 0.0056 - acc: 1.000 - ETA: 6s - loss: 0.0056 - acc: 1.000 - ETA: 5s - loss: 0.0055 - acc: 1.000 - ETA: 5s - loss: 0.0056 - acc: 1.000 - ETA: 4s - loss: 0.0056 - acc: 1.000 - ETA: 3s - loss: 0.0055 - acc: 1.000 - ETA: 3s - loss: 0.0056 - acc: 1.000 - ETA: 2s - loss: 0.0055 - acc: 1.000 - ETA: 2s - loss: 0.0055 - acc: 1.000 - ETA: 1s - loss: 0.0055 - acc: 1.000 - ETA: 0s - loss: 0.0055 - acc: 1.000 - ETA: 0s - loss: 0.0055 - acc: 1.000 - 21s 10ms/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.0531 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.05023\n",
      "Epoch 94/100\n",
      "2076/2076 [==============================] - ETA: 18s - loss: 0.0086 - acc: 1.00 - ETA: 17s - loss: 0.0069 - acc: 1.00 - ETA: 16s - loss: 0.0066 - acc: 1.00 - ETA: 16s - loss: 0.0062 - acc: 1.00 - ETA: 15s - loss: 0.0053 - acc: 1.00 - ETA: 15s - loss: 0.0048 - acc: 1.00 - ETA: 14s - loss: 0.0045 - acc: 1.00 - ETA: 13s - loss: 0.0043 - acc: 1.00 - ETA: 13s - loss: 0.0043 - acc: 1.00 - ETA: 12s - loss: 0.0042 - acc: 1.00 - ETA: 12s - loss: 0.0050 - acc: 0.99 - ETA: 11s - loss: 0.0048 - acc: 0.99 - ETA: 11s - loss: 0.0053 - acc: 0.99 - ETA: 10s - loss: 0.0063 - acc: 0.99 - ETA: 10s - loss: 0.0062 - acc: 0.99 - ETA: 9s - loss: 0.0062 - acc: 0.9985 - ETA: 8s - loss: 0.0064 - acc: 0.998 - ETA: 8s - loss: 0.0062 - acc: 0.998 - ETA: 7s - loss: 0.0061 - acc: 0.998 - ETA: 7s - loss: 0.0060 - acc: 0.998 - ETA: 6s - loss: 0.0059 - acc: 0.998 - ETA: 6s - loss: 0.0058 - acc: 0.998 - ETA: 5s - loss: 0.0058 - acc: 0.999 - ETA: 4s - loss: 0.0058 - acc: 0.999 - ETA: 4s - loss: 0.0058 - acc: 0.999 - ETA: 3s - loss: 0.0061 - acc: 0.999 - ETA: 3s - loss: 0.0060 - acc: 0.999 - ETA: 2s - loss: 0.0064 - acc: 0.998 - ETA: 2s - loss: 0.0064 - acc: 0.998 - ETA: 1s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.998 - 21s 10ms/step - loss: 0.0066 - acc: 0.9988 - val_loss: 0.0569 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.05023\n",
      "Epoch 95/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.0040 - acc: 1.00 - ETA: 17s - loss: 0.0096 - acc: 1.00 - ETA: 16s - loss: 0.0077 - acc: 1.00 - ETA: 16s - loss: 0.0066 - acc: 1.00 - ETA: 15s - loss: 0.0069 - acc: 1.00 - ETA: 15s - loss: 0.0061 - acc: 1.00 - ETA: 15s - loss: 0.0065 - acc: 1.00 - ETA: 15s - loss: 0.0066 - acc: 1.00 - ETA: 14s - loss: 0.0063 - acc: 1.00 - ETA: 14s - loss: 0.0061 - acc: 1.00 - ETA: 14s - loss: 0.0061 - acc: 1.00 - ETA: 13s - loss: 0.0063 - acc: 1.00 - ETA: 12s - loss: 0.0063 - acc: 1.00 - ETA: 12s - loss: 0.0060 - acc: 1.00 - ETA: 11s - loss: 0.0058 - acc: 1.00 - ETA: 10s - loss: 0.0060 - acc: 1.00 - ETA: 10s - loss: 0.0058 - acc: 1.00 - ETA: 9s - loss: 0.0058 - acc: 1.0000 - ETA: 8s - loss: 0.0058 - acc: 1.000 - ETA: 8s - loss: 0.0059 - acc: 1.000 - ETA: 7s - loss: 0.0059 - acc: 1.000 - ETA: 6s - loss: 0.0058 - acc: 1.000 - ETA: 6s - loss: 0.0059 - acc: 1.000 - ETA: 5s - loss: 0.0059 - acc: 1.000 - ETA: 4s - loss: 0.0059 - acc: 1.000 - ETA: 4s - loss: 0.0059 - acc: 1.000 - ETA: 3s - loss: 0.0058 - acc: 1.000 - ETA: 2s - loss: 0.0057 - acc: 1.000 - ETA: 2s - loss: 0.0057 - acc: 1.000 - ETA: 1s - loss: 0.0057 - acc: 1.000 - ETA: 0s - loss: 0.0056 - acc: 1.000 - ETA: 0s - loss: 0.0057 - acc: 1.000 - 22s 11ms/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.0521 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.05023\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2076/2076 [==============================] - ETA: 18s - loss: 0.0043 - acc: 1.00 - ETA: 17s - loss: 0.0042 - acc: 1.00 - ETA: 17s - loss: 0.0042 - acc: 1.00 - ETA: 17s - loss: 0.0040 - acc: 1.00 - ETA: 17s - loss: 0.0041 - acc: 1.00 - ETA: 16s - loss: 0.0050 - acc: 1.00 - ETA: 15s - loss: 0.0052 - acc: 1.00 - ETA: 14s - loss: 0.0051 - acc: 1.00 - ETA: 14s - loss: 0.0050 - acc: 1.00 - ETA: 13s - loss: 0.0052 - acc: 1.00 - ETA: 12s - loss: 0.0051 - acc: 1.00 - ETA: 12s - loss: 0.0049 - acc: 1.00 - ETA: 11s - loss: 0.0050 - acc: 1.00 - ETA: 10s - loss: 0.0048 - acc: 1.00 - ETA: 10s - loss: 0.0048 - acc: 1.00 - ETA: 10s - loss: 0.0048 - acc: 1.00 - ETA: 9s - loss: 0.0048 - acc: 1.0000 - ETA: 9s - loss: 0.0056 - acc: 0.999 - ETA: 8s - loss: 0.0055 - acc: 0.999 - ETA: 7s - loss: 0.0057 - acc: 0.999 - ETA: 7s - loss: 0.0058 - acc: 0.999 - ETA: 6s - loss: 0.0059 - acc: 0.999 - ETA: 5s - loss: 0.0058 - acc: 0.999 - ETA: 5s - loss: 0.0057 - acc: 0.999 - ETA: 4s - loss: 0.0058 - acc: 0.999 - ETA: 3s - loss: 0.0057 - acc: 0.999 - ETA: 3s - loss: 0.0055 - acc: 0.999 - ETA: 2s - loss: 0.0054 - acc: 0.999 - ETA: 2s - loss: 0.0056 - acc: 0.999 - ETA: 1s - loss: 0.0056 - acc: 0.999 - ETA: 0s - loss: 0.0055 - acc: 0.999 - ETA: 0s - loss: 0.0054 - acc: 0.999 - 22s 10ms/step - loss: 0.0054 - acc: 0.9995 - val_loss: 0.0550 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.05023\n",
      "Epoch 97/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.0061 - acc: 1.00 - ETA: 17s - loss: 0.0044 - acc: 1.00 - ETA: 16s - loss: 0.0046 - acc: 1.00 - ETA: 16s - loss: 0.0047 - acc: 1.00 - ETA: 15s - loss: 0.0044 - acc: 1.00 - ETA: 15s - loss: 0.0061 - acc: 1.00 - ETA: 14s - loss: 0.0064 - acc: 1.00 - ETA: 13s - loss: 0.0058 - acc: 1.00 - ETA: 13s - loss: 0.0060 - acc: 1.00 - ETA: 12s - loss: 0.0063 - acc: 1.00 - ETA: 12s - loss: 0.0059 - acc: 1.00 - ETA: 11s - loss: 0.0061 - acc: 1.00 - ETA: 10s - loss: 0.0060 - acc: 1.00 - ETA: 10s - loss: 0.0057 - acc: 1.00 - ETA: 9s - loss: 0.0058 - acc: 1.0000 - ETA: 9s - loss: 0.0056 - acc: 1.000 - ETA: 8s - loss: 0.0060 - acc: 1.000 - ETA: 8s - loss: 0.0062 - acc: 1.000 - ETA: 7s - loss: 0.0061 - acc: 1.000 - ETA: 7s - loss: 0.0059 - acc: 1.000 - ETA: 6s - loss: 0.0058 - acc: 1.000 - ETA: 5s - loss: 0.0056 - acc: 1.000 - ETA: 5s - loss: 0.0057 - acc: 1.000 - ETA: 4s - loss: 0.0055 - acc: 1.000 - ETA: 4s - loss: 0.0054 - acc: 1.000 - ETA: 3s - loss: 0.0054 - acc: 1.000 - ETA: 3s - loss: 0.0054 - acc: 1.000 - ETA: 2s - loss: 0.0054 - acc: 1.000 - ETA: 1s - loss: 0.0053 - acc: 1.000 - ETA: 1s - loss: 0.0053 - acc: 1.000 - ETA: 0s - loss: 0.0052 - acc: 1.000 - ETA: 0s - loss: 0.0052 - acc: 1.000 - 20s 10ms/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.0541 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.05023\n",
      "Epoch 98/100\n",
      "2076/2076 [==============================] - ETA: 21s - loss: 0.0017 - acc: 1.00 - ETA: 19s - loss: 0.0056 - acc: 1.00 - ETA: 18s - loss: 0.0055 - acc: 1.00 - ETA: 18s - loss: 0.0059 - acc: 1.00 - ETA: 18s - loss: 0.0061 - acc: 1.00 - ETA: 17s - loss: 0.0061 - acc: 1.00 - ETA: 17s - loss: 0.0063 - acc: 1.00 - ETA: 16s - loss: 0.0061 - acc: 1.00 - ETA: 15s - loss: 0.0066 - acc: 1.00 - ETA: 14s - loss: 0.0069 - acc: 1.00 - ETA: 14s - loss: 0.0067 - acc: 1.00 - ETA: 13s - loss: 0.0064 - acc: 1.00 - ETA: 12s - loss: 0.0064 - acc: 1.00 - ETA: 11s - loss: 0.0061 - acc: 1.00 - ETA: 11s - loss: 0.0061 - acc: 1.00 - ETA: 10s - loss: 0.0060 - acc: 1.00 - ETA: 9s - loss: 0.0059 - acc: 1.0000 - ETA: 9s - loss: 0.0057 - acc: 1.000 - ETA: 8s - loss: 0.0057 - acc: 1.000 - ETA: 7s - loss: 0.0055 - acc: 1.000 - ETA: 7s - loss: 0.0056 - acc: 1.000 - ETA: 6s - loss: 0.0054 - acc: 1.000 - ETA: 5s - loss: 0.0055 - acc: 1.000 - ETA: 5s - loss: 0.0055 - acc: 1.000 - ETA: 4s - loss: 0.0053 - acc: 1.000 - ETA: 4s - loss: 0.0053 - acc: 1.000 - ETA: 3s - loss: 0.0054 - acc: 1.000 - ETA: 2s - loss: 0.0053 - acc: 1.000 - ETA: 2s - loss: 0.0053 - acc: 1.000 - ETA: 1s - loss: 0.0053 - acc: 1.000 - ETA: 0s - loss: 0.0054 - acc: 1.000 - ETA: 0s - loss: 0.0054 - acc: 1.000 - 21s 10ms/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.0565 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.05023\n",
      "Epoch 99/100\n",
      "2076/2076 [==============================] - ETA: 17s - loss: 0.0035 - acc: 1.00 - ETA: 17s - loss: 0.0067 - acc: 1.00 - ETA: 16s - loss: 0.0055 - acc: 1.00 - ETA: 16s - loss: 0.0055 - acc: 1.00 - ETA: 15s - loss: 0.0054 - acc: 1.00 - ETA: 15s - loss: 0.0058 - acc: 1.00 - ETA: 14s - loss: 0.0058 - acc: 1.00 - ETA: 14s - loss: 0.0054 - acc: 1.00 - ETA: 13s - loss: 0.0054 - acc: 1.00 - ETA: 13s - loss: 0.0054 - acc: 1.00 - ETA: 12s - loss: 0.0056 - acc: 1.00 - ETA: 11s - loss: 0.0056 - acc: 1.00 - ETA: 11s - loss: 0.0059 - acc: 1.00 - ETA: 11s - loss: 0.0058 - acc: 1.00 - ETA: 10s - loss: 0.0055 - acc: 1.00 - ETA: 10s - loss: 0.0057 - acc: 1.00 - ETA: 9s - loss: 0.0057 - acc: 1.0000 - ETA: 9s - loss: 0.0054 - acc: 1.000 - ETA: 8s - loss: 0.0054 - acc: 1.000 - ETA: 7s - loss: 0.0054 - acc: 1.000 - ETA: 7s - loss: 0.0053 - acc: 1.000 - ETA: 6s - loss: 0.0052 - acc: 1.000 - ETA: 5s - loss: 0.0051 - acc: 1.000 - ETA: 5s - loss: 0.0049 - acc: 1.000 - ETA: 4s - loss: 0.0048 - acc: 1.000 - ETA: 3s - loss: 0.0047 - acc: 1.000 - ETA: 3s - loss: 0.0047 - acc: 1.000 - ETA: 2s - loss: 0.0046 - acc: 1.000 - ETA: 2s - loss: 0.0048 - acc: 1.000 - ETA: 1s - loss: 0.0048 - acc: 1.000 - ETA: 0s - loss: 0.0048 - acc: 1.000 - ETA: 0s - loss: 0.0049 - acc: 1.000 - 21s 10ms/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.0561 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.05023\n",
      "Epoch 100/100\n",
      "2076/2076 [==============================] - ETA: 20s - loss: 0.0062 - acc: 1.00 - ETA: 18s - loss: 0.0064 - acc: 1.00 - ETA: 17s - loss: 0.0103 - acc: 1.00 - ETA: 17s - loss: 0.0097 - acc: 1.00 - ETA: 16s - loss: 0.0083 - acc: 1.00 - ETA: 15s - loss: 0.0076 - acc: 1.00 - ETA: 15s - loss: 0.0070 - acc: 1.00 - ETA: 14s - loss: 0.0074 - acc: 1.00 - ETA: 14s - loss: 0.0071 - acc: 1.00 - ETA: 13s - loss: 0.0066 - acc: 1.00 - ETA: 12s - loss: 0.0063 - acc: 1.00 - ETA: 12s - loss: 0.0060 - acc: 1.00 - ETA: 11s - loss: 0.0058 - acc: 1.00 - ETA: 11s - loss: 0.0057 - acc: 1.00 - ETA: 10s - loss: 0.0054 - acc: 1.00 - ETA: 9s - loss: 0.0055 - acc: 1.0000 - ETA: 9s - loss: 0.0054 - acc: 1.000 - ETA: 8s - loss: 0.0053 - acc: 1.000 - ETA: 7s - loss: 0.0052 - acc: 1.000 - ETA: 7s - loss: 0.0051 - acc: 1.000 - ETA: 6s - loss: 0.0051 - acc: 1.000 - ETA: 6s - loss: 0.0053 - acc: 1.000 - ETA: 5s - loss: 0.0052 - acc: 1.000 - ETA: 5s - loss: 0.0052 - acc: 1.000 - ETA: 4s - loss: 0.0050 - acc: 1.000 - ETA: 3s - loss: 0.0049 - acc: 1.000 - ETA: 3s - loss: 0.0049 - acc: 1.000 - ETA: 2s - loss: 0.0049 - acc: 1.000 - ETA: 2s - loss: 0.0049 - acc: 1.000 - ETA: 1s - loss: 0.0049 - acc: 1.000 - ETA: 0s - loss: 0.0049 - acc: 1.000 - ETA: 0s - loss: 0.0049 - acc: 1.000 - 21s 10ms/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.0570 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.05023\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[mcp_save], batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
