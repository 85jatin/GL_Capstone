{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video Classification Model : https://towardsdatascience.com/transfer-learning-using-mobilenet-and-keras-c75daf7ff299"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1:  import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, InputLayer, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications import MobileNet\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from IPython.display import Image\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Read each frame and their corresponding tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A_OffSide_shot1.mp4_frame100.jpg</td>\n",
       "      <td>OffSide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A_OffSide_shot1.mp4_frame101.jpg</td>\n",
       "      <td>OffSide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A_OffSide_shot1.mp4_frame102.jpg</td>\n",
       "      <td>OffSide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A_OffSide_shot1.mp4_frame103.jpg</td>\n",
       "      <td>OffSide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>A_OffSide_shot1.mp4_frame104.jpg</td>\n",
       "      <td>OffSide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              image    class\n",
       "0  A_OffSide_shot1.mp4_frame100.jpg  OffSide\n",
       "1  A_OffSide_shot1.mp4_frame101.jpg  OffSide\n",
       "2  A_OffSide_shot1.mp4_frame102.jpg  OffSide\n",
       "3  A_OffSide_shot1.mp4_frame103.jpg  OffSide\n",
       "4  A_OffSide_shot1.mp4_frame104.jpg  OffSide"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train_new.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Read the frames and then store them as a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5426/5426 [00:35<00:00, 151.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5426, 224, 224, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an empty list\n",
    "train_image = []\n",
    "\n",
    "# for loop to read and store frames\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    # loading the image and keeping the target size as (224,224,3)\n",
    "    img = image.load_img('train_1/'+train['image'][i], target_size=(224,224,3))\n",
    "    # converting it to array\n",
    "    img = image.img_to_array(img)\n",
    "    # normalizing the pixel value\n",
    "    img = img/255\n",
    "    # appending the image to the train_image list\n",
    "    train_image.append(img)\n",
    "    \n",
    "# converting the list to numpy array\n",
    "X = np.array(train_image)\n",
    "\n",
    "# shape of the array\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: Create the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the target\n",
    "y = train['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training and validation set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: Create 4 different columns in the target, one for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dummies of target variable for train and validation set\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_test = pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OffSide     3255\n",
       "Legside     1291\n",
       "Straight     880\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: Use the MobileNet pre-trained model to create the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now re-use MobileNet as it is quite lightweight (17Mb), freeze the base layers and lets add and train the top few layers (https://machinethink.net/blog/compressing-deep-neural-nets/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin_Thakkar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras_applications\\mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jatin_Thakkar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Jatin_Thakkar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Jatin_Thakkar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Jatin_Thakkar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Jatin_Thakkar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Jatin_Thakkar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#imports the mobilenet model and discards the last 1000 neuron layer.\n",
    "base_model = MobileNet(weights='imagenet',include_top=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: Extract features from this pre-trained model for our training and validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4340, 7, 7, 1024)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting features for training frames\n",
    "X_train = base_model.predict(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1086, 7, 7, 1024)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting features for validation frames\n",
    "X_test = base_model.predict(X_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8: reshape the images into a single dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the training as well as validation frames in single dimension\n",
    "X_train = X_train.reshape(4340, 7*7*1024)\n",
    "X_test = X_test.reshape(1086, 7*7*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 9: Normalize the pixel values (between 0 and 1, helps the model to converge faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the pixel values\n",
    "max = X_train.max()\n",
    "X_train = X_train/max\n",
    "X_test = X_test/max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 10: Create the architecture of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4340, 50176)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of images\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jatin_Thakkar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "#defining the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu', input_shape=(50176,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 11: Train our model using the training frames and validate using validation frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to save the weights of best model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "mcp_save = ModelCheckpoint('weight_MobileNet.hdf5', save_best_only=True, monitor='val_loss', mode='min', verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving Generalization Performance by Switching from Adam to SGD\n",
    "(https://towardsdatascience.com/normalized-direction-preserving-adam-switching-from-adam-to-sgd-and-nesterov-momentum-adam-with-460be5ddf686)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jatin_Thakkar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compiling the model\n",
    "from keras.optimizers import SGD\n",
    "opt = SGD()\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1024)              51381248  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 52,070,659\n",
      "Trainable params: 52,070,659\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jatin_Thakkar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 4340 samples, validate on 1086 samples\n",
      "Epoch 1/100\n",
      "4340/4340 [==============================] - ETA: 1:39 - loss: 1.2426 - acc: 0.320 - ETA: 56s - loss: 1.1899 - acc: 0.386 - ETA: 41s - loss: 1.1471 - acc: 0.39 - ETA: 33s - loss: 1.1297 - acc: 0.40 - ETA: 28s - loss: 1.1428 - acc: 0.38 - ETA: 24s - loss: 1.1137 - acc: 0.40 - ETA: 22s - loss: 1.1095 - acc: 0.41 - ETA: 20s - loss: 1.1059 - acc: 0.42 - ETA: 18s - loss: 1.0879 - acc: 0.44 - ETA: 17s - loss: 1.0760 - acc: 0.45 - ETA: 16s - loss: 1.0748 - acc: 0.46 - ETA: 15s - loss: 1.0660 - acc: 0.47 - ETA: 14s - loss: 1.0617 - acc: 0.47 - ETA: 13s - loss: 1.0600 - acc: 0.47 - ETA: 12s - loss: 1.0580 - acc: 0.47 - ETA: 11s - loss: 1.0576 - acc: 0.47 - ETA: 10s - loss: 1.0560 - acc: 0.48 - ETA: 9s - loss: 1.0528 - acc: 0.4857 - ETA: 9s - loss: 1.0502 - acc: 0.487 - ETA: 8s - loss: 1.0503 - acc: 0.488 - ETA: 7s - loss: 1.0506 - acc: 0.488 - ETA: 7s - loss: 1.0515 - acc: 0.490 - ETA: 6s - loss: 1.0476 - acc: 0.490 - ETA: 5s - loss: 1.0469 - acc: 0.491 - ETA: 5s - loss: 1.0419 - acc: 0.496 - ETA: 4s - loss: 1.0391 - acc: 0.500 - ETA: 3s - loss: 1.0372 - acc: 0.500 - ETA: 3s - loss: 1.0363 - acc: 0.502 - ETA: 2s - loss: 1.0320 - acc: 0.505 - ETA: 2s - loss: 1.0291 - acc: 0.508 - ETA: 1s - loss: 1.0283 - acc: 0.508 - ETA: 1s - loss: 1.0289 - acc: 0.508 - ETA: 0s - loss: 1.0299 - acc: 0.509 - 20s 5ms/step - loss: 1.0293 - acc: 0.5101 - val_loss: 0.9126 - val_acc: 0.6004\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.91263, saving model to weight_MobileNet.hdf5\n",
      "Epoch 2/100\n",
      "4340/4340 [==============================] - ETA: 13s - loss: 0.9909 - acc: 0.52 - ETA: 17s - loss: 0.9984 - acc: 0.54 - ETA: 15s - loss: 1.0190 - acc: 0.53 - ETA: 14s - loss: 1.0312 - acc: 0.51 - ETA: 13s - loss: 1.0117 - acc: 0.53 - ETA: 13s - loss: 1.0106 - acc: 0.53 - ETA: 12s - loss: 1.0014 - acc: 0.54 - ETA: 12s - loss: 0.9975 - acc: 0.54 - ETA: 11s - loss: 0.9865 - acc: 0.55 - ETA: 11s - loss: 0.9813 - acc: 0.55 - ETA: 11s - loss: 0.9875 - acc: 0.55 - ETA: 10s - loss: 0.9823 - acc: 0.55 - ETA: 10s - loss: 0.9784 - acc: 0.56 - ETA: 9s - loss: 0.9817 - acc: 0.5597 - ETA: 9s - loss: 0.9782 - acc: 0.561 - ETA: 8s - loss: 0.9778 - acc: 0.563 - ETA: 8s - loss: 0.9722 - acc: 0.566 - ETA: 7s - loss: 0.9679 - acc: 0.571 - ETA: 7s - loss: 0.9702 - acc: 0.568 - ETA: 6s - loss: 0.9685 - acc: 0.568 - ETA: 6s - loss: 0.9654 - acc: 0.569 - ETA: 5s - loss: 0.9608 - acc: 0.574 - ETA: 5s - loss: 0.9590 - acc: 0.575 - ETA: 4s - loss: 0.9635 - acc: 0.571 - ETA: 4s - loss: 0.9658 - acc: 0.569 - ETA: 3s - loss: 0.9635 - acc: 0.573 - ETA: 3s - loss: 0.9629 - acc: 0.573 - ETA: 2s - loss: 0.9610 - acc: 0.573 - ETA: 2s - loss: 0.9587 - acc: 0.574 - ETA: 1s - loss: 0.9601 - acc: 0.574 - ETA: 1s - loss: 0.9624 - acc: 0.572 - ETA: 0s - loss: 0.9601 - acc: 0.573 - ETA: 0s - loss: 0.9606 - acc: 0.572 - 18s 4ms/step - loss: 0.9600 - acc: 0.5733 - val_loss: 0.8720 - val_acc: 0.6004\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.91263 to 0.87203, saving model to weight_MobileNet.hdf5\n",
      "Epoch 3/100\n",
      "4340/4340 [==============================] - ETA: 21s - loss: 0.9202 - acc: 0.60 - ETA: 17s - loss: 0.9090 - acc: 0.60 - ETA: 22s - loss: 0.9344 - acc: 0.58 - ETA: 22s - loss: 0.9240 - acc: 0.57 - ETA: 20s - loss: 0.9395 - acc: 0.56 - ETA: 18s - loss: 0.9413 - acc: 0.56 - ETA: 17s - loss: 0.9407 - acc: 0.56 - ETA: 16s - loss: 0.9371 - acc: 0.57 - ETA: 15s - loss: 0.9303 - acc: 0.58 - ETA: 14s - loss: 0.9348 - acc: 0.57 - ETA: 13s - loss: 0.9341 - acc: 0.57 - ETA: 12s - loss: 0.9267 - acc: 0.58 - ETA: 11s - loss: 0.9162 - acc: 0.59 - ETA: 11s - loss: 0.9132 - acc: 0.59 - ETA: 10s - loss: 0.9100 - acc: 0.59 - ETA: 9s - loss: 0.9064 - acc: 0.5933 - ETA: 9s - loss: 0.9082 - acc: 0.593 - ETA: 8s - loss: 0.9099 - acc: 0.591 - ETA: 7s - loss: 0.9173 - acc: 0.583 - ETA: 7s - loss: 0.9159 - acc: 0.585 - ETA: 6s - loss: 0.9101 - acc: 0.589 - ETA: 6s - loss: 0.9095 - acc: 0.589 - ETA: 5s - loss: 0.9043 - acc: 0.593 - ETA: 5s - loss: 0.9076 - acc: 0.591 - ETA: 4s - loss: 0.9078 - acc: 0.592 - ETA: 4s - loss: 0.9088 - acc: 0.590 - ETA: 3s - loss: 0.9099 - acc: 0.588 - ETA: 2s - loss: 0.9050 - acc: 0.590 - ETA: 2s - loss: 0.9044 - acc: 0.591 - ETA: 1s - loss: 0.9025 - acc: 0.591 - ETA: 1s - loss: 0.9014 - acc: 0.593 - ETA: 0s - loss: 0.9044 - acc: 0.592 - ETA: 0s - loss: 0.9016 - acc: 0.594 - 18s 4ms/step - loss: 0.9039 - acc: 0.5933 - val_loss: 0.8362 - val_acc: 0.6087\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.87203 to 0.83620, saving model to weight_MobileNet.hdf5\n",
      "Epoch 4/100\n",
      "4340/4340 [==============================] - ETA: 15s - loss: 0.8678 - acc: 0.60 - ETA: 14s - loss: 0.9220 - acc: 0.57 - ETA: 13s - loss: 0.9372 - acc: 0.55 - ETA: 13s - loss: 0.9155 - acc: 0.56 - ETA: 12s - loss: 0.9226 - acc: 0.56 - ETA: 12s - loss: 0.9097 - acc: 0.57 - ETA: 11s - loss: 0.9077 - acc: 0.57 - ETA: 11s - loss: 0.9184 - acc: 0.56 - ETA: 11s - loss: 0.9030 - acc: 0.57 - ETA: 10s - loss: 0.8956 - acc: 0.58 - ETA: 10s - loss: 0.8916 - acc: 0.58 - ETA: 9s - loss: 0.8918 - acc: 0.5885 - ETA: 9s - loss: 0.8918 - acc: 0.589 - ETA: 9s - loss: 0.8872 - acc: 0.591 - ETA: 8s - loss: 0.8871 - acc: 0.589 - ETA: 8s - loss: 0.8833 - acc: 0.593 - ETA: 7s - loss: 0.8796 - acc: 0.598 - ETA: 7s - loss: 0.8844 - acc: 0.594 - ETA: 6s - loss: 0.8848 - acc: 0.594 - ETA: 6s - loss: 0.8875 - acc: 0.593 - ETA: 5s - loss: 0.8856 - acc: 0.593 - ETA: 5s - loss: 0.8816 - acc: 0.595 - ETA: 4s - loss: 0.8847 - acc: 0.593 - ETA: 4s - loss: 0.8807 - acc: 0.596 - ETA: 4s - loss: 0.8763 - acc: 0.599 - ETA: 3s - loss: 0.8744 - acc: 0.598 - ETA: 3s - loss: 0.8749 - acc: 0.598 - ETA: 2s - loss: 0.8735 - acc: 0.599 - ETA: 2s - loss: 0.8739 - acc: 0.599 - ETA: 1s - loss: 0.8738 - acc: 0.597 - ETA: 1s - loss: 0.8770 - acc: 0.595 - ETA: 0s - loss: 0.8744 - acc: 0.597 - ETA: 0s - loss: 0.8736 - acc: 0.596 - 17s 4ms/step - loss: 0.8706 - acc: 0.5991 - val_loss: 0.7880 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.83620 to 0.78797, saving model to weight_MobileNet.hdf5\n",
      "Epoch 5/100\n",
      "4340/4340 [==============================] - ETA: 13s - loss: 0.7506 - acc: 0.67 - ETA: 14s - loss: 0.7884 - acc: 0.65 - ETA: 14s - loss: 0.7984 - acc: 0.65 - ETA: 14s - loss: 0.8181 - acc: 0.63 - ETA: 14s - loss: 0.8378 - acc: 0.62 - ETA: 13s - loss: 0.8347 - acc: 0.62 - ETA: 13s - loss: 0.8327 - acc: 0.62 - ETA: 13s - loss: 0.8216 - acc: 0.63 - ETA: 12s - loss: 0.8264 - acc: 0.62 - ETA: 12s - loss: 0.8172 - acc: 0.63 - ETA: 11s - loss: 0.8314 - acc: 0.62 - ETA: 11s - loss: 0.8225 - acc: 0.62 - ETA: 10s - loss: 0.8259 - acc: 0.62 - ETA: 10s - loss: 0.8254 - acc: 0.62 - ETA: 9s - loss: 0.8264 - acc: 0.6271 - ETA: 9s - loss: 0.8241 - acc: 0.627 - ETA: 8s - loss: 0.8259 - acc: 0.625 - ETA: 8s - loss: 0.8227 - acc: 0.630 - ETA: 7s - loss: 0.8254 - acc: 0.630 - ETA: 6s - loss: 0.8245 - acc: 0.630 - ETA: 6s - loss: 0.8257 - acc: 0.629 - ETA: 5s - loss: 0.8302 - acc: 0.625 - ETA: 5s - loss: 0.8261 - acc: 0.625 - ETA: 4s - loss: 0.8254 - acc: 0.627 - ETA: 4s - loss: 0.8263 - acc: 0.626 - ETA: 3s - loss: 0.8263 - acc: 0.627 - ETA: 3s - loss: 0.8268 - acc: 0.628 - ETA: 2s - loss: 0.8257 - acc: 0.629 - ETA: 2s - loss: 0.8272 - acc: 0.629 - ETA: 1s - loss: 0.8267 - acc: 0.629 - ETA: 1s - loss: 0.8284 - acc: 0.629 - ETA: 0s - loss: 0.8275 - acc: 0.628 - ETA: 0s - loss: 0.8244 - acc: 0.630 - 18s 4ms/step - loss: 0.8257 - acc: 0.6293 - val_loss: 0.7502 - val_acc: 0.6713\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.78797 to 0.75019, saving model to weight_MobileNet.hdf5\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4340/4340 [==============================] - ETA: 13s - loss: 0.8096 - acc: 0.65 - ETA: 15s - loss: 0.7936 - acc: 0.63 - ETA: 14s - loss: 0.7818 - acc: 0.66 - ETA: 14s - loss: 0.7870 - acc: 0.64 - ETA: 13s - loss: 0.7963 - acc: 0.64 - ETA: 13s - loss: 0.8037 - acc: 0.63 - ETA: 12s - loss: 0.7971 - acc: 0.64 - ETA: 12s - loss: 0.8102 - acc: 0.63 - ETA: 13s - loss: 0.8062 - acc: 0.63 - ETA: 13s - loss: 0.8133 - acc: 0.62 - ETA: 12s - loss: 0.8092 - acc: 0.62 - ETA: 11s - loss: 0.8030 - acc: 0.62 - ETA: 11s - loss: 0.8101 - acc: 0.62 - ETA: 10s - loss: 0.8122 - acc: 0.62 - ETA: 10s - loss: 0.8070 - acc: 0.63 - ETA: 9s - loss: 0.8029 - acc: 0.6323 - ETA: 8s - loss: 0.7984 - acc: 0.634 - ETA: 8s - loss: 0.8032 - acc: 0.631 - ETA: 7s - loss: 0.8012 - acc: 0.634 - ETA: 7s - loss: 0.7983 - acc: 0.635 - ETA: 6s - loss: 0.7920 - acc: 0.639 - ETA: 6s - loss: 0.7926 - acc: 0.638 - ETA: 5s - loss: 0.7940 - acc: 0.639 - ETA: 5s - loss: 0.7950 - acc: 0.640 - ETA: 4s - loss: 0.7954 - acc: 0.640 - ETA: 4s - loss: 0.7940 - acc: 0.640 - ETA: 3s - loss: 0.7909 - acc: 0.642 - ETA: 3s - loss: 0.7936 - acc: 0.641 - ETA: 2s - loss: 0.7934 - acc: 0.640 - ETA: 2s - loss: 0.7918 - acc: 0.641 - ETA: 1s - loss: 0.7932 - acc: 0.642 - ETA: 0s - loss: 0.7929 - acc: 0.641 - ETA: 0s - loss: 0.7929 - acc: 0.641 - 19s 4ms/step - loss: 0.7913 - acc: 0.6410 - val_loss: 0.7021 - val_acc: 0.6796\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.75019 to 0.70211, saving model to weight_MobileNet.hdf5\n",
      "Epoch 7/100\n",
      "4340/4340 [==============================] - ETA: 13s - loss: 0.7199 - acc: 0.68 - ETA: 18s - loss: 0.7572 - acc: 0.65 - ETA: 17s - loss: 0.7696 - acc: 0.64 - ETA: 15s - loss: 0.7738 - acc: 0.63 - ETA: 14s - loss: 0.7742 - acc: 0.63 - ETA: 14s - loss: 0.7734 - acc: 0.63 - ETA: 13s - loss: 0.7595 - acc: 0.64 - ETA: 12s - loss: 0.7530 - acc: 0.64 - ETA: 11s - loss: 0.7471 - acc: 0.64 - ETA: 11s - loss: 0.7400 - acc: 0.65 - ETA: 10s - loss: 0.7506 - acc: 0.64 - ETA: 10s - loss: 0.7515 - acc: 0.64 - ETA: 9s - loss: 0.7563 - acc: 0.6436 - ETA: 9s - loss: 0.7497 - acc: 0.650 - ETA: 8s - loss: 0.7510 - acc: 0.647 - ETA: 8s - loss: 0.7480 - acc: 0.651 - ETA: 7s - loss: 0.7516 - acc: 0.650 - ETA: 7s - loss: 0.7448 - acc: 0.654 - ETA: 6s - loss: 0.7511 - acc: 0.649 - ETA: 6s - loss: 0.7546 - acc: 0.649 - ETA: 5s - loss: 0.7614 - acc: 0.646 - ETA: 5s - loss: 0.7634 - acc: 0.646 - ETA: 5s - loss: 0.7591 - acc: 0.649 - ETA: 4s - loss: 0.7548 - acc: 0.652 - ETA: 4s - loss: 0.7503 - acc: 0.653 - ETA: 3s - loss: 0.7498 - acc: 0.653 - ETA: 3s - loss: 0.7477 - acc: 0.655 - ETA: 2s - loss: 0.7434 - acc: 0.657 - ETA: 2s - loss: 0.7431 - acc: 0.657 - ETA: 1s - loss: 0.7394 - acc: 0.659 - ETA: 1s - loss: 0.7431 - acc: 0.656 - ETA: 0s - loss: 0.7431 - acc: 0.655 - ETA: 0s - loss: 0.7440 - acc: 0.655 - 18s 4ms/step - loss: 0.7447 - acc: 0.6546 - val_loss: 0.6589 - val_acc: 0.7366\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.70211 to 0.65891, saving model to weight_MobileNet.hdf5\n",
      "Epoch 8/100\n",
      "4340/4340 [==============================] - ETA: 17s - loss: 0.7113 - acc: 0.68 - ETA: 15s - loss: 0.6991 - acc: 0.69 - ETA: 15s - loss: 0.6890 - acc: 0.69 - ETA: 14s - loss: 0.6806 - acc: 0.69 - ETA: 14s - loss: 0.7008 - acc: 0.68 - ETA: 13s - loss: 0.7113 - acc: 0.67 - ETA: 13s - loss: 0.7200 - acc: 0.66 - ETA: 13s - loss: 0.7093 - acc: 0.67 - ETA: 13s - loss: 0.6929 - acc: 0.68 - ETA: 12s - loss: 0.7037 - acc: 0.67 - ETA: 11s - loss: 0.7017 - acc: 0.67 - ETA: 11s - loss: 0.7058 - acc: 0.67 - ETA: 10s - loss: 0.7026 - acc: 0.68 - ETA: 10s - loss: 0.7024 - acc: 0.67 - ETA: 9s - loss: 0.7097 - acc: 0.6734 - ETA: 9s - loss: 0.7098 - acc: 0.672 - ETA: 8s - loss: 0.7125 - acc: 0.670 - ETA: 8s - loss: 0.7164 - acc: 0.666 - ETA: 7s - loss: 0.7143 - acc: 0.666 - ETA: 6s - loss: 0.7154 - acc: 0.665 - ETA: 6s - loss: 0.7118 - acc: 0.668 - ETA: 5s - loss: 0.7131 - acc: 0.668 - ETA: 5s - loss: 0.7122 - acc: 0.670 - ETA: 4s - loss: 0.7117 - acc: 0.668 - ETA: 4s - loss: 0.7105 - acc: 0.670 - ETA: 3s - loss: 0.7092 - acc: 0.672 - ETA: 3s - loss: 0.7069 - acc: 0.673 - ETA: 2s - loss: 0.7046 - acc: 0.675 - ETA: 2s - loss: 0.7043 - acc: 0.674 - ETA: 1s - loss: 0.7026 - acc: 0.675 - ETA: 1s - loss: 0.7026 - acc: 0.674 - ETA: 0s - loss: 0.7033 - acc: 0.672 - ETA: 0s - loss: 0.7030 - acc: 0.674 - 17s 4ms/step - loss: 0.7031 - acc: 0.6747 - val_loss: 0.6412 - val_acc: 0.7320\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.65891 to 0.64125, saving model to weight_MobileNet.hdf5\n",
      "Epoch 9/100\n",
      "4340/4340 [==============================] - ETA: 16s - loss: 0.6539 - acc: 0.73 - ETA: 14s - loss: 0.6896 - acc: 0.69 - ETA: 13s - loss: 0.6763 - acc: 0.69 - ETA: 12s - loss: 0.6873 - acc: 0.68 - ETA: 12s - loss: 0.6773 - acc: 0.69 - ETA: 12s - loss: 0.6853 - acc: 0.68 - ETA: 12s - loss: 0.6857 - acc: 0.68 - ETA: 12s - loss: 0.6777 - acc: 0.69 - ETA: 11s - loss: 0.6877 - acc: 0.69 - ETA: 11s - loss: 0.6831 - acc: 0.69 - ETA: 11s - loss: 0.6859 - acc: 0.69 - ETA: 11s - loss: 0.6850 - acc: 0.69 - ETA: 11s - loss: 0.6849 - acc: 0.69 - ETA: 10s - loss: 0.6828 - acc: 0.69 - ETA: 10s - loss: 0.6847 - acc: 0.69 - ETA: 9s - loss: 0.6752 - acc: 0.7021 - ETA: 9s - loss: 0.6766 - acc: 0.701 - ETA: 9s - loss: 0.6770 - acc: 0.701 - ETA: 8s - loss: 0.6749 - acc: 0.702 - ETA: 8s - loss: 0.6750 - acc: 0.700 - ETA: 7s - loss: 0.6712 - acc: 0.701 - ETA: 7s - loss: 0.6699 - acc: 0.699 - ETA: 6s - loss: 0.6733 - acc: 0.696 - ETA: 5s - loss: 0.6732 - acc: 0.696 - ETA: 5s - loss: 0.6736 - acc: 0.698 - ETA: 4s - loss: 0.6756 - acc: 0.695 - ETA: 4s - loss: 0.6755 - acc: 0.695 - ETA: 3s - loss: 0.6750 - acc: 0.695 - ETA: 2s - loss: 0.6715 - acc: 0.696 - ETA: 2s - loss: 0.6680 - acc: 0.699 - ETA: 1s - loss: 0.6668 - acc: 0.700 - ETA: 1s - loss: 0.6665 - acc: 0.699 - ETA: 0s - loss: 0.6651 - acc: 0.700 - 22s 5ms/step - loss: 0.6665 - acc: 0.6984 - val_loss: 0.5954 - val_acc: 0.7468\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.64125 to 0.59536, saving model to weight_MobileNet.hdf5\n",
      "Epoch 10/100\n",
      "4340/4340 [==============================] - ETA: 13s - loss: 0.6878 - acc: 0.71 - ETA: 13s - loss: 0.6430 - acc: 0.72 - ETA: 15s - loss: 0.6384 - acc: 0.71 - ETA: 21s - loss: 0.6404 - acc: 0.71 - ETA: 24s - loss: 0.6491 - acc: 0.70 - ETA: 25s - loss: 0.6446 - acc: 0.70 - ETA: 25s - loss: 0.6373 - acc: 0.71 - ETA: 24s - loss: 0.6336 - acc: 0.72 - ETA: 22s - loss: 0.6385 - acc: 0.72 - ETA: 20s - loss: 0.6366 - acc: 0.72 - ETA: 19s - loss: 0.6348 - acc: 0.72 - ETA: 17s - loss: 0.6355 - acc: 0.72 - ETA: 16s - loss: 0.6242 - acc: 0.72 - ETA: 14s - loss: 0.6270 - acc: 0.72 - ETA: 15s - loss: 0.6312 - acc: 0.72 - ETA: 14s - loss: 0.6345 - acc: 0.72 - ETA: 13s - loss: 0.6333 - acc: 0.72 - ETA: 12s - loss: 0.6338 - acc: 0.72 - ETA: 11s - loss: 0.6303 - acc: 0.72 - ETA: 10s - loss: 0.6321 - acc: 0.72 - ETA: 9s - loss: 0.6294 - acc: 0.7243 - ETA: 9s - loss: 0.6279 - acc: 0.724 - ETA: 8s - loss: 0.6290 - acc: 0.722 - ETA: 7s - loss: 0.6281 - acc: 0.725 - ETA: 6s - loss: 0.6264 - acc: 0.726 - ETA: 5s - loss: 0.6250 - acc: 0.727 - ETA: 4s - loss: 0.6212 - acc: 0.729 - ETA: 4s - loss: 0.6222 - acc: 0.728 - ETA: 3s - loss: 0.6219 - acc: 0.729 - ETA: 2s - loss: 0.6242 - acc: 0.727 - ETA: 1s - loss: 0.6233 - acc: 0.727 - ETA: 1s - loss: 0.6217 - acc: 0.728 - ETA: 0s - loss: 0.6231 - acc: 0.726 - 23s 5ms/step - loss: 0.6244 - acc: 0.7263 - val_loss: 0.5167 - val_acc: 0.8241\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.59536 to 0.51669, saving model to weight_MobileNet.hdf5\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4340/4340 [==============================] - ETA: 14s - loss: 0.6634 - acc: 0.72 - ETA: 16s - loss: 0.6475 - acc: 0.71 - ETA: 14s - loss: 0.6090 - acc: 0.72 - ETA: 14s - loss: 0.6015 - acc: 0.73 - ETA: 13s - loss: 0.5870 - acc: 0.74 - ETA: 13s - loss: 0.5816 - acc: 0.74 - ETA: 12s - loss: 0.5910 - acc: 0.74 - ETA: 11s - loss: 0.5959 - acc: 0.73 - ETA: 11s - loss: 0.5975 - acc: 0.73 - ETA: 10s - loss: 0.5900 - acc: 0.74 - ETA: 10s - loss: 0.5912 - acc: 0.74 - ETA: 9s - loss: 0.5868 - acc: 0.7422 - ETA: 9s - loss: 0.5870 - acc: 0.738 - ETA: 8s - loss: 0.5910 - acc: 0.735 - ETA: 8s - loss: 0.5929 - acc: 0.733 - ETA: 7s - loss: 0.5878 - acc: 0.739 - ETA: 7s - loss: 0.5877 - acc: 0.739 - ETA: 7s - loss: 0.5878 - acc: 0.738 - ETA: 6s - loss: 0.5829 - acc: 0.742 - ETA: 6s - loss: 0.5815 - acc: 0.743 - ETA: 5s - loss: 0.5794 - acc: 0.744 - ETA: 5s - loss: 0.5796 - acc: 0.746 - ETA: 4s - loss: 0.5803 - acc: 0.745 - ETA: 4s - loss: 0.5803 - acc: 0.744 - ETA: 4s - loss: 0.5776 - acc: 0.745 - ETA: 3s - loss: 0.5771 - acc: 0.745 - ETA: 3s - loss: 0.5758 - acc: 0.747 - ETA: 2s - loss: 0.5754 - acc: 0.747 - ETA: 2s - loss: 0.5736 - acc: 0.749 - ETA: 1s - loss: 0.5738 - acc: 0.750 - ETA: 1s - loss: 0.5746 - acc: 0.749 - ETA: 0s - loss: 0.5745 - acc: 0.748 - ETA: 0s - loss: 0.5735 - acc: 0.749 - 18s 4ms/step - loss: 0.5728 - acc: 0.7500 - val_loss: 0.4547 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.51669 to 0.45468, saving model to weight_MobileNet.hdf5\n",
      "Epoch 12/100\n",
      "4340/4340 [==============================] - ETA: 13s - loss: 0.5380 - acc: 0.77 - ETA: 13s - loss: 0.5034 - acc: 0.79 - ETA: 12s - loss: 0.5252 - acc: 0.77 - ETA: 13s - loss: 0.5232 - acc: 0.78 - ETA: 12s - loss: 0.5329 - acc: 0.76 - ETA: 13s - loss: 0.5505 - acc: 0.75 - ETA: 13s - loss: 0.5480 - acc: 0.75 - ETA: 12s - loss: 0.5446 - acc: 0.75 - ETA: 11s - loss: 0.5424 - acc: 0.75 - ETA: 11s - loss: 0.5425 - acc: 0.74 - ETA: 10s - loss: 0.5411 - acc: 0.75 - ETA: 10s - loss: 0.5467 - acc: 0.75 - ETA: 9s - loss: 0.5489 - acc: 0.7524 - ETA: 9s - loss: 0.5465 - acc: 0.755 - ETA: 9s - loss: 0.5473 - acc: 0.755 - ETA: 8s - loss: 0.5473 - acc: 0.757 - ETA: 8s - loss: 0.5466 - acc: 0.757 - ETA: 7s - loss: 0.5400 - acc: 0.759 - ETA: 7s - loss: 0.5409 - acc: 0.759 - ETA: 6s - loss: 0.5435 - acc: 0.760 - ETA: 6s - loss: 0.5438 - acc: 0.761 - ETA: 5s - loss: 0.5430 - acc: 0.761 - ETA: 5s - loss: 0.5407 - acc: 0.762 - ETA: 4s - loss: 0.5388 - acc: 0.764 - ETA: 4s - loss: 0.5371 - acc: 0.765 - ETA: 3s - loss: 0.5390 - acc: 0.764 - ETA: 3s - loss: 0.5401 - acc: 0.764 - ETA: 2s - loss: 0.5377 - acc: 0.765 - ETA: 2s - loss: 0.5371 - acc: 0.765 - ETA: 1s - loss: 0.5367 - acc: 0.767 - ETA: 1s - loss: 0.5355 - acc: 0.767 - ETA: 0s - loss: 0.5351 - acc: 0.768 - ETA: 0s - loss: 0.5320 - acc: 0.770 - 17s 4ms/step - loss: 0.5299 - acc: 0.7719 - val_loss: 0.4264 - val_acc: 0.8444\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.45468 to 0.42645, saving model to weight_MobileNet.hdf5\n",
      "Epoch 13/100\n",
      "4340/4340 [==============================] - ETA: 12s - loss: 0.5810 - acc: 0.75 - ETA: 13s - loss: 0.5626 - acc: 0.76 - ETA: 12s - loss: 0.5409 - acc: 0.77 - ETA: 13s - loss: 0.5318 - acc: 0.77 - ETA: 12s - loss: 0.5340 - acc: 0.77 - ETA: 12s - loss: 0.5224 - acc: 0.78 - ETA: 11s - loss: 0.5159 - acc: 0.78 - ETA: 11s - loss: 0.5101 - acc: 0.78 - ETA: 11s - loss: 0.5060 - acc: 0.78 - ETA: 10s - loss: 0.4975 - acc: 0.79 - ETA: 10s - loss: 0.4959 - acc: 0.79 - ETA: 9s - loss: 0.4875 - acc: 0.8008 - ETA: 9s - loss: 0.4921 - acc: 0.801 - ETA: 8s - loss: 0.4892 - acc: 0.801 - ETA: 8s - loss: 0.4816 - acc: 0.806 - ETA: 8s - loss: 0.4836 - acc: 0.801 - ETA: 7s - loss: 0.4863 - acc: 0.801 - ETA: 7s - loss: 0.4843 - acc: 0.803 - ETA: 6s - loss: 0.4873 - acc: 0.801 - ETA: 6s - loss: 0.4857 - acc: 0.801 - ETA: 5s - loss: 0.4843 - acc: 0.801 - ETA: 5s - loss: 0.4867 - acc: 0.799 - ETA: 4s - loss: 0.4869 - acc: 0.798 - ETA: 4s - loss: 0.4888 - acc: 0.796 - ETA: 4s - loss: 0.4912 - acc: 0.795 - ETA: 3s - loss: 0.4963 - acc: 0.793 - ETA: 3s - loss: 0.4979 - acc: 0.792 - ETA: 2s - loss: 0.4994 - acc: 0.792 - ETA: 2s - loss: 0.4998 - acc: 0.792 - ETA: 1s - loss: 0.4992 - acc: 0.792 - ETA: 1s - loss: 0.4979 - acc: 0.794 - ETA: 0s - loss: 0.4961 - acc: 0.794 - ETA: 0s - loss: 0.4942 - acc: 0.795 - 17s 4ms/step - loss: 0.4919 - acc: 0.7968 - val_loss: 0.3654 - val_acc: 0.8775\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.42645 to 0.36540, saving model to weight_MobileNet.hdf5\n",
      "Epoch 14/100\n",
      "4340/4340 [==============================] - ETA: 12s - loss: 0.4360 - acc: 0.83 - ETA: 13s - loss: 0.4518 - acc: 0.82 - ETA: 13s - loss: 0.4636 - acc: 0.82 - ETA: 13s - loss: 0.4495 - acc: 0.82 - ETA: 12s - loss: 0.4420 - acc: 0.83 - ETA: 13s - loss: 0.4485 - acc: 0.82 - ETA: 12s - loss: 0.4497 - acc: 0.82 - ETA: 12s - loss: 0.4454 - acc: 0.82 - ETA: 11s - loss: 0.4417 - acc: 0.82 - ETA: 10s - loss: 0.4469 - acc: 0.82 - ETA: 10s - loss: 0.4478 - acc: 0.82 - ETA: 10s - loss: 0.4510 - acc: 0.82 - ETA: 9s - loss: 0.4480 - acc: 0.8305 - ETA: 9s - loss: 0.4505 - acc: 0.828 - ETA: 8s - loss: 0.4447 - acc: 0.830 - ETA: 8s - loss: 0.4387 - acc: 0.833 - ETA: 7s - loss: 0.4424 - acc: 0.828 - ETA: 7s - loss: 0.4428 - acc: 0.828 - ETA: 6s - loss: 0.4483 - acc: 0.824 - ETA: 6s - loss: 0.4494 - acc: 0.823 - ETA: 6s - loss: 0.4484 - acc: 0.823 - ETA: 5s - loss: 0.4487 - acc: 0.825 - ETA: 5s - loss: 0.4451 - acc: 0.827 - ETA: 4s - loss: 0.4443 - acc: 0.828 - ETA: 4s - loss: 0.4468 - acc: 0.825 - ETA: 3s - loss: 0.4471 - acc: 0.824 - ETA: 3s - loss: 0.4479 - acc: 0.824 - ETA: 2s - loss: 0.4435 - acc: 0.827 - ETA: 2s - loss: 0.4407 - acc: 0.828 - ETA: 1s - loss: 0.4409 - acc: 0.827 - ETA: 1s - loss: 0.4378 - acc: 0.829 - ETA: 0s - loss: 0.4392 - acc: 0.829 - ETA: 0s - loss: 0.4391 - acc: 0.829 - 17s 4ms/step - loss: 0.4407 - acc: 0.8270 - val_loss: 0.4534 - val_acc: 0.8241\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.36540\n",
      "Epoch 15/100\n",
      "4340/4340 [==============================] - ETA: 19s - loss: 0.5148 - acc: 0.78 - ETA: 15s - loss: 0.5666 - acc: 0.75 - ETA: 14s - loss: 0.6037 - acc: 0.72 - ETA: 14s - loss: 0.5845 - acc: 0.73 - ETA: 13s - loss: 0.5632 - acc: 0.75 - ETA: 13s - loss: 0.5415 - acc: 0.75 - ETA: 12s - loss: 0.5131 - acc: 0.78 - ETA: 12s - loss: 0.5078 - acc: 0.78 - ETA: 12s - loss: 0.5011 - acc: 0.78 - ETA: 12s - loss: 0.4793 - acc: 0.79 - ETA: 12s - loss: 0.4690 - acc: 0.80 - ETA: 14s - loss: 0.4613 - acc: 0.80 - ETA: 14s - loss: 0.4566 - acc: 0.80 - ETA: 13s - loss: 0.4531 - acc: 0.81 - ETA: 12s - loss: 0.4587 - acc: 0.80 - ETA: 11s - loss: 0.4647 - acc: 0.80 - ETA: 10s - loss: 0.4690 - acc: 0.80 - ETA: 10s - loss: 0.4686 - acc: 0.80 - ETA: 9s - loss: 0.4684 - acc: 0.8051 - ETA: 8s - loss: 0.4698 - acc: 0.802 - ETA: 7s - loss: 0.4672 - acc: 0.801 - ETA: 7s - loss: 0.4664 - acc: 0.802 - ETA: 6s - loss: 0.4686 - acc: 0.801 - ETA: 5s - loss: 0.4668 - acc: 0.802 - ETA: 5s - loss: 0.4645 - acc: 0.804 - ETA: 4s - loss: 0.4596 - acc: 0.809 - ETA: 4s - loss: 0.4573 - acc: 0.811 - ETA: 3s - loss: 0.4582 - acc: 0.809 - ETA: 3s - loss: 0.4562 - acc: 0.812 - ETA: 2s - loss: 0.4536 - acc: 0.813 - ETA: 1s - loss: 0.4521 - acc: 0.815 - ETA: 1s - loss: 0.4526 - acc: 0.813 - ETA: 0s - loss: 0.4508 - acc: 0.815 - 23s 5ms/step - loss: 0.4480 - acc: 0.8168 - val_loss: 0.2732 - val_acc: 0.9236\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.36540 to 0.27318, saving model to weight_MobileNet.hdf5\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4340/4340 [==============================] - ETA: 15s - loss: 0.4102 - acc: 0.80 - ETA: 16s - loss: 0.4031 - acc: 0.83 - ETA: 16s - loss: 0.4080 - acc: 0.83 - ETA: 15s - loss: 0.4091 - acc: 0.83 - ETA: 14s - loss: 0.4014 - acc: 0.85 - ETA: 14s - loss: 0.4281 - acc: 0.84 - ETA: 14s - loss: 0.4776 - acc: 0.82 - ETA: 13s - loss: 0.4972 - acc: 0.80 - ETA: 13s - loss: 0.5054 - acc: 0.80 - ETA: 13s - loss: 0.4888 - acc: 0.81 - ETA: 13s - loss: 0.4839 - acc: 0.81 - ETA: 12s - loss: 0.4742 - acc: 0.81 - ETA: 12s - loss: 0.4625 - acc: 0.82 - ETA: 11s - loss: 0.4594 - acc: 0.82 - ETA: 10s - loss: 0.4543 - acc: 0.82 - ETA: 10s - loss: 0.4563 - acc: 0.82 - ETA: 9s - loss: 0.4578 - acc: 0.8226 - ETA: 9s - loss: 0.4528 - acc: 0.822 - ETA: 8s - loss: 0.4471 - acc: 0.824 - ETA: 7s - loss: 0.4444 - acc: 0.827 - ETA: 7s - loss: 0.4469 - acc: 0.825 - ETA: 6s - loss: 0.4448 - acc: 0.827 - ETA: 6s - loss: 0.4472 - acc: 0.825 - ETA: 5s - loss: 0.4476 - acc: 0.826 - ETA: 4s - loss: 0.4421 - acc: 0.828 - ETA: 4s - loss: 0.4396 - acc: 0.830 - ETA: 3s - loss: 0.4355 - acc: 0.832 - ETA: 3s - loss: 0.4312 - acc: 0.834 - ETA: 2s - loss: 0.4267 - acc: 0.835 - ETA: 2s - loss: 0.4221 - acc: 0.837 - ETA: 1s - loss: 0.4184 - acc: 0.840 - ETA: 1s - loss: 0.4166 - acc: 0.841 - ETA: 0s - loss: 0.4159 - acc: 0.841 - 22s 5ms/step - loss: 0.4171 - acc: 0.8417 - val_loss: 0.2635 - val_acc: 0.9208\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.27318 to 0.26351, saving model to weight_MobileNet.hdf5\n",
      "Epoch 17/100\n",
      "4340/4340 [==============================] - ETA: 15s - loss: 0.2933 - acc: 0.92 - ETA: 18s - loss: 0.2876 - acc: 0.92 - ETA: 16s - loss: 0.2973 - acc: 0.90 - ETA: 16s - loss: 0.3233 - acc: 0.89 - ETA: 15s - loss: 0.3192 - acc: 0.89 - ETA: 15s - loss: 0.3295 - acc: 0.89 - ETA: 15s - loss: 0.3352 - acc: 0.89 - ETA: 14s - loss: 0.3280 - acc: 0.89 - ETA: 14s - loss: 0.3224 - acc: 0.89 - ETA: 13s - loss: 0.3154 - acc: 0.89 - ETA: 12s - loss: 0.3166 - acc: 0.89 - ETA: 12s - loss: 0.3134 - acc: 0.89 - ETA: 11s - loss: 0.3129 - acc: 0.89 - ETA: 10s - loss: 0.3179 - acc: 0.89 - ETA: 10s - loss: 0.3154 - acc: 0.89 - ETA: 9s - loss: 0.3176 - acc: 0.8877 - ETA: 9s - loss: 0.3116 - acc: 0.891 - ETA: 8s - loss: 0.3120 - acc: 0.891 - ETA: 8s - loss: 0.3133 - acc: 0.889 - ETA: 7s - loss: 0.3158 - acc: 0.888 - ETA: 6s - loss: 0.3143 - acc: 0.889 - ETA: 6s - loss: 0.3174 - acc: 0.887 - ETA: 6s - loss: 0.3180 - acc: 0.886 - ETA: 5s - loss: 0.3180 - acc: 0.886 - ETA: 5s - loss: 0.3198 - acc: 0.886 - ETA: 4s - loss: 0.3185 - acc: 0.887 - ETA: 4s - loss: 0.3179 - acc: 0.888 - ETA: 3s - loss: 0.3164 - acc: 0.888 - ETA: 2s - loss: 0.3145 - acc: 0.889 - ETA: 2s - loss: 0.3144 - acc: 0.890 - ETA: 1s - loss: 0.3150 - acc: 0.888 - ETA: 1s - loss: 0.3171 - acc: 0.886 - ETA: 0s - loss: 0.3184 - acc: 0.884 - 22s 5ms/step - loss: 0.3199 - acc: 0.8839 - val_loss: 0.5162 - val_acc: 0.8048\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.26351\n",
      "Epoch 18/100\n",
      "4340/4340 [==============================] - ETA: 22s - loss: 0.4942 - acc: 0.77 - ETA: 18s - loss: 0.4927 - acc: 0.79 - ETA: 22s - loss: 0.4533 - acc: 0.80 - ETA: 24s - loss: 0.4188 - acc: 0.82 - ETA: 25s - loss: 0.4046 - acc: 0.83 - ETA: 26s - loss: 0.3944 - acc: 0.84 - ETA: 26s - loss: 0.3860 - acc: 0.84 - ETA: 25s - loss: 0.3723 - acc: 0.85 - ETA: 24s - loss: 0.3576 - acc: 0.86 - ETA: 23s - loss: 0.3464 - acc: 0.86 - ETA: 22s - loss: 0.3402 - acc: 0.87 - ETA: 21s - loss: 0.3402 - acc: 0.86 - ETA: 21s - loss: 0.3373 - acc: 0.87 - ETA: 19s - loss: 0.3478 - acc: 0.86 - ETA: 18s - loss: 0.3737 - acc: 0.85 - ETA: 17s - loss: 0.4272 - acc: 0.84 - ETA: 16s - loss: 0.4532 - acc: 0.82 - ETA: 14s - loss: 0.4573 - acc: 0.82 - ETA: 13s - loss: 0.4548 - acc: 0.82 - ETA: 12s - loss: 0.4524 - acc: 0.82 - ETA: 11s - loss: 0.4457 - acc: 0.83 - ETA: 10s - loss: 0.4406 - acc: 0.83 - ETA: 9s - loss: 0.4391 - acc: 0.8329 - ETA: 8s - loss: 0.4376 - acc: 0.832 - ETA: 7s - loss: 0.4340 - acc: 0.834 - ETA: 6s - loss: 0.4305 - acc: 0.835 - ETA: 5s - loss: 0.4270 - acc: 0.837 - ETA: 4s - loss: 0.4237 - acc: 0.839 - ETA: 3s - loss: 0.4213 - acc: 0.840 - ETA: 3s - loss: 0.4189 - acc: 0.841 - ETA: 2s - loss: 0.4162 - acc: 0.843 - ETA: 1s - loss: 0.4152 - acc: 0.842 - ETA: 0s - loss: 0.4113 - acc: 0.844 - 28s 7ms/step - loss: 0.4090 - acc: 0.8456 - val_loss: 0.2114 - val_acc: 0.9438\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.26351 to 0.21145, saving model to weight_MobileNet.hdf5\n",
      "Epoch 19/100\n",
      "4340/4340 [==============================] - ETA: 29s - loss: 0.3110 - acc: 0.86 - ETA: 27s - loss: 0.2709 - acc: 0.91 - ETA: 26s - loss: 0.2683 - acc: 0.90 - ETA: 22s - loss: 0.2840 - acc: 0.90 - ETA: 20s - loss: 0.2765 - acc: 0.90 - ETA: 18s - loss: 0.2728 - acc: 0.90 - ETA: 17s - loss: 0.2690 - acc: 0.91 - ETA: 16s - loss: 0.2665 - acc: 0.91 - ETA: 14s - loss: 0.2705 - acc: 0.91 - ETA: 14s - loss: 0.2659 - acc: 0.91 - ETA: 13s - loss: 0.2786 - acc: 0.91 - ETA: 12s - loss: 0.2926 - acc: 0.90 - ETA: 11s - loss: 0.3019 - acc: 0.89 - ETA: 10s - loss: 0.3189 - acc: 0.88 - ETA: 10s - loss: 0.3257 - acc: 0.88 - ETA: 9s - loss: 0.3225 - acc: 0.8887 - ETA: 9s - loss: 0.3204 - acc: 0.889 - ETA: 8s - loss: 0.3185 - acc: 0.891 - ETA: 8s - loss: 0.3162 - acc: 0.892 - ETA: 7s - loss: 0.3169 - acc: 0.893 - ETA: 6s - loss: 0.3171 - acc: 0.893 - ETA: 6s - loss: 0.3240 - acc: 0.890 - ETA: 5s - loss: 0.3302 - acc: 0.888 - ETA: 5s - loss: 0.3311 - acc: 0.887 - ETA: 4s - loss: 0.3303 - acc: 0.887 - ETA: 4s - loss: 0.3276 - acc: 0.890 - ETA: 3s - loss: 0.3277 - acc: 0.889 - ETA: 3s - loss: 0.3253 - acc: 0.891 - ETA: 2s - loss: 0.3216 - acc: 0.892 - ETA: 2s - loss: 0.3195 - acc: 0.893 - ETA: 1s - loss: 0.3152 - acc: 0.895 - ETA: 1s - loss: 0.3126 - acc: 0.896 - ETA: 0s - loss: 0.3119 - acc: 0.894 - 19s 4ms/step - loss: 0.3106 - acc: 0.8947 - val_loss: 0.1784 - val_acc: 0.9576\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.21145 to 0.17844, saving model to weight_MobileNet.hdf5\n",
      "Epoch 20/100\n",
      "4340/4340 [==============================] - ETA: 13s - loss: 0.2824 - acc: 0.92 - ETA: 15s - loss: 0.2521 - acc: 0.92 - ETA: 14s - loss: 0.2346 - acc: 0.92 - ETA: 14s - loss: 0.2264 - acc: 0.93 - ETA: 13s - loss: 0.2387 - acc: 0.92 - ETA: 13s - loss: 0.2330 - acc: 0.93 - ETA: 12s - loss: 0.2306 - acc: 0.93 - ETA: 12s - loss: 0.2334 - acc: 0.92 - ETA: 12s - loss: 0.2321 - acc: 0.92 - ETA: 11s - loss: 0.2333 - acc: 0.92 - ETA: 11s - loss: 0.2393 - acc: 0.92 - ETA: 10s - loss: 0.2441 - acc: 0.92 - ETA: 10s - loss: 0.2437 - acc: 0.92 - ETA: 10s - loss: 0.2411 - acc: 0.92 - ETA: 10s - loss: 0.2402 - acc: 0.92 - ETA: 10s - loss: 0.2402 - acc: 0.92 - ETA: 9s - loss: 0.2388 - acc: 0.9233 - ETA: 8s - loss: 0.2402 - acc: 0.922 - ETA: 8s - loss: 0.2418 - acc: 0.921 - ETA: 7s - loss: 0.2413 - acc: 0.921 - ETA: 7s - loss: 0.2421 - acc: 0.923 - ETA: 6s - loss: 0.2386 - acc: 0.925 - ETA: 5s - loss: 0.2400 - acc: 0.923 - ETA: 5s - loss: 0.2362 - acc: 0.926 - ETA: 4s - loss: 0.2341 - acc: 0.926 - ETA: 4s - loss: 0.2334 - acc: 0.926 - ETA: 3s - loss: 0.2332 - acc: 0.926 - ETA: 3s - loss: 0.2311 - acc: 0.927 - ETA: 2s - loss: 0.2311 - acc: 0.927 - ETA: 2s - loss: 0.2293 - acc: 0.928 - ETA: 1s - loss: 0.2301 - acc: 0.927 - ETA: 1s - loss: 0.2316 - acc: 0.927 - ETA: 0s - loss: 0.2307 - acc: 0.927 - 19s 4ms/step - loss: 0.2295 - acc: 0.9281 - val_loss: 0.1822 - val_acc: 0.9392\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.17844\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4340/4340 [==============================] - ETA: 15s - loss: 0.2239 - acc: 0.91 - ETA: 14s - loss: 0.2050 - acc: 0.94 - ETA: 13s - loss: 0.1984 - acc: 0.94 - ETA: 13s - loss: 0.2217 - acc: 0.93 - ETA: 12s - loss: 0.2256 - acc: 0.93 - ETA: 13s - loss: 0.2274 - acc: 0.93 - ETA: 12s - loss: 0.2162 - acc: 0.93 - ETA: 12s - loss: 0.2118 - acc: 0.94 - ETA: 11s - loss: 0.2067 - acc: 0.94 - ETA: 11s - loss: 0.2094 - acc: 0.94 - ETA: 10s - loss: 0.2162 - acc: 0.93 - ETA: 10s - loss: 0.2180 - acc: 0.93 - ETA: 9s - loss: 0.2149 - acc: 0.9375 - ETA: 9s - loss: 0.2121 - acc: 0.937 - ETA: 9s - loss: 0.2141 - acc: 0.935 - ETA: 8s - loss: 0.2280 - acc: 0.931 - ETA: 8s - loss: 0.2356 - acc: 0.925 - ETA: 8s - loss: 0.2445 - acc: 0.921 - ETA: 7s - loss: 0.2510 - acc: 0.919 - ETA: 7s - loss: 0.2610 - acc: 0.914 - ETA: 6s - loss: 0.2677 - acc: 0.909 - ETA: 6s - loss: 0.2715 - acc: 0.908 - ETA: 5s - loss: 0.2692 - acc: 0.909 - ETA: 5s - loss: 0.2664 - acc: 0.910 - ETA: 4s - loss: 0.2635 - acc: 0.911 - ETA: 3s - loss: 0.2605 - acc: 0.913 - ETA: 3s - loss: 0.2570 - acc: 0.915 - ETA: 2s - loss: 0.2535 - acc: 0.916 - ETA: 2s - loss: 0.2509 - acc: 0.918 - ETA: 1s - loss: 0.2479 - acc: 0.920 - ETA: 1s - loss: 0.2477 - acc: 0.919 - ETA: 0s - loss: 0.2453 - acc: 0.920 - ETA: 0s - loss: 0.2437 - acc: 0.921 - 18s 4ms/step - loss: 0.2414 - acc: 0.9226 - val_loss: 0.1152 - val_acc: 0.9751\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.17844 to 0.11516, saving model to weight_MobileNet.hdf5\n",
      "Epoch 22/100\n",
      "4340/4340 [==============================] - ETA: 14s - loss: 0.1812 - acc: 0.93 - ETA: 13s - loss: 0.1887 - acc: 0.94 - ETA: 13s - loss: 0.1746 - acc: 0.95 - ETA: 14s - loss: 0.1713 - acc: 0.95 - ETA: 13s - loss: 0.1728 - acc: 0.95 - ETA: 12s - loss: 0.1816 - acc: 0.95 - ETA: 11s - loss: 0.1857 - acc: 0.94 - ETA: 11s - loss: 0.1918 - acc: 0.94 - ETA: 10s - loss: 0.1880 - acc: 0.94 - ETA: 10s - loss: 0.1900 - acc: 0.94 - ETA: 10s - loss: 0.1904 - acc: 0.94 - ETA: 9s - loss: 0.1905 - acc: 0.9447 - ETA: 9s - loss: 0.1880 - acc: 0.944 - ETA: 9s - loss: 0.1849 - acc: 0.944 - ETA: 8s - loss: 0.1873 - acc: 0.942 - ETA: 8s - loss: 0.1845 - acc: 0.944 - ETA: 7s - loss: 0.1816 - acc: 0.945 - ETA: 7s - loss: 0.1817 - acc: 0.945 - ETA: 6s - loss: 0.1809 - acc: 0.945 - ETA: 6s - loss: 0.1802 - acc: 0.945 - ETA: 5s - loss: 0.1804 - acc: 0.945 - ETA: 5s - loss: 0.1796 - acc: 0.945 - ETA: 5s - loss: 0.1793 - acc: 0.946 - ETA: 4s - loss: 0.1791 - acc: 0.946 - ETA: 4s - loss: 0.1779 - acc: 0.947 - ETA: 3s - loss: 0.1771 - acc: 0.947 - ETA: 3s - loss: 0.1765 - acc: 0.947 - ETA: 2s - loss: 0.1762 - acc: 0.947 - ETA: 2s - loss: 0.1758 - acc: 0.948 - ETA: 1s - loss: 0.1762 - acc: 0.947 - ETA: 1s - loss: 0.1751 - acc: 0.948 - ETA: 0s - loss: 0.1766 - acc: 0.948 - ETA: 0s - loss: 0.1760 - acc: 0.949 - 17s 4ms/step - loss: 0.1750 - acc: 0.9498 - val_loss: 0.0937 - val_acc: 0.9797\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.11516 to 0.09372, saving model to weight_MobileNet.hdf5\n",
      "Epoch 23/100\n",
      "4340/4340 [==============================] - ETA: 25s - loss: 0.1943 - acc: 0.92 - ETA: 23s - loss: 0.1701 - acc: 0.94 - ETA: 22s - loss: 0.1485 - acc: 0.95 - ETA: 22s - loss: 0.1440 - acc: 0.96 - ETA: 21s - loss: 0.1388 - acc: 0.96 - ETA: 19s - loss: 0.1432 - acc: 0.96 - ETA: 19s - loss: 0.1516 - acc: 0.95 - ETA: 19s - loss: 0.1518 - acc: 0.95 - ETA: 18s - loss: 0.1542 - acc: 0.95 - ETA: 18s - loss: 0.1609 - acc: 0.95 - ETA: 18s - loss: 0.1592 - acc: 0.95 - ETA: 17s - loss: 0.1643 - acc: 0.94 - ETA: 16s - loss: 0.1645 - acc: 0.94 - ETA: 15s - loss: 0.1653 - acc: 0.94 - ETA: 15s - loss: 0.1632 - acc: 0.94 - ETA: 14s - loss: 0.1621 - acc: 0.94 - ETA: 13s - loss: 0.1608 - acc: 0.95 - ETA: 12s - loss: 0.1598 - acc: 0.95 - ETA: 12s - loss: 0.1596 - acc: 0.95 - ETA: 11s - loss: 0.1602 - acc: 0.95 - ETA: 10s - loss: 0.1592 - acc: 0.95 - ETA: 9s - loss: 0.1604 - acc: 0.9510 - ETA: 8s - loss: 0.1608 - acc: 0.951 - ETA: 7s - loss: 0.1602 - acc: 0.951 - ETA: 7s - loss: 0.1582 - acc: 0.952 - ETA: 6s - loss: 0.1570 - acc: 0.953 - ETA: 5s - loss: 0.1557 - acc: 0.953 - ETA: 4s - loss: 0.1553 - acc: 0.953 - ETA: 3s - loss: 0.1541 - acc: 0.953 - ETA: 3s - loss: 0.1553 - acc: 0.953 - ETA: 2s - loss: 0.1546 - acc: 0.953 - ETA: 1s - loss: 0.1529 - acc: 0.954 - ETA: 0s - loss: 0.1526 - acc: 0.954 - 27s 6ms/step - loss: 0.1524 - acc: 0.9551 - val_loss: 0.0981 - val_acc: 0.9724\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.09372\n",
      "Epoch 24/100\n",
      "4340/4340 [==============================] - ETA: 15s - loss: 0.1192 - acc: 0.98 - ETA: 15s - loss: 0.1380 - acc: 0.95 - ETA: 14s - loss: 0.1440 - acc: 0.95 - ETA: 14s - loss: 0.1405 - acc: 0.96 - ETA: 15s - loss: 0.1346 - acc: 0.96 - ETA: 14s - loss: 0.1339 - acc: 0.96 - ETA: 13s - loss: 0.1351 - acc: 0.96 - ETA: 12s - loss: 0.1323 - acc: 0.96 - ETA: 12s - loss: 0.1294 - acc: 0.96 - ETA: 12s - loss: 0.1261 - acc: 0.96 - ETA: 11s - loss: 0.1253 - acc: 0.96 - ETA: 11s - loss: 0.1271 - acc: 0.96 - ETA: 10s - loss: 0.1291 - acc: 0.96 - ETA: 10s - loss: 0.1276 - acc: 0.96 - ETA: 9s - loss: 0.1271 - acc: 0.9693 - ETA: 9s - loss: 0.1259 - acc: 0.969 - ETA: 8s - loss: 0.1283 - acc: 0.968 - ETA: 8s - loss: 0.1264 - acc: 0.970 - ETA: 7s - loss: 0.1283 - acc: 0.967 - ETA: 7s - loss: 0.1298 - acc: 0.966 - ETA: 6s - loss: 0.1280 - acc: 0.967 - ETA: 6s - loss: 0.1257 - acc: 0.968 - ETA: 6s - loss: 0.1247 - acc: 0.969 - ETA: 5s - loss: 0.1237 - acc: 0.970 - ETA: 4s - loss: 0.1239 - acc: 0.970 - ETA: 4s - loss: 0.1232 - acc: 0.971 - ETA: 3s - loss: 0.1231 - acc: 0.971 - ETA: 3s - loss: 0.1225 - acc: 0.971 - ETA: 2s - loss: 0.1220 - acc: 0.971 - ETA: 2s - loss: 0.1220 - acc: 0.971 - ETA: 1s - loss: 0.1210 - acc: 0.972 - ETA: 1s - loss: 0.1209 - acc: 0.971 - ETA: 0s - loss: 0.1201 - acc: 0.971 - 20s 5ms/step - loss: 0.1205 - acc: 0.9710 - val_loss: 0.0633 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.09372 to 0.06325, saving model to weight_MobileNet.hdf5\n",
      "Epoch 25/100\n",
      "4340/4340 [==============================] - ETA: 13s - loss: 0.1233 - acc: 0.96 - ETA: 16s - loss: 0.1318 - acc: 0.95 - ETA: 14s - loss: 0.1163 - acc: 0.96 - ETA: 16s - loss: 0.1234 - acc: 0.95 - ETA: 17s - loss: 0.1261 - acc: 0.95 - ETA: 18s - loss: 0.1457 - acc: 0.95 - ETA: 18s - loss: 0.1448 - acc: 0.95 - ETA: 17s - loss: 0.1428 - acc: 0.95 - ETA: 17s - loss: 0.1373 - acc: 0.95 - ETA: 17s - loss: 0.1344 - acc: 0.96 - ETA: 16s - loss: 0.1326 - acc: 0.96 - ETA: 15s - loss: 0.1283 - acc: 0.96 - ETA: 15s - loss: 0.1260 - acc: 0.96 - ETA: 14s - loss: 0.1240 - acc: 0.96 - ETA: 13s - loss: 0.1220 - acc: 0.96 - ETA: 12s - loss: 0.1237 - acc: 0.96 - ETA: 12s - loss: 0.1233 - acc: 0.96 - ETA: 11s - loss: 0.1224 - acc: 0.96 - ETA: 10s - loss: 0.1215 - acc: 0.96 - ETA: 9s - loss: 0.1198 - acc: 0.9688 - ETA: 8s - loss: 0.1206 - acc: 0.969 - ETA: 8s - loss: 0.1221 - acc: 0.969 - ETA: 7s - loss: 0.1243 - acc: 0.967 - ETA: 6s - loss: 0.1262 - acc: 0.965 - ETA: 5s - loss: 0.1281 - acc: 0.963 - ETA: 5s - loss: 0.1295 - acc: 0.962 - ETA: 4s - loss: 0.1308 - acc: 0.962 - ETA: 3s - loss: 0.1303 - acc: 0.963 - ETA: 3s - loss: 0.1283 - acc: 0.963 - ETA: 2s - loss: 0.1274 - acc: 0.964 - ETA: 1s - loss: 0.1267 - acc: 0.964 - ETA: 1s - loss: 0.1252 - acc: 0.965 - ETA: 0s - loss: 0.1245 - acc: 0.965 - 22s 5ms/step - loss: 0.1241 - acc: 0.9659 - val_loss: 0.0705 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.06325\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4340/4340 [==============================] - ETA: 21s - loss: 0.1194 - acc: 0.96 - ETA: 28s - loss: 0.0946 - acc: 0.96 - ETA: 25s - loss: 0.0886 - acc: 0.97 - ETA: 22s - loss: 0.0847 - acc: 0.97 - ETA: 20s - loss: 0.0827 - acc: 0.97 - ETA: 18s - loss: 0.0863 - acc: 0.98 - ETA: 17s - loss: 0.0869 - acc: 0.97 - ETA: 16s - loss: 0.0881 - acc: 0.97 - ETA: 15s - loss: 0.0909 - acc: 0.97 - ETA: 14s - loss: 0.0953 - acc: 0.97 - ETA: 13s - loss: 0.0956 - acc: 0.97 - ETA: 12s - loss: 0.0974 - acc: 0.97 - ETA: 12s - loss: 0.1041 - acc: 0.97 - ETA: 11s - loss: 0.1046 - acc: 0.97 - ETA: 10s - loss: 0.1020 - acc: 0.97 - ETA: 9s - loss: 0.1000 - acc: 0.9775 - ETA: 9s - loss: 0.0995 - acc: 0.977 - ETA: 8s - loss: 0.0985 - acc: 0.978 - ETA: 8s - loss: 0.1003 - acc: 0.977 - ETA: 7s - loss: 0.1003 - acc: 0.976 - ETA: 7s - loss: 0.0989 - acc: 0.977 - ETA: 6s - loss: 0.0979 - acc: 0.977 - ETA: 5s - loss: 0.0989 - acc: 0.976 - ETA: 5s - loss: 0.0992 - acc: 0.976 - ETA: 4s - loss: 0.0988 - acc: 0.976 - ETA: 4s - loss: 0.1005 - acc: 0.975 - ETA: 3s - loss: 0.0999 - acc: 0.976 - ETA: 3s - loss: 0.1007 - acc: 0.974 - ETA: 2s - loss: 0.1007 - acc: 0.974 - ETA: 2s - loss: 0.1007 - acc: 0.975 - ETA: 1s - loss: 0.0998 - acc: 0.975 - ETA: 0s - loss: 0.0985 - acc: 0.975 - ETA: 0s - loss: 0.0976 - acc: 0.976 - 19s 4ms/step - loss: 0.0963 - acc: 0.9763 - val_loss: 0.0493 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.06325 to 0.04930, saving model to weight_MobileNet.hdf5\n",
      "Epoch 27/100\n",
      "4340/4340 [==============================] - ETA: 14s - loss: 0.1161 - acc: 0.96 - ETA: 13s - loss: 0.0963 - acc: 0.98 - ETA: 14s - loss: 0.1031 - acc: 0.97 - ETA: 13s - loss: 0.0943 - acc: 0.98 - ETA: 12s - loss: 0.0860 - acc: 0.98 - ETA: 12s - loss: 0.0832 - acc: 0.98 - ETA: 11s - loss: 0.0858 - acc: 0.98 - ETA: 11s - loss: 0.0853 - acc: 0.98 - ETA: 11s - loss: 0.0837 - acc: 0.98 - ETA: 10s - loss: 0.0823 - acc: 0.98 - ETA: 10s - loss: 0.0839 - acc: 0.97 - ETA: 9s - loss: 0.0849 - acc: 0.9792 - ETA: 9s - loss: 0.0853 - acc: 0.978 - ETA: 8s - loss: 0.0855 - acc: 0.977 - ETA: 8s - loss: 0.0865 - acc: 0.976 - ETA: 7s - loss: 0.0869 - acc: 0.976 - ETA: 7s - loss: 0.0870 - acc: 0.976 - ETA: 7s - loss: 0.0868 - acc: 0.977 - ETA: 6s - loss: 0.0866 - acc: 0.977 - ETA: 6s - loss: 0.0863 - acc: 0.978 - ETA: 5s - loss: 0.0868 - acc: 0.978 - ETA: 5s - loss: 0.0870 - acc: 0.978 - ETA: 4s - loss: 0.0869 - acc: 0.977 - ETA: 4s - loss: 0.0878 - acc: 0.977 - ETA: 3s - loss: 0.0892 - acc: 0.977 - ETA: 3s - loss: 0.0893 - acc: 0.977 - ETA: 3s - loss: 0.0885 - acc: 0.977 - ETA: 2s - loss: 0.0884 - acc: 0.977 - ETA: 2s - loss: 0.0882 - acc: 0.976 - ETA: 1s - loss: 0.0887 - acc: 0.976 - ETA: 1s - loss: 0.0893 - acc: 0.976 - ETA: 0s - loss: 0.0902 - acc: 0.976 - ETA: 0s - loss: 0.0897 - acc: 0.975 - 20s 5ms/step - loss: 0.0901 - acc: 0.9758 - val_loss: 0.0387 - val_acc: 0.9899\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.04930 to 0.03866, saving model to weight_MobileNet.hdf5\n",
      "Epoch 28/100\n",
      "4340/4340 [==============================] - ETA: 15s - loss: 0.0645 - acc: 0.97 - ETA: 17s - loss: 0.0630 - acc: 0.98 - ETA: 18s - loss: 0.0668 - acc: 0.98 - ETA: 16s - loss: 0.0655 - acc: 0.98 - ETA: 15s - loss: 0.0631 - acc: 0.98 - ETA: 14s - loss: 0.0617 - acc: 0.98 - ETA: 14s - loss: 0.0672 - acc: 0.98 - ETA: 13s - loss: 0.0672 - acc: 0.98 - ETA: 13s - loss: 0.0656 - acc: 0.98 - ETA: 12s - loss: 0.0657 - acc: 0.98 - ETA: 11s - loss: 0.0654 - acc: 0.98 - ETA: 11s - loss: 0.0634 - acc: 0.98 - ETA: 10s - loss: 0.0619 - acc: 0.98 - ETA: 10s - loss: 0.0625 - acc: 0.98 - ETA: 9s - loss: 0.0617 - acc: 0.9896 - ETA: 9s - loss: 0.0625 - acc: 0.989 - ETA: 9s - loss: 0.0625 - acc: 0.989 - ETA: 8s - loss: 0.0618 - acc: 0.989 - ETA: 8s - loss: 0.0623 - acc: 0.989 - ETA: 7s - loss: 0.0610 - acc: 0.989 - ETA: 7s - loss: 0.0615 - acc: 0.989 - ETA: 6s - loss: 0.0613 - acc: 0.989 - ETA: 5s - loss: 0.0622 - acc: 0.988 - ETA: 5s - loss: 0.0637 - acc: 0.987 - ETA: 4s - loss: 0.0653 - acc: 0.987 - ETA: 4s - loss: 0.0665 - acc: 0.985 - ETA: 3s - loss: 0.0666 - acc: 0.985 - ETA: 3s - loss: 0.0676 - acc: 0.985 - ETA: 2s - loss: 0.0668 - acc: 0.985 - ETA: 2s - loss: 0.0659 - acc: 0.986 - ETA: 1s - loss: 0.0669 - acc: 0.986 - ETA: 1s - loss: 0.0672 - acc: 0.986 - ETA: 0s - loss: 0.0679 - acc: 0.986 - 22s 5ms/step - loss: 0.0698 - acc: 0.9859 - val_loss: 0.0413 - val_acc: 0.9871\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.03866\n",
      "Epoch 29/100\n",
      "4340/4340 [==============================] - ETA: 13s - loss: 0.1528 - acc: 0.94 - ETA: 15s - loss: 0.1057 - acc: 0.96 - ETA: 13s - loss: 0.1092 - acc: 0.96 - ETA: 14s - loss: 0.0930 - acc: 0.97 - ETA: 13s - loss: 0.0874 - acc: 0.97 - ETA: 12s - loss: 0.0859 - acc: 0.97 - ETA: 12s - loss: 0.0834 - acc: 0.97 - ETA: 11s - loss: 0.0780 - acc: 0.97 - ETA: 11s - loss: 0.0784 - acc: 0.98 - ETA: 11s - loss: 0.0764 - acc: 0.98 - ETA: 10s - loss: 0.0736 - acc: 0.98 - ETA: 10s - loss: 0.0754 - acc: 0.98 - ETA: 9s - loss: 0.0742 - acc: 0.9814 - ETA: 9s - loss: 0.0752 - acc: 0.981 - ETA: 8s - loss: 0.0745 - acc: 0.981 - ETA: 8s - loss: 0.0737 - acc: 0.981 - ETA: 7s - loss: 0.0747 - acc: 0.980 - ETA: 7s - loss: 0.0741 - acc: 0.980 - ETA: 7s - loss: 0.0729 - acc: 0.981 - ETA: 6s - loss: 0.0729 - acc: 0.981 - ETA: 6s - loss: 0.0731 - acc: 0.981 - ETA: 5s - loss: 0.0719 - acc: 0.981 - ETA: 5s - loss: 0.0716 - acc: 0.982 - ETA: 4s - loss: 0.0743 - acc: 0.981 - ETA: 4s - loss: 0.0795 - acc: 0.978 - ETA: 3s - loss: 0.0945 - acc: 0.970 - ETA: 3s - loss: 0.1480 - acc: 0.959 - ETA: 2s - loss: 0.1985 - acc: 0.943 - ETA: 2s - loss: 0.2077 - acc: 0.939 - ETA: 1s - loss: 0.2092 - acc: 0.938 - ETA: 1s - loss: 0.2067 - acc: 0.940 - ETA: 0s - loss: 0.2062 - acc: 0.940 - ETA: 0s - loss: 0.2061 - acc: 0.940 - 17s 4ms/step - loss: 0.2040 - acc: 0.9417 - val_loss: 0.0813 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.03866\n",
      "Epoch 30/100\n",
      "4340/4340 [==============================] - ETA: 12s - loss: 0.1612 - acc: 0.95 - ETA: 13s - loss: 0.1342 - acc: 0.96 - ETA: 13s - loss: 0.1282 - acc: 0.97 - ETA: 13s - loss: 0.1319 - acc: 0.97 - ETA: 12s - loss: 0.1263 - acc: 0.97 - ETA: 12s - loss: 0.1224 - acc: 0.97 - ETA: 11s - loss: 0.1221 - acc: 0.97 - ETA: 11s - loss: 0.1214 - acc: 0.97 - ETA: 11s - loss: 0.1230 - acc: 0.97 - ETA: 10s - loss: 0.1217 - acc: 0.97 - ETA: 10s - loss: 0.1193 - acc: 0.97 - ETA: 9s - loss: 0.1184 - acc: 0.9766 - ETA: 9s - loss: 0.1173 - acc: 0.977 - ETA: 8s - loss: 0.1164 - acc: 0.976 - ETA: 8s - loss: 0.1172 - acc: 0.975 - ETA: 7s - loss: 0.1169 - acc: 0.975 - ETA: 7s - loss: 0.1147 - acc: 0.975 - ETA: 7s - loss: 0.1144 - acc: 0.975 - ETA: 6s - loss: 0.1156 - acc: 0.974 - ETA: 6s - loss: 0.1153 - acc: 0.974 - ETA: 5s - loss: 0.1143 - acc: 0.975 - ETA: 5s - loss: 0.1121 - acc: 0.976 - ETA: 4s - loss: 0.1108 - acc: 0.976 - ETA: 4s - loss: 0.1080 - acc: 0.977 - ETA: 3s - loss: 0.1063 - acc: 0.977 - ETA: 3s - loss: 0.1065 - acc: 0.977 - ETA: 3s - loss: 0.1055 - acc: 0.977 - ETA: 2s - loss: 0.1054 - acc: 0.977 - ETA: 2s - loss: 0.1047 - acc: 0.977 - ETA: 1s - loss: 0.1050 - acc: 0.977 - ETA: 1s - loss: 0.1039 - acc: 0.977 - ETA: 0s - loss: 0.1026 - acc: 0.978 - ETA: 0s - loss: 0.1033 - acc: 0.977 - 18s 4ms/step - loss: 0.1040 - acc: 0.9770 - val_loss: 0.0571 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.03866\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4340/4340 [==============================] - ETA: 20s - loss: 0.1157 - acc: 0.96 - ETA: 21s - loss: 0.1130 - acc: 0.97 - ETA: 18s - loss: 0.0923 - acc: 0.97 - ETA: 17s - loss: 0.0917 - acc: 0.97 - ETA: 15s - loss: 0.0888 - acc: 0.98 - ETA: 15s - loss: 0.0919 - acc: 0.97 - ETA: 14s - loss: 0.0937 - acc: 0.97 - ETA: 13s - loss: 0.0937 - acc: 0.97 - ETA: 12s - loss: 0.0905 - acc: 0.97 - ETA: 12s - loss: 0.0900 - acc: 0.97 - ETA: 11s - loss: 0.0902 - acc: 0.97 - ETA: 10s - loss: 0.0913 - acc: 0.97 - ETA: 10s - loss: 0.0897 - acc: 0.97 - ETA: 9s - loss: 0.0879 - acc: 0.9749 - ETA: 9s - loss: 0.0891 - acc: 0.975 - ETA: 8s - loss: 0.0887 - acc: 0.975 - ETA: 8s - loss: 0.0867 - acc: 0.975 - ETA: 7s - loss: 0.0863 - acc: 0.976 - ETA: 7s - loss: 0.0859 - acc: 0.976 - ETA: 6s - loss: 0.0872 - acc: 0.975 - ETA: 6s - loss: 0.0860 - acc: 0.976 - ETA: 5s - loss: 0.0871 - acc: 0.976 - ETA: 5s - loss: 0.0875 - acc: 0.976 - ETA: 4s - loss: 0.0859 - acc: 0.977 - ETA: 4s - loss: 0.0858 - acc: 0.977 - ETA: 3s - loss: 0.0851 - acc: 0.978 - ETA: 3s - loss: 0.0838 - acc: 0.979 - ETA: 2s - loss: 0.0838 - acc: 0.979 - ETA: 2s - loss: 0.0838 - acc: 0.979 - ETA: 1s - loss: 0.0828 - acc: 0.979 - ETA: 1s - loss: 0.0827 - acc: 0.979 - ETA: 0s - loss: 0.0835 - acc: 0.979 - ETA: 0s - loss: 0.0829 - acc: 0.979 - 17s 4ms/step - loss: 0.0824 - acc: 0.9800 - val_loss: 0.0582 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.03866\n",
      "Epoch 32/100\n",
      "4340/4340 [==============================] - ETA: 17s - loss: 0.0930 - acc: 0.97 - ETA: 14s - loss: 0.0727 - acc: 0.98 - ETA: 15s - loss: 0.0812 - acc: 0.98 - ETA: 14s - loss: 0.0800 - acc: 0.98 - ETA: 14s - loss: 0.0711 - acc: 0.98 - ETA: 13s - loss: 0.0699 - acc: 0.98 - ETA: 12s - loss: 0.0732 - acc: 0.98 - ETA: 12s - loss: 0.0723 - acc: 0.98 - ETA: 11s - loss: 0.0693 - acc: 0.98 - ETA: 11s - loss: 0.0695 - acc: 0.98 - ETA: 10s - loss: 0.0676 - acc: 0.98 - ETA: 10s - loss: 0.0671 - acc: 0.98 - ETA: 9s - loss: 0.0652 - acc: 0.9856 - ETA: 9s - loss: 0.0638 - acc: 0.986 - ETA: 9s - loss: 0.0640 - acc: 0.986 - ETA: 8s - loss: 0.0641 - acc: 0.986 - ETA: 9s - loss: 0.0647 - acc: 0.985 - ETA: 8s - loss: 0.0643 - acc: 0.986 - ETA: 8s - loss: 0.0652 - acc: 0.986 - ETA: 7s - loss: 0.0656 - acc: 0.985 - ETA: 7s - loss: 0.0648 - acc: 0.985 - ETA: 6s - loss: 0.0637 - acc: 0.986 - ETA: 6s - loss: 0.0650 - acc: 0.985 - ETA: 5s - loss: 0.0643 - acc: 0.986 - ETA: 5s - loss: 0.0639 - acc: 0.986 - ETA: 4s - loss: 0.0635 - acc: 0.986 - ETA: 4s - loss: 0.0645 - acc: 0.986 - ETA: 3s - loss: 0.0642 - acc: 0.986 - ETA: 3s - loss: 0.0643 - acc: 0.986 - ETA: 2s - loss: 0.0634 - acc: 0.986 - ETA: 1s - loss: 0.0630 - acc: 0.986 - ETA: 1s - loss: 0.0632 - acc: 0.986 - ETA: 0s - loss: 0.0621 - acc: 0.987 - 23s 5ms/step - loss: 0.0614 - acc: 0.9871 - val_loss: 0.0364 - val_acc: 0.9899\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.03866 to 0.03643, saving model to weight_MobileNet.hdf5\n",
      "Epoch 33/100\n",
      "4340/4340 [==============================] - ETA: 30s - loss: 0.0494 - acc: 0.99 - ETA: 31s - loss: 0.0561 - acc: 0.98 - ETA: 28s - loss: 0.0582 - acc: 0.97 - ETA: 26s - loss: 0.0594 - acc: 0.97 - ETA: 23s - loss: 0.0618 - acc: 0.98 - ETA: 22s - loss: 0.0609 - acc: 0.98 - ETA: 21s - loss: 0.0618 - acc: 0.98 - ETA: 19s - loss: 0.0572 - acc: 0.98 - ETA: 19s - loss: 0.0556 - acc: 0.98 - ETA: 18s - loss: 0.0570 - acc: 0.98 - ETA: 17s - loss: 0.0563 - acc: 0.98 - ETA: 16s - loss: 0.0561 - acc: 0.98 - ETA: 15s - loss: 0.0553 - acc: 0.98 - ETA: 14s - loss: 0.0555 - acc: 0.98 - ETA: 13s - loss: 0.0562 - acc: 0.98 - ETA: 13s - loss: 0.0560 - acc: 0.98 - ETA: 12s - loss: 0.0567 - acc: 0.98 - ETA: 12s - loss: 0.0585 - acc: 0.98 - ETA: 11s - loss: 0.0572 - acc: 0.98 - ETA: 11s - loss: 0.0573 - acc: 0.98 - ETA: 10s - loss: 0.0577 - acc: 0.98 - ETA: 10s - loss: 0.0579 - acc: 0.98 - ETA: 9s - loss: 0.0573 - acc: 0.9874 - ETA: 9s - loss: 0.0563 - acc: 0.988 - ETA: 8s - loss: 0.0555 - acc: 0.988 - ETA: 7s - loss: 0.0550 - acc: 0.988 - ETA: 6s - loss: 0.0549 - acc: 0.988 - ETA: 5s - loss: 0.0543 - acc: 0.989 - ETA: 4s - loss: 0.0541 - acc: 0.989 - ETA: 3s - loss: 0.0544 - acc: 0.989 - ETA: 2s - loss: 0.0540 - acc: 0.989 - ETA: 1s - loss: 0.0535 - acc: 0.989 - ETA: 0s - loss: 0.0528 - acc: 0.989 - 39s 9ms/step - loss: 0.0526 - acc: 0.9899 - val_loss: 0.0425 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.03643\n",
      "Epoch 34/100\n",
      "4340/4340 [==============================] - ETA: 31s - loss: 0.0430 - acc: 0.97 - ETA: 29s - loss: 0.0491 - acc: 0.98 - ETA: 27s - loss: 0.0435 - acc: 0.98 - ETA: 28s - loss: 0.0503 - acc: 0.98 - ETA: 28s - loss: 0.0570 - acc: 0.97 - ETA: 31s - loss: 0.0605 - acc: 0.97 - ETA: 32s - loss: 0.0587 - acc: 0.97 - ETA: 31s - loss: 0.0586 - acc: 0.97 - ETA: 29s - loss: 0.0599 - acc: 0.97 - ETA: 27s - loss: 0.0642 - acc: 0.97 - ETA: 26s - loss: 0.0643 - acc: 0.97 - ETA: 25s - loss: 0.0637 - acc: 0.97 - ETA: 24s - loss: 0.0637 - acc: 0.97 - ETA: 22s - loss: 0.0616 - acc: 0.97 - ETA: 21s - loss: 0.0609 - acc: 0.97 - ETA: 19s - loss: 0.0603 - acc: 0.98 - ETA: 18s - loss: 0.0597 - acc: 0.97 - ETA: 16s - loss: 0.0579 - acc: 0.98 - ETA: 15s - loss: 0.0565 - acc: 0.98 - ETA: 13s - loss: 0.0570 - acc: 0.98 - ETA: 12s - loss: 0.0570 - acc: 0.98 - ETA: 11s - loss: 0.0561 - acc: 0.98 - ETA: 10s - loss: 0.0553 - acc: 0.98 - ETA: 9s - loss: 0.0545 - acc: 0.9837 - ETA: 8s - loss: 0.0534 - acc: 0.984 - ETA: 7s - loss: 0.0536 - acc: 0.984 - ETA: 6s - loss: 0.0525 - acc: 0.984 - ETA: 5s - loss: 0.0524 - acc: 0.984 - ETA: 4s - loss: 0.0518 - acc: 0.985 - ETA: 3s - loss: 0.0520 - acc: 0.985 - ETA: 2s - loss: 0.0516 - acc: 0.985 - ETA: 1s - loss: 0.0513 - acc: 0.985 - ETA: 0s - loss: 0.0510 - acc: 0.985 - 29s 7ms/step - loss: 0.0510 - acc: 0.9857 - val_loss: 0.0211 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.03643 to 0.02111, saving model to weight_MobileNet.hdf5\n",
      "Epoch 35/100\n",
      "4340/4340 [==============================] - ETA: 14s - loss: 0.0475 - acc: 0.99 - ETA: 14s - loss: 0.0384 - acc: 0.99 - ETA: 14s - loss: 0.0361 - acc: 0.99 - ETA: 14s - loss: 0.0353 - acc: 0.99 - ETA: 14s - loss: 0.0381 - acc: 0.99 - ETA: 13s - loss: 0.0386 - acc: 0.99 - ETA: 13s - loss: 0.0388 - acc: 0.99 - ETA: 12s - loss: 0.0389 - acc: 0.99 - ETA: 12s - loss: 0.0373 - acc: 0.99 - ETA: 11s - loss: 0.0366 - acc: 0.99 - ETA: 11s - loss: 0.0377 - acc: 0.99 - ETA: 10s - loss: 0.0368 - acc: 0.99 - ETA: 10s - loss: 0.0366 - acc: 0.99 - ETA: 9s - loss: 0.0356 - acc: 0.9939 - ETA: 9s - loss: 0.0385 - acc: 0.992 - ETA: 8s - loss: 0.0384 - acc: 0.992 - ETA: 8s - loss: 0.0396 - acc: 0.992 - ETA: 7s - loss: 0.0393 - acc: 0.993 - ETA: 7s - loss: 0.0392 - acc: 0.993 - ETA: 6s - loss: 0.0390 - acc: 0.993 - ETA: 6s - loss: 0.0403 - acc: 0.992 - ETA: 5s - loss: 0.0406 - acc: 0.992 - ETA: 5s - loss: 0.0412 - acc: 0.991 - ETA: 4s - loss: 0.0431 - acc: 0.990 - ETA: 4s - loss: 0.0442 - acc: 0.990 - ETA: 3s - loss: 0.0435 - acc: 0.990 - ETA: 3s - loss: 0.0437 - acc: 0.990 - ETA: 2s - loss: 0.0433 - acc: 0.990 - ETA: 2s - loss: 0.0435 - acc: 0.990 - ETA: 1s - loss: 0.0426 - acc: 0.990 - ETA: 1s - loss: 0.0423 - acc: 0.990 - ETA: 0s - loss: 0.0421 - acc: 0.991 - ETA: 0s - loss: 0.0417 - acc: 0.991 - 18s 4ms/step - loss: 0.0413 - acc: 0.9910 - val_loss: 0.0258 - val_acc: 0.9899\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.02111\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4340/4340 [==============================] - ETA: 20s - loss: 0.0440 - acc: 0.97 - ETA: 16s - loss: 0.0323 - acc: 0.98 - ETA: 14s - loss: 0.0304 - acc: 0.99 - ETA: 15s - loss: 0.0309 - acc: 0.99 - ETA: 14s - loss: 0.0296 - acc: 0.99 - ETA: 14s - loss: 0.0314 - acc: 0.99 - ETA: 13s - loss: 0.0310 - acc: 0.99 - ETA: 13s - loss: 0.0368 - acc: 0.99 - ETA: 12s - loss: 0.0383 - acc: 0.99 - ETA: 12s - loss: 0.0395 - acc: 0.99 - ETA: 11s - loss: 0.0401 - acc: 0.99 - ETA: 10s - loss: 0.0403 - acc: 0.99 - ETA: 10s - loss: 0.0433 - acc: 0.98 - ETA: 9s - loss: 0.0420 - acc: 0.9905 - ETA: 9s - loss: 0.0415 - acc: 0.991 - ETA: 8s - loss: 0.0408 - acc: 0.990 - ETA: 8s - loss: 0.0395 - acc: 0.991 - ETA: 7s - loss: 0.0408 - acc: 0.991 - ETA: 7s - loss: 0.0395 - acc: 0.991 - ETA: 6s - loss: 0.0399 - acc: 0.992 - ETA: 6s - loss: 0.0393 - acc: 0.992 - ETA: 5s - loss: 0.0390 - acc: 0.992 - ETA: 5s - loss: 0.0390 - acc: 0.992 - ETA: 4s - loss: 0.0391 - acc: 0.992 - ETA: 4s - loss: 0.0384 - acc: 0.993 - ETA: 3s - loss: 0.0380 - acc: 0.993 - ETA: 3s - loss: 0.0378 - acc: 0.993 - ETA: 2s - loss: 0.0379 - acc: 0.992 - ETA: 2s - loss: 0.0376 - acc: 0.992 - ETA: 1s - loss: 0.0373 - acc: 0.992 - ETA: 1s - loss: 0.0377 - acc: 0.992 - ETA: 0s - loss: 0.0382 - acc: 0.992 - ETA: 0s - loss: 0.0389 - acc: 0.992 - 18s 4ms/step - loss: 0.0406 - acc: 0.9915 - val_loss: 0.0205 - val_acc: 0.9926\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.02111 to 0.02050, saving model to weight_MobileNet.hdf5\n",
      "Epoch 37/100\n",
      "4340/4340 [==============================] - ETA: 22s - loss: 0.0461 - acc: 0.99 - ETA: 25s - loss: 0.0382 - acc: 0.99 - ETA: 28s - loss: 0.0320 - acc: 0.99 - ETA: 23s - loss: 0.0393 - acc: 0.99 - ETA: 21s - loss: 0.0461 - acc: 0.99 - ETA: 19s - loss: 0.0471 - acc: 0.98 - ETA: 18s - loss: 0.0426 - acc: 0.99 - ETA: 18s - loss: 0.0405 - acc: 0.99 - ETA: 16s - loss: 0.0392 - acc: 0.99 - ETA: 15s - loss: 0.0379 - acc: 0.99 - ETA: 14s - loss: 0.0392 - acc: 0.99 - ETA: 13s - loss: 0.0384 - acc: 0.99 - ETA: 12s - loss: 0.0386 - acc: 0.99 - ETA: 11s - loss: 0.0372 - acc: 0.99 - ETA: 11s - loss: 0.0383 - acc: 0.99 - ETA: 10s - loss: 0.0386 - acc: 0.99 - ETA: 9s - loss: 0.0374 - acc: 0.9926 - ETA: 8s - loss: 0.0365 - acc: 0.993 - ETA: 8s - loss: 0.0355 - acc: 0.993 - ETA: 7s - loss: 0.0351 - acc: 0.993 - ETA: 7s - loss: 0.0349 - acc: 0.993 - ETA: 6s - loss: 0.0354 - acc: 0.993 - ETA: 6s - loss: 0.0357 - acc: 0.992 - ETA: 5s - loss: 0.0353 - acc: 0.993 - ETA: 4s - loss: 0.0351 - acc: 0.993 - ETA: 4s - loss: 0.0352 - acc: 0.992 - ETA: 3s - loss: 0.0359 - acc: 0.992 - ETA: 3s - loss: 0.0363 - acc: 0.992 - ETA: 2s - loss: 0.0361 - acc: 0.992 - ETA: 2s - loss: 0.0358 - acc: 0.993 - ETA: 1s - loss: 0.0362 - acc: 0.993 - ETA: 1s - loss: 0.0358 - acc: 0.993 - ETA: 0s - loss: 0.0353 - acc: 0.993 - 19s 4ms/step - loss: 0.0348 - acc: 0.9938 - val_loss: 0.0202 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.02050 to 0.02022, saving model to weight_MobileNet.hdf5\n",
      "Epoch 38/100\n",
      "4340/4340 [==============================] - ETA: 12s - loss: 0.0263 - acc: 1.00 - ETA: 13s - loss: 0.0361 - acc: 0.99 - ETA: 13s - loss: 0.0335 - acc: 0.99 - ETA: 13s - loss: 0.0299 - acc: 0.99 - ETA: 13s - loss: 0.0271 - acc: 0.99 - ETA: 13s - loss: 0.0324 - acc: 0.99 - ETA: 12s - loss: 0.0307 - acc: 0.99 - ETA: 12s - loss: 0.0321 - acc: 0.99 - ETA: 11s - loss: 0.0331 - acc: 0.99 - ETA: 11s - loss: 0.0321 - acc: 0.99 - ETA: 10s - loss: 0.0304 - acc: 0.99 - ETA: 10s - loss: 0.0310 - acc: 0.99 - ETA: 9s - loss: 0.0308 - acc: 0.9952 - ETA: 9s - loss: 0.0304 - acc: 0.995 - ETA: 9s - loss: 0.0312 - acc: 0.993 - ETA: 8s - loss: 0.0321 - acc: 0.993 - ETA: 8s - loss: 0.0326 - acc: 0.993 - ETA: 7s - loss: 0.0336 - acc: 0.992 - ETA: 7s - loss: 0.0337 - acc: 0.992 - ETA: 6s - loss: 0.0330 - acc: 0.992 - ETA: 6s - loss: 0.0326 - acc: 0.992 - ETA: 5s - loss: 0.0322 - acc: 0.993 - ETA: 5s - loss: 0.0317 - acc: 0.993 - ETA: 4s - loss: 0.0328 - acc: 0.992 - ETA: 4s - loss: 0.0343 - acc: 0.991 - ETA: 3s - loss: 0.0338 - acc: 0.992 - ETA: 3s - loss: 0.0335 - acc: 0.992 - ETA: 2s - loss: 0.0335 - acc: 0.992 - ETA: 2s - loss: 0.0331 - acc: 0.993 - ETA: 1s - loss: 0.0325 - acc: 0.993 - ETA: 1s - loss: 0.0327 - acc: 0.993 - ETA: 0s - loss: 0.0322 - acc: 0.993 - ETA: 0s - loss: 0.0327 - acc: 0.993 - 18s 4ms/step - loss: 0.0331 - acc: 0.9929 - val_loss: 0.0267 - val_acc: 0.9899\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.02022\n",
      "Epoch 39/100\n",
      "4340/4340 [==============================] - ETA: 14s - loss: 0.0355 - acc: 0.99 - ETA: 18s - loss: 0.0282 - acc: 0.99 - ETA: 18s - loss: 0.0269 - acc: 0.99 - ETA: 17s - loss: 0.0305 - acc: 0.99 - ETA: 17s - loss: 0.0273 - acc: 0.99 - ETA: 16s - loss: 0.0291 - acc: 0.99 - ETA: 15s - loss: 0.0274 - acc: 0.99 - ETA: 14s - loss: 0.0278 - acc: 0.99 - ETA: 13s - loss: 0.0286 - acc: 0.99 - ETA: 13s - loss: 0.0291 - acc: 0.99 - ETA: 12s - loss: 0.0301 - acc: 0.99 - ETA: 11s - loss: 0.0302 - acc: 0.99 - ETA: 11s - loss: 0.0291 - acc: 0.99 - ETA: 10s - loss: 0.0290 - acc: 0.99 - ETA: 10s - loss: 0.0311 - acc: 0.99 - ETA: 9s - loss: 0.0329 - acc: 0.9902 - ETA: 9s - loss: 0.0337 - acc: 0.990 - ETA: 8s - loss: 0.0342 - acc: 0.990 - ETA: 7s - loss: 0.0337 - acc: 0.990 - ETA: 7s - loss: 0.0338 - acc: 0.990 - ETA: 6s - loss: 0.0330 - acc: 0.990 - ETA: 6s - loss: 0.0323 - acc: 0.991 - ETA: 5s - loss: 0.0321 - acc: 0.991 - ETA: 5s - loss: 0.0320 - acc: 0.991 - ETA: 4s - loss: 0.0318 - acc: 0.991 - ETA: 4s - loss: 0.0314 - acc: 0.992 - ETA: 3s - loss: 0.0310 - acc: 0.992 - ETA: 3s - loss: 0.0314 - acc: 0.992 - ETA: 2s - loss: 0.0312 - acc: 0.992 - ETA: 2s - loss: 0.0312 - acc: 0.992 - ETA: 1s - loss: 0.0309 - acc: 0.992 - ETA: 1s - loss: 0.0306 - acc: 0.992 - ETA: 0s - loss: 0.0311 - acc: 0.992 - 20s 5ms/step - loss: 0.0311 - acc: 0.9926 - val_loss: 0.0163 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.02022 to 0.01631, saving model to weight_MobileNet.hdf5\n",
      "Epoch 40/100\n",
      "4340/4340 [==============================] - ETA: 14s - loss: 0.0391 - acc: 1.00 - ETA: 14s - loss: 0.0290 - acc: 1.00 - ETA: 13s - loss: 0.0303 - acc: 0.99 - ETA: 17s - loss: 0.0279 - acc: 0.99 - ETA: 18s - loss: 0.0262 - acc: 0.99 - ETA: 16s - loss: 0.0264 - acc: 0.99 - ETA: 15s - loss: 0.0262 - acc: 0.99 - ETA: 14s - loss: 0.0250 - acc: 0.99 - ETA: 13s - loss: 0.0239 - acc: 0.99 - ETA: 13s - loss: 0.0228 - acc: 0.99 - ETA: 13s - loss: 0.0222 - acc: 0.99 - ETA: 13s - loss: 0.0211 - acc: 0.99 - ETA: 12s - loss: 0.0211 - acc: 0.99 - ETA: 11s - loss: 0.0220 - acc: 0.99 - ETA: 11s - loss: 0.0238 - acc: 0.99 - ETA: 10s - loss: 0.0239 - acc: 0.99 - ETA: 9s - loss: 0.0241 - acc: 0.9968 - ETA: 9s - loss: 0.0248 - acc: 0.996 - ETA: 8s - loss: 0.0251 - acc: 0.995 - ETA: 7s - loss: 0.0250 - acc: 0.996 - ETA: 7s - loss: 0.0246 - acc: 0.996 - ETA: 6s - loss: 0.0248 - acc: 0.996 - ETA: 6s - loss: 0.0245 - acc: 0.996 - ETA: 5s - loss: 0.0241 - acc: 0.996 - ETA: 5s - loss: 0.0247 - acc: 0.995 - ETA: 4s - loss: 0.0244 - acc: 0.995 - ETA: 4s - loss: 0.0254 - acc: 0.995 - ETA: 3s - loss: 0.0255 - acc: 0.995 - ETA: 2s - loss: 0.0262 - acc: 0.995 - ETA: 2s - loss: 0.0260 - acc: 0.995 - ETA: 1s - loss: 0.0259 - acc: 0.996 - ETA: 1s - loss: 0.0268 - acc: 0.995 - ETA: 0s - loss: 0.0265 - acc: 0.995 - 21s 5ms/step - loss: 0.0265 - acc: 0.9959 - val_loss: 0.0178 - val_acc: 0.9926\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.01631\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4340/4340 [==============================] - ETA: 23s - loss: 0.0338 - acc: 0.98 - ETA: 18s - loss: 0.0252 - acc: 0.99 - ETA: 17s - loss: 0.0274 - acc: 0.99 - ETA: 15s - loss: 0.0271 - acc: 0.99 - ETA: 15s - loss: 0.0283 - acc: 0.99 - ETA: 14s - loss: 0.0286 - acc: 0.99 - ETA: 13s - loss: 0.0302 - acc: 0.99 - ETA: 13s - loss: 0.0295 - acc: 0.99 - ETA: 12s - loss: 0.0294 - acc: 0.99 - ETA: 12s - loss: 0.0287 - acc: 0.99 - ETA: 11s - loss: 0.0290 - acc: 0.99 - ETA: 11s - loss: 0.0278 - acc: 0.99 - ETA: 10s - loss: 0.0274 - acc: 0.99 - ETA: 10s - loss: 0.0275 - acc: 0.99 - ETA: 9s - loss: 0.0266 - acc: 0.9958 - ETA: 8s - loss: 0.0262 - acc: 0.996 - ETA: 8s - loss: 0.0259 - acc: 0.996 - ETA: 7s - loss: 0.0258 - acc: 0.996 - ETA: 7s - loss: 0.0259 - acc: 0.995 - ETA: 6s - loss: 0.0256 - acc: 0.995 - ETA: 6s - loss: 0.0262 - acc: 0.995 - ETA: 5s - loss: 0.0257 - acc: 0.995 - ETA: 5s - loss: 0.0257 - acc: 0.995 - ETA: 4s - loss: 0.0252 - acc: 0.995 - ETA: 4s - loss: 0.0251 - acc: 0.995 - ETA: 3s - loss: 0.0249 - acc: 0.995 - ETA: 3s - loss: 0.0253 - acc: 0.995 - ETA: 2s - loss: 0.0253 - acc: 0.995 - ETA: 2s - loss: 0.0259 - acc: 0.994 - ETA: 1s - loss: 0.0256 - acc: 0.995 - ETA: 1s - loss: 0.0255 - acc: 0.995 - ETA: 0s - loss: 0.0253 - acc: 0.995 - ETA: 0s - loss: 0.0251 - acc: 0.995 - 19s 4ms/step - loss: 0.0248 - acc: 0.9954 - val_loss: 0.0156 - val_acc: 0.9945\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.01631 to 0.01564, saving model to weight_MobileNet.hdf5\n",
      "Epoch 42/100\n",
      "4340/4340 [==============================] - ETA: 23s - loss: 0.0613 - acc: 0.99 - ETA: 22s - loss: 0.0405 - acc: 0.99 - ETA: 29s - loss: 0.0312 - acc: 0.99 - ETA: 27s - loss: 0.0337 - acc: 0.99 - ETA: 23s - loss: 0.0301 - acc: 0.99 - ETA: 21s - loss: 0.0277 - acc: 0.99 - ETA: 19s - loss: 0.0248 - acc: 0.99 - ETA: 18s - loss: 0.0256 - acc: 0.99 - ETA: 17s - loss: 0.0244 - acc: 0.99 - ETA: 17s - loss: 0.0254 - acc: 0.99 - ETA: 17s - loss: 0.0259 - acc: 0.99 - ETA: 16s - loss: 0.0270 - acc: 0.99 - ETA: 14s - loss: 0.0270 - acc: 0.99 - ETA: 13s - loss: 0.0272 - acc: 0.99 - ETA: 12s - loss: 0.0271 - acc: 0.99 - ETA: 11s - loss: 0.0266 - acc: 0.99 - ETA: 10s - loss: 0.0265 - acc: 0.99 - ETA: 10s - loss: 0.0271 - acc: 0.99 - ETA: 9s - loss: 0.0267 - acc: 0.9938 - ETA: 8s - loss: 0.0260 - acc: 0.994 - ETA: 8s - loss: 0.0261 - acc: 0.994 - ETA: 7s - loss: 0.0258 - acc: 0.994 - ETA: 6s - loss: 0.0262 - acc: 0.993 - ETA: 6s - loss: 0.0265 - acc: 0.993 - ETA: 5s - loss: 0.0264 - acc: 0.993 - ETA: 4s - loss: 0.0266 - acc: 0.993 - ETA: 4s - loss: 0.0270 - acc: 0.993 - ETA: 3s - loss: 0.0265 - acc: 0.993 - ETA: 2s - loss: 0.0265 - acc: 0.993 - ETA: 2s - loss: 0.0265 - acc: 0.993 - ETA: 1s - loss: 0.0267 - acc: 0.993 - ETA: 1s - loss: 0.0263 - acc: 0.993 - ETA: 0s - loss: 0.0262 - acc: 0.993 - 23s 5ms/step - loss: 0.0259 - acc: 0.9940 - val_loss: 0.0173 - val_acc: 0.9945\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.01564\n",
      "Epoch 43/100\n",
      "4340/4340 [==============================] - ETA: 32s - loss: 0.0272 - acc: 1.00 - ETA: 23s - loss: 0.0275 - acc: 0.99 - ETA: 20s - loss: 0.0260 - acc: 0.99 - ETA: 18s - loss: 0.0268 - acc: 0.99 - ETA: 18s - loss: 0.0243 - acc: 0.99 - ETA: 16s - loss: 0.0244 - acc: 0.99 - ETA: 17s - loss: 0.0249 - acc: 0.99 - ETA: 16s - loss: 0.0251 - acc: 0.99 - ETA: 15s - loss: 0.0237 - acc: 0.99 - ETA: 15s - loss: 0.0257 - acc: 0.99 - ETA: 14s - loss: 0.0260 - acc: 0.99 - ETA: 13s - loss: 0.0251 - acc: 0.99 - ETA: 12s - loss: 0.0248 - acc: 0.99 - ETA: 12s - loss: 0.0254 - acc: 0.99 - ETA: 11s - loss: 0.0252 - acc: 0.99 - ETA: 11s - loss: 0.0250 - acc: 0.99 - ETA: 10s - loss: 0.0246 - acc: 0.99 - ETA: 10s - loss: 0.0244 - acc: 0.99 - ETA: 9s - loss: 0.0241 - acc: 0.9947 - ETA: 9s - loss: 0.0236 - acc: 0.994 - ETA: 8s - loss: 0.0233 - acc: 0.995 - ETA: 7s - loss: 0.0230 - acc: 0.995 - ETA: 6s - loss: 0.0225 - acc: 0.995 - ETA: 6s - loss: 0.0222 - acc: 0.995 - ETA: 5s - loss: 0.0217 - acc: 0.995 - ETA: 4s - loss: 0.0213 - acc: 0.996 - ETA: 4s - loss: 0.0210 - acc: 0.996 - ETA: 3s - loss: 0.0220 - acc: 0.995 - ETA: 2s - loss: 0.0220 - acc: 0.996 - ETA: 2s - loss: 0.0218 - acc: 0.996 - ETA: 1s - loss: 0.0218 - acc: 0.996 - ETA: 1s - loss: 0.0219 - acc: 0.995 - ETA: 0s - loss: 0.0217 - acc: 0.996 - 21s 5ms/step - loss: 0.0218 - acc: 0.9959 - val_loss: 0.0137 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.01564 to 0.01374, saving model to weight_MobileNet.hdf5\n",
      "Epoch 44/100\n",
      "4340/4340 [==============================] - ETA: 14s - loss: 0.0605 - acc: 0.97 - ETA: 16s - loss: 0.0708 - acc: 0.98 - ETA: 15s - loss: 0.0530 - acc: 0.98 - ETA: 14s - loss: 0.0496 - acc: 0.98 - ETA: 13s - loss: 0.0458 - acc: 0.98 - ETA: 13s - loss: 0.0415 - acc: 0.98 - ETA: 12s - loss: 0.0384 - acc: 0.99 - ETA: 12s - loss: 0.0379 - acc: 0.99 - ETA: 11s - loss: 0.0366 - acc: 0.99 - ETA: 11s - loss: 0.0355 - acc: 0.99 - ETA: 10s - loss: 0.0349 - acc: 0.99 - ETA: 10s - loss: 0.0326 - acc: 0.99 - ETA: 10s - loss: 0.0323 - acc: 0.99 - ETA: 9s - loss: 0.0312 - acc: 0.9944 - ETA: 9s - loss: 0.0299 - acc: 0.994 - ETA: 8s - loss: 0.0286 - acc: 0.995 - ETA: 8s - loss: 0.0282 - acc: 0.995 - ETA: 7s - loss: 0.0285 - acc: 0.995 - ETA: 7s - loss: 0.0276 - acc: 0.995 - ETA: 6s - loss: 0.0271 - acc: 0.995 - ETA: 6s - loss: 0.0270 - acc: 0.995 - ETA: 5s - loss: 0.0272 - acc: 0.995 - ETA: 5s - loss: 0.0269 - acc: 0.995 - ETA: 4s - loss: 0.0269 - acc: 0.995 - ETA: 4s - loss: 0.0270 - acc: 0.995 - ETA: 3s - loss: 0.0272 - acc: 0.994 - ETA: 3s - loss: 0.0269 - acc: 0.995 - ETA: 2s - loss: 0.0274 - acc: 0.995 - ETA: 2s - loss: 0.0273 - acc: 0.995 - ETA: 1s - loss: 0.0272 - acc: 0.995 - ETA: 1s - loss: 0.0278 - acc: 0.995 - ETA: 0s - loss: 0.0274 - acc: 0.995 - ETA: 0s - loss: 0.0279 - acc: 0.994 - 17s 4ms/step - loss: 0.0276 - acc: 0.9949 - val_loss: 0.0113 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.01374 to 0.01128, saving model to weight_MobileNet.hdf5\n",
      "Epoch 45/100\n",
      "4340/4340 [==============================] - ETA: 13s - loss: 0.0104 - acc: 1.00 - ETA: 16s - loss: 0.0152 - acc: 1.00 - ETA: 15s - loss: 0.0160 - acc: 1.00 - ETA: 14s - loss: 0.0157 - acc: 1.00 - ETA: 15s - loss: 0.0150 - acc: 1.00 - ETA: 15s - loss: 0.0174 - acc: 0.99 - ETA: 16s - loss: 0.0194 - acc: 0.99 - ETA: 15s - loss: 0.0197 - acc: 0.99 - ETA: 14s - loss: 0.0188 - acc: 0.99 - ETA: 13s - loss: 0.0180 - acc: 0.99 - ETA: 13s - loss: 0.0210 - acc: 0.99 - ETA: 12s - loss: 0.0237 - acc: 0.99 - ETA: 11s - loss: 0.0241 - acc: 0.99 - ETA: 10s - loss: 0.0236 - acc: 0.99 - ETA: 10s - loss: 0.0244 - acc: 0.99 - ETA: 9s - loss: 0.0244 - acc: 0.9971 - ETA: 9s - loss: 0.0237 - acc: 0.997 - ETA: 8s - loss: 0.0238 - acc: 0.997 - ETA: 7s - loss: 0.0235 - acc: 0.997 - ETA: 7s - loss: 0.0229 - acc: 0.997 - ETA: 6s - loss: 0.0227 - acc: 0.997 - ETA: 6s - loss: 0.0223 - acc: 0.997 - ETA: 5s - loss: 0.0222 - acc: 0.997 - ETA: 5s - loss: 0.0220 - acc: 0.997 - ETA: 4s - loss: 0.0221 - acc: 0.997 - ETA: 4s - loss: 0.0218 - acc: 0.997 - ETA: 3s - loss: 0.0215 - acc: 0.997 - ETA: 2s - loss: 0.0217 - acc: 0.997 - ETA: 2s - loss: 0.0212 - acc: 0.997 - ETA: 1s - loss: 0.0217 - acc: 0.997 - ETA: 1s - loss: 0.0215 - acc: 0.997 - ETA: 0s - loss: 0.0212 - acc: 0.997 - ETA: 0s - loss: 0.0212 - acc: 0.997 - 20s 5ms/step - loss: 0.0210 - acc: 0.9975 - val_loss: 0.0148 - val_acc: 0.9945\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.01128\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4340/4340 [==============================] - ETA: 15s - loss: 0.0130 - acc: 1.00 - ETA: 13s - loss: 0.0119 - acc: 1.00 - ETA: 14s - loss: 0.0171 - acc: 0.99 - ETA: 13s - loss: 0.0182 - acc: 0.99 - ETA: 14s - loss: 0.0158 - acc: 0.99 - ETA: 13s - loss: 0.0160 - acc: 0.99 - ETA: 13s - loss: 0.0153 - acc: 0.99 - ETA: 12s - loss: 0.0153 - acc: 0.99 - ETA: 12s - loss: 0.0147 - acc: 0.99 - ETA: 11s - loss: 0.0145 - acc: 0.99 - ETA: 11s - loss: 0.0138 - acc: 0.99 - ETA: 10s - loss: 0.0142 - acc: 0.99 - ETA: 10s - loss: 0.0141 - acc: 0.99 - ETA: 9s - loss: 0.0145 - acc: 0.9989 - ETA: 9s - loss: 0.0147 - acc: 0.998 - ETA: 8s - loss: 0.0154 - acc: 0.998 - ETA: 8s - loss: 0.0185 - acc: 0.997 - ETA: 7s - loss: 0.0187 - acc: 0.997 - ETA: 7s - loss: 0.0193 - acc: 0.997 - ETA: 6s - loss: 0.0194 - acc: 0.996 - ETA: 6s - loss: 0.0195 - acc: 0.996 - ETA: 5s - loss: 0.0193 - acc: 0.996 - ETA: 5s - loss: 0.0191 - acc: 0.996 - ETA: 4s - loss: 0.0187 - acc: 0.997 - ETA: 4s - loss: 0.0183 - acc: 0.997 - ETA: 3s - loss: 0.0186 - acc: 0.997 - ETA: 3s - loss: 0.0200 - acc: 0.996 - ETA: 2s - loss: 0.0197 - acc: 0.996 - ETA: 2s - loss: 0.0200 - acc: 0.996 - ETA: 1s - loss: 0.0197 - acc: 0.996 - ETA: 1s - loss: 0.0195 - acc: 0.996 - ETA: 0s - loss: 0.0191 - acc: 0.996 - ETA: 0s - loss: 0.0191 - acc: 0.996 - 18s 4ms/step - loss: 0.0190 - acc: 0.9963 - val_loss: 0.0146 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.01128\n",
      "Epoch 47/100\n",
      "4340/4340 [==============================] - ETA: 17s - loss: 0.0381 - acc: 0.98 - ETA: 16s - loss: 0.0280 - acc: 0.99 - ETA: 16s - loss: 0.0363 - acc: 0.98 - ETA: 14s - loss: 0.0292 - acc: 0.98 - ETA: 14s - loss: 0.0265 - acc: 0.99 - ETA: 13s - loss: 0.0260 - acc: 0.99 - ETA: 13s - loss: 0.0276 - acc: 0.99 - ETA: 12s - loss: 0.0280 - acc: 0.99 - ETA: 11s - loss: 0.0281 - acc: 0.99 - ETA: 11s - loss: 0.0260 - acc: 0.99 - ETA: 10s - loss: 0.0262 - acc: 0.99 - ETA: 10s - loss: 0.0252 - acc: 0.99 - ETA: 10s - loss: 0.0248 - acc: 0.99 - ETA: 9s - loss: 0.0252 - acc: 0.9933 - ETA: 9s - loss: 0.0243 - acc: 0.993 - ETA: 8s - loss: 0.0248 - acc: 0.993 - ETA: 8s - loss: 0.0241 - acc: 0.994 - ETA: 7s - loss: 0.0241 - acc: 0.993 - ETA: 7s - loss: 0.0251 - acc: 0.993 - ETA: 7s - loss: 0.0244 - acc: 0.993 - ETA: 6s - loss: 0.0241 - acc: 0.993 - ETA: 6s - loss: 0.0241 - acc: 0.994 - ETA: 5s - loss: 0.0247 - acc: 0.993 - ETA: 5s - loss: 0.0242 - acc: 0.994 - ETA: 4s - loss: 0.0244 - acc: 0.993 - ETA: 4s - loss: 0.0241 - acc: 0.994 - ETA: 3s - loss: 0.0236 - acc: 0.994 - ETA: 3s - loss: 0.0233 - acc: 0.994 - ETA: 2s - loss: 0.0240 - acc: 0.994 - ETA: 2s - loss: 0.0237 - acc: 0.994 - ETA: 1s - loss: 0.0231 - acc: 0.994 - ETA: 1s - loss: 0.0231 - acc: 0.994 - ETA: 0s - loss: 0.0228 - acc: 0.994 - 19s 4ms/step - loss: 0.0226 - acc: 0.9947 - val_loss: 0.0208 - val_acc: 0.9926\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.01128\n",
      "Epoch 48/100\n",
      "4340/4340 [==============================] - ETA: 13s - loss: 0.0275 - acc: 0.98 - ETA: 13s - loss: 0.0227 - acc: 0.99 - ETA: 13s - loss: 0.0182 - acc: 0.99 - ETA: 13s - loss: 0.0198 - acc: 0.99 - ETA: 13s - loss: 0.0179 - acc: 0.99 - ETA: 13s - loss: 0.0199 - acc: 0.99 - ETA: 12s - loss: 0.0199 - acc: 0.99 - ETA: 12s - loss: 0.0205 - acc: 0.99 - ETA: 11s - loss: 0.0187 - acc: 0.99 - ETA: 11s - loss: 0.0182 - acc: 0.99 - ETA: 10s - loss: 0.0172 - acc: 0.99 - ETA: 10s - loss: 0.0174 - acc: 0.99 - ETA: 9s - loss: 0.0173 - acc: 0.9964 - ETA: 9s - loss: 0.0169 - acc: 0.996 - ETA: 8s - loss: 0.0166 - acc: 0.996 - ETA: 8s - loss: 0.0162 - acc: 0.997 - ETA: 8s - loss: 0.0163 - acc: 0.996 - ETA: 7s - loss: 0.0161 - acc: 0.997 - ETA: 7s - loss: 0.0160 - acc: 0.997 - ETA: 6s - loss: 0.0175 - acc: 0.996 - ETA: 6s - loss: 0.0181 - acc: 0.995 - ETA: 5s - loss: 0.0179 - acc: 0.995 - ETA: 5s - loss: 0.0184 - acc: 0.995 - ETA: 4s - loss: 0.0182 - acc: 0.995 - ETA: 4s - loss: 0.0185 - acc: 0.995 - ETA: 3s - loss: 0.0189 - acc: 0.995 - ETA: 3s - loss: 0.0187 - acc: 0.995 - ETA: 2s - loss: 0.0183 - acc: 0.995 - ETA: 2s - loss: 0.0182 - acc: 0.996 - ETA: 1s - loss: 0.0180 - acc: 0.996 - ETA: 1s - loss: 0.0176 - acc: 0.996 - ETA: 0s - loss: 0.0173 - acc: 0.996 - ETA: 0s - loss: 0.0173 - acc: 0.996 - 17s 4ms/step - loss: 0.0170 - acc: 0.9965 - val_loss: 0.0121 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.01128\n",
      "Epoch 49/100\n",
      "4340/4340 [==============================] - ETA: 15s - loss: 0.0264 - acc: 1.00 - ETA: 14s - loss: 0.0255 - acc: 0.99 - ETA: 14s - loss: 0.0181 - acc: 0.99 - ETA: 13s - loss: 0.0169 - acc: 0.99 - ETA: 12s - loss: 0.0180 - acc: 0.99 - ETA: 12s - loss: 0.0220 - acc: 0.99 - ETA: 11s - loss: 0.0202 - acc: 0.99 - ETA: 12s - loss: 0.0222 - acc: 0.99 - ETA: 11s - loss: 0.0221 - acc: 0.99 - ETA: 11s - loss: 0.0220 - acc: 0.99 - ETA: 12s - loss: 0.0223 - acc: 0.99 - ETA: 11s - loss: 0.0218 - acc: 0.99 - ETA: 11s - loss: 0.0211 - acc: 0.99 - ETA: 10s - loss: 0.0230 - acc: 0.99 - ETA: 9s - loss: 0.0232 - acc: 0.9964 - ETA: 9s - loss: 0.0234 - acc: 0.996 - ETA: 8s - loss: 0.0227 - acc: 0.996 - ETA: 8s - loss: 0.0222 - acc: 0.997 - ETA: 7s - loss: 0.0220 - acc: 0.997 - ETA: 7s - loss: 0.0221 - acc: 0.997 - ETA: 6s - loss: 0.0221 - acc: 0.997 - ETA: 6s - loss: 0.0216 - acc: 0.997 - ETA: 5s - loss: 0.0214 - acc: 0.997 - ETA: 5s - loss: 0.0215 - acc: 0.997 - ETA: 4s - loss: 0.0212 - acc: 0.996 - ETA: 4s - loss: 0.0209 - acc: 0.997 - ETA: 3s - loss: 0.0204 - acc: 0.997 - ETA: 3s - loss: 0.0200 - acc: 0.997 - ETA: 2s - loss: 0.0196 - acc: 0.997 - ETA: 2s - loss: 0.0196 - acc: 0.997 - ETA: 1s - loss: 0.0197 - acc: 0.997 - ETA: 0s - loss: 0.0195 - acc: 0.997 - ETA: 0s - loss: 0.0193 - acc: 0.997 - 20s 5ms/step - loss: 0.0189 - acc: 0.9975 - val_loss: 0.0160 - val_acc: 0.9926\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.01128\n",
      "Epoch 50/100\n",
      "4340/4340 [==============================] - ETA: 16s - loss: 0.0295 - acc: 0.99 - ETA: 14s - loss: 0.0194 - acc: 0.99 - ETA: 15s - loss: 0.0168 - acc: 0.99 - ETA: 14s - loss: 0.0164 - acc: 0.99 - ETA: 14s - loss: 0.0160 - acc: 0.99 - ETA: 13s - loss: 0.0190 - acc: 0.99 - ETA: 13s - loss: 0.0190 - acc: 0.99 - ETA: 12s - loss: 0.0185 - acc: 0.99 - ETA: 11s - loss: 0.0171 - acc: 0.99 - ETA: 11s - loss: 0.0179 - acc: 0.99 - ETA: 11s - loss: 0.0178 - acc: 0.99 - ETA: 10s - loss: 0.0168 - acc: 0.99 - ETA: 10s - loss: 0.0168 - acc: 0.99 - ETA: 9s - loss: 0.0165 - acc: 0.9972 - ETA: 9s - loss: 0.0163 - acc: 0.997 - ETA: 8s - loss: 0.0157 - acc: 0.997 - ETA: 8s - loss: 0.0154 - acc: 0.997 - ETA: 7s - loss: 0.0151 - acc: 0.997 - ETA: 7s - loss: 0.0150 - acc: 0.997 - ETA: 6s - loss: 0.0152 - acc: 0.997 - ETA: 6s - loss: 0.0157 - acc: 0.997 - ETA: 5s - loss: 0.0154 - acc: 0.997 - ETA: 5s - loss: 0.0156 - acc: 0.997 - ETA: 4s - loss: 0.0159 - acc: 0.997 - ETA: 4s - loss: 0.0159 - acc: 0.997 - ETA: 3s - loss: 0.0170 - acc: 0.997 - ETA: 3s - loss: 0.0168 - acc: 0.997 - ETA: 2s - loss: 0.0167 - acc: 0.997 - ETA: 2s - loss: 0.0165 - acc: 0.997 - ETA: 1s - loss: 0.0167 - acc: 0.997 - ETA: 1s - loss: 0.0167 - acc: 0.997 - ETA: 0s - loss: 0.0181 - acc: 0.997 - ETA: 0s - loss: 0.0184 - acc: 0.996 - 18s 4ms/step - loss: 0.0188 - acc: 0.9965 - val_loss: 0.0109 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.01128 to 0.01085, saving model to weight_MobileNet.hdf5\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4340/4340 [==============================] - ETA: 13s - loss: 0.0288 - acc: 0.99 - ETA: 15s - loss: 0.0190 - acc: 0.99 - ETA: 14s - loss: 0.0168 - acc: 0.99 - ETA: 14s - loss: 0.0176 - acc: 0.99 - ETA: 13s - loss: 0.0171 - acc: 0.99 - ETA: 14s - loss: 0.0160 - acc: 0.99 - ETA: 13s - loss: 0.0155 - acc: 0.99 - ETA: 13s - loss: 0.0183 - acc: 0.99 - ETA: 12s - loss: 0.0188 - acc: 0.99 - ETA: 11s - loss: 0.0181 - acc: 0.99 - ETA: 11s - loss: 0.0190 - acc: 0.99 - ETA: 10s - loss: 0.0182 - acc: 0.99 - ETA: 10s - loss: 0.0178 - acc: 0.99 - ETA: 9s - loss: 0.0184 - acc: 0.9961 - ETA: 9s - loss: 0.0182 - acc: 0.996 - ETA: 9s - loss: 0.0179 - acc: 0.996 - ETA: 8s - loss: 0.0177 - acc: 0.996 - ETA: 8s - loss: 0.0175 - acc: 0.996 - ETA: 7s - loss: 0.0176 - acc: 0.996 - ETA: 7s - loss: 0.0178 - acc: 0.996 - ETA: 6s - loss: 0.0183 - acc: 0.995 - ETA: 6s - loss: 0.0182 - acc: 0.995 - ETA: 5s - loss: 0.0180 - acc: 0.995 - ETA: 5s - loss: 0.0177 - acc: 0.995 - ETA: 4s - loss: 0.0174 - acc: 0.995 - ETA: 4s - loss: 0.0175 - acc: 0.996 - ETA: 3s - loss: 0.0172 - acc: 0.996 - ETA: 2s - loss: 0.0170 - acc: 0.996 - ETA: 2s - loss: 0.0170 - acc: 0.996 - ETA: 1s - loss: 0.0175 - acc: 0.996 - ETA: 1s - loss: 0.0177 - acc: 0.996 - ETA: 0s - loss: 0.0175 - acc: 0.996 - ETA: 0s - loss: 0.0173 - acc: 0.996 - 19s 4ms/step - loss: 0.0170 - acc: 0.9968 - val_loss: 0.0120 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.01085\n",
      "Epoch 52/100\n",
      "4340/4340 [==============================] - ETA: 14s - loss: 0.0069 - acc: 1.00 - ETA: 14s - loss: 0.0090 - acc: 1.00 - ETA: 13s - loss: 0.0080 - acc: 1.00 - ETA: 13s - loss: 0.0088 - acc: 1.00 - ETA: 13s - loss: 0.0077 - acc: 1.00 - ETA: 12s - loss: 0.0089 - acc: 1.00 - ETA: 12s - loss: 0.0083 - acc: 1.00 - ETA: 11s - loss: 0.0092 - acc: 1.00 - ETA: 11s - loss: 0.0100 - acc: 0.99 - ETA: 12s - loss: 0.0101 - acc: 0.99 - ETA: 12s - loss: 0.0106 - acc: 0.99 - ETA: 12s - loss: 0.0109 - acc: 0.99 - ETA: 14s - loss: 0.0107 - acc: 0.99 - ETA: 14s - loss: 0.0106 - acc: 0.99 - ETA: 13s - loss: 0.0106 - acc: 0.99 - ETA: 12s - loss: 0.0107 - acc: 0.99 - ETA: 11s - loss: 0.0106 - acc: 0.99 - ETA: 11s - loss: 0.0104 - acc: 0.99 - ETA: 10s - loss: 0.0102 - acc: 0.99 - ETA: 10s - loss: 0.0103 - acc: 0.99 - ETA: 10s - loss: 0.0103 - acc: 0.99 - ETA: 9s - loss: 0.0103 - acc: 0.9993 - ETA: 8s - loss: 0.0105 - acc: 0.999 - ETA: 7s - loss: 0.0110 - acc: 0.999 - ETA: 6s - loss: 0.0109 - acc: 0.999 - ETA: 6s - loss: 0.0109 - acc: 0.999 - ETA: 5s - loss: 0.0110 - acc: 0.998 - ETA: 4s - loss: 0.0108 - acc: 0.998 - ETA: 3s - loss: 0.0110 - acc: 0.998 - ETA: 2s - loss: 0.0111 - acc: 0.998 - ETA: 2s - loss: 0.0110 - acc: 0.998 - ETA: 1s - loss: 0.0112 - acc: 0.998 - ETA: 0s - loss: 0.0114 - acc: 0.998 - 28s 6ms/step - loss: 0.0118 - acc: 0.9984 - val_loss: 0.0121 - val_acc: 0.9945\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.01085\n",
      "Epoch 53/100\n",
      "4340/4340 [==============================] - ETA: 17s - loss: 0.0233 - acc: 1.00 - ETA: 20s - loss: 0.0134 - acc: 1.00 - ETA: 20s - loss: 0.0144 - acc: 0.99 - ETA: 19s - loss: 0.0149 - acc: 0.99 - ETA: 18s - loss: 0.0153 - acc: 0.99 - ETA: 18s - loss: 0.0141 - acc: 0.99 - ETA: 17s - loss: 0.0142 - acc: 0.99 - ETA: 16s - loss: 0.0132 - acc: 0.99 - ETA: 15s - loss: 0.0128 - acc: 0.99 - ETA: 14s - loss: 0.0147 - acc: 0.99 - ETA: 13s - loss: 0.0141 - acc: 0.99 - ETA: 12s - loss: 0.0144 - acc: 0.99 - ETA: 12s - loss: 0.0140 - acc: 0.99 - ETA: 11s - loss: 0.0140 - acc: 0.99 - ETA: 11s - loss: 0.0138 - acc: 0.99 - ETA: 10s - loss: 0.0138 - acc: 0.99 - ETA: 9s - loss: 0.0136 - acc: 0.9972 - ETA: 9s - loss: 0.0136 - acc: 0.997 - ETA: 8s - loss: 0.0133 - acc: 0.997 - ETA: 7s - loss: 0.0132 - acc: 0.997 - ETA: 7s - loss: 0.0132 - acc: 0.997 - ETA: 6s - loss: 0.0134 - acc: 0.997 - ETA: 6s - loss: 0.0131 - acc: 0.997 - ETA: 5s - loss: 0.0127 - acc: 0.997 - ETA: 5s - loss: 0.0143 - acc: 0.997 - ETA: 4s - loss: 0.0151 - acc: 0.997 - ETA: 3s - loss: 0.0155 - acc: 0.997 - ETA: 3s - loss: 0.0154 - acc: 0.997 - ETA: 2s - loss: 0.0159 - acc: 0.997 - ETA: 2s - loss: 0.0156 - acc: 0.997 - ETA: 1s - loss: 0.0160 - acc: 0.996 - ETA: 1s - loss: 0.0164 - acc: 0.996 - ETA: 0s - loss: 0.0161 - acc: 0.996 - 20s 5ms/step - loss: 0.0160 - acc: 0.9965 - val_loss: 0.0126 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.01085\n",
      "Epoch 54/100\n",
      "4340/4340 [==============================] - ETA: 23s - loss: 0.0098 - acc: 1.00 - ETA: 21s - loss: 0.0167 - acc: 0.99 - ETA: 22s - loss: 0.0161 - acc: 0.99 - ETA: 21s - loss: 0.0131 - acc: 0.99 - ETA: 22s - loss: 0.0121 - acc: 0.99 - ETA: 21s - loss: 0.0137 - acc: 0.99 - ETA: 21s - loss: 0.0129 - acc: 0.99 - ETA: 21s - loss: 0.0127 - acc: 0.99 - ETA: 19s - loss: 0.0124 - acc: 0.99 - ETA: 19s - loss: 0.0120 - acc: 0.99 - ETA: 17s - loss: 0.0121 - acc: 0.99 - ETA: 16s - loss: 0.0122 - acc: 0.99 - ETA: 15s - loss: 0.0119 - acc: 0.99 - ETA: 14s - loss: 0.0123 - acc: 0.99 - ETA: 13s - loss: 0.0125 - acc: 0.99 - ETA: 12s - loss: 0.0120 - acc: 0.99 - ETA: 11s - loss: 0.0128 - acc: 0.99 - ETA: 10s - loss: 0.0132 - acc: 0.99 - ETA: 10s - loss: 0.0127 - acc: 0.99 - ETA: 9s - loss: 0.0129 - acc: 0.9977 - ETA: 8s - loss: 0.0130 - acc: 0.997 - ETA: 7s - loss: 0.0131 - acc: 0.997 - ETA: 7s - loss: 0.0129 - acc: 0.997 - ETA: 6s - loss: 0.0127 - acc: 0.997 - ETA: 5s - loss: 0.0125 - acc: 0.997 - ETA: 5s - loss: 0.0123 - acc: 0.997 - ETA: 4s - loss: 0.0125 - acc: 0.998 - ETA: 3s - loss: 0.0123 - acc: 0.998 - ETA: 3s - loss: 0.0123 - acc: 0.998 - ETA: 2s - loss: 0.0126 - acc: 0.997 - ETA: 1s - loss: 0.0126 - acc: 0.997 - ETA: 1s - loss: 0.0124 - acc: 0.997 - ETA: 0s - loss: 0.0128 - acc: 0.997 - 24s 6ms/step - loss: 0.0136 - acc: 0.9972 - val_loss: 0.0171 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.01085\n",
      "Epoch 55/100\n",
      "4340/4340 [==============================] - ETA: 14s - loss: 0.0154 - acc: 0.99 - ETA: 14s - loss: 0.0136 - acc: 0.99 - ETA: 14s - loss: 0.0216 - acc: 0.99 - ETA: 14s - loss: 0.0213 - acc: 0.99 - ETA: 15s - loss: 0.0209 - acc: 0.99 - ETA: 14s - loss: 0.0268 - acc: 0.98 - ETA: 14s - loss: 0.0245 - acc: 0.99 - ETA: 14s - loss: 0.0243 - acc: 0.99 - ETA: 14s - loss: 0.0230 - acc: 0.99 - ETA: 13s - loss: 0.0229 - acc: 0.99 - ETA: 13s - loss: 0.0221 - acc: 0.99 - ETA: 12s - loss: 0.0210 - acc: 0.99 - ETA: 11s - loss: 0.0213 - acc: 0.99 - ETA: 11s - loss: 0.0220 - acc: 0.99 - ETA: 10s - loss: 0.0211 - acc: 0.99 - ETA: 10s - loss: 0.0208 - acc: 0.99 - ETA: 9s - loss: 0.0204 - acc: 0.9931 - ETA: 8s - loss: 0.0199 - acc: 0.993 - ETA: 8s - loss: 0.0196 - acc: 0.993 - ETA: 7s - loss: 0.0190 - acc: 0.994 - ETA: 7s - loss: 0.0188 - acc: 0.994 - ETA: 6s - loss: 0.0183 - acc: 0.994 - ETA: 5s - loss: 0.0178 - acc: 0.994 - ETA: 5s - loss: 0.0173 - acc: 0.995 - ETA: 4s - loss: 0.0174 - acc: 0.995 - ETA: 4s - loss: 0.0174 - acc: 0.995 - ETA: 3s - loss: 0.0172 - acc: 0.995 - ETA: 3s - loss: 0.0170 - acc: 0.995 - ETA: 2s - loss: 0.0168 - acc: 0.995 - ETA: 2s - loss: 0.0165 - acc: 0.995 - ETA: 1s - loss: 0.0164 - acc: 0.995 - ETA: 1s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0158 - acc: 0.996 - 20s 5ms/step - loss: 0.0160 - acc: 0.9961 - val_loss: 0.0121 - val_acc: 0.9945\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.01085\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4340/4340 [==============================] - ETA: 15s - loss: 0.0074 - acc: 1.00 - ETA: 17s - loss: 0.0088 - acc: 1.00 - ETA: 17s - loss: 0.0093 - acc: 1.00 - ETA: 16s - loss: 0.0086 - acc: 1.00 - ETA: 16s - loss: 0.0089 - acc: 1.00 - ETA: 15s - loss: 0.0084 - acc: 1.00 - ETA: 14s - loss: 0.0109 - acc: 0.99 - ETA: 14s - loss: 0.0126 - acc: 0.99 - ETA: 13s - loss: 0.0135 - acc: 0.99 - ETA: 13s - loss: 0.0134 - acc: 0.99 - ETA: 12s - loss: 0.0129 - acc: 0.99 - ETA: 11s - loss: 0.0130 - acc: 0.99 - ETA: 11s - loss: 0.0133 - acc: 0.99 - ETA: 10s - loss: 0.0134 - acc: 0.99 - ETA: 10s - loss: 0.0137 - acc: 0.99 - ETA: 9s - loss: 0.0139 - acc: 0.9976 - ETA: 9s - loss: 0.0145 - acc: 0.997 - ETA: 8s - loss: 0.0144 - acc: 0.997 - ETA: 7s - loss: 0.0143 - acc: 0.997 - ETA: 7s - loss: 0.0152 - acc: 0.997 - ETA: 6s - loss: 0.0148 - acc: 0.997 - ETA: 6s - loss: 0.0156 - acc: 0.996 - ETA: 5s - loss: 0.0153 - acc: 0.996 - ETA: 5s - loss: 0.0153 - acc: 0.997 - ETA: 4s - loss: 0.0151 - acc: 0.997 - ETA: 4s - loss: 0.0150 - acc: 0.997 - ETA: 3s - loss: 0.0147 - acc: 0.997 - ETA: 3s - loss: 0.0145 - acc: 0.997 - ETA: 2s - loss: 0.0142 - acc: 0.997 - ETA: 2s - loss: 0.0141 - acc: 0.997 - ETA: 1s - loss: 0.0138 - acc: 0.997 - ETA: 1s - loss: 0.0137 - acc: 0.997 - ETA: 0s - loss: 0.0134 - acc: 0.997 - 19s 4ms/step - loss: 0.0132 - acc: 0.9977 - val_loss: 0.0154 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.01085\n",
      "Epoch 57/100\n",
      "4340/4340 [==============================] - ETA: 18s - loss: 0.0042 - acc: 1.00 - ETA: 15s - loss: 0.0137 - acc: 0.99 - ETA: 16s - loss: 0.0141 - acc: 0.99 - ETA: 16s - loss: 0.0128 - acc: 0.99 - ETA: 15s - loss: 0.0117 - acc: 0.99 - ETA: 14s - loss: 0.0128 - acc: 0.99 - ETA: 14s - loss: 0.0128 - acc: 0.99 - ETA: 14s - loss: 0.0129 - acc: 0.99 - ETA: 13s - loss: 0.0129 - acc: 0.99 - ETA: 12s - loss: 0.0123 - acc: 0.99 - ETA: 12s - loss: 0.0129 - acc: 0.99 - ETA: 11s - loss: 0.0130 - acc: 0.99 - ETA: 11s - loss: 0.0137 - acc: 0.99 - ETA: 10s - loss: 0.0134 - acc: 0.99 - ETA: 10s - loss: 0.0130 - acc: 0.99 - ETA: 9s - loss: 0.0133 - acc: 0.9976 - ETA: 8s - loss: 0.0130 - acc: 0.997 - ETA: 8s - loss: 0.0129 - acc: 0.997 - ETA: 7s - loss: 0.0128 - acc: 0.997 - ETA: 7s - loss: 0.0124 - acc: 0.998 - ETA: 6s - loss: 0.0123 - acc: 0.998 - ETA: 6s - loss: 0.0120 - acc: 0.998 - ETA: 5s - loss: 0.0119 - acc: 0.998 - ETA: 5s - loss: 0.0122 - acc: 0.998 - ETA: 4s - loss: 0.0119 - acc: 0.998 - ETA: 4s - loss: 0.0120 - acc: 0.997 - ETA: 3s - loss: 0.0120 - acc: 0.998 - ETA: 3s - loss: 0.0118 - acc: 0.998 - ETA: 2s - loss: 0.0115 - acc: 0.998 - ETA: 2s - loss: 0.0115 - acc: 0.998 - ETA: 1s - loss: 0.0118 - acc: 0.997 - ETA: 0s - loss: 0.0116 - acc: 0.997 - ETA: 0s - loss: 0.0115 - acc: 0.997 - 21s 5ms/step - loss: 0.0115 - acc: 0.9979 - val_loss: 0.0095 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.01085 to 0.00953, saving model to weight_MobileNet.hdf5\n",
      "Epoch 58/100\n",
      "4340/4340 [==============================] - ETA: 15s - loss: 0.0068 - acc: 1.00 - ETA: 17s - loss: 0.0116 - acc: 0.99 - ETA: 16s - loss: 0.0135 - acc: 0.99 - ETA: 15s - loss: 0.0114 - acc: 0.99 - ETA: 14s - loss: 0.0110 - acc: 0.99 - ETA: 14s - loss: 0.0126 - acc: 0.99 - ETA: 13s - loss: 0.0116 - acc: 0.99 - ETA: 13s - loss: 0.0107 - acc: 0.99 - ETA: 13s - loss: 0.0102 - acc: 0.99 - ETA: 12s - loss: 0.0103 - acc: 0.99 - ETA: 12s - loss: 0.0105 - acc: 0.99 - ETA: 12s - loss: 0.0107 - acc: 0.99 - ETA: 11s - loss: 0.0104 - acc: 0.99 - ETA: 10s - loss: 0.0100 - acc: 0.99 - ETA: 10s - loss: 0.0099 - acc: 0.99 - ETA: 9s - loss: 0.0101 - acc: 0.9980 - ETA: 9s - loss: 0.0099 - acc: 0.998 - ETA: 8s - loss: 0.0101 - acc: 0.998 - ETA: 8s - loss: 0.0099 - acc: 0.998 - ETA: 7s - loss: 0.0097 - acc: 0.998 - ETA: 6s - loss: 0.0094 - acc: 0.998 - ETA: 6s - loss: 0.0094 - acc: 0.998 - ETA: 5s - loss: 0.0091 - acc: 0.998 - ETA: 5s - loss: 0.0089 - acc: 0.998 - ETA: 4s - loss: 0.0093 - acc: 0.998 - ETA: 4s - loss: 0.0101 - acc: 0.998 - ETA: 3s - loss: 0.0100 - acc: 0.998 - ETA: 3s - loss: 0.0104 - acc: 0.998 - ETA: 2s - loss: 0.0102 - acc: 0.998 - ETA: 2s - loss: 0.0101 - acc: 0.998 - ETA: 1s - loss: 0.0103 - acc: 0.998 - ETA: 0s - loss: 0.0100 - acc: 0.998 - ETA: 0s - loss: 0.0101 - acc: 0.998 - 19s 4ms/step - loss: 0.0100 - acc: 0.9986 - val_loss: 0.0087 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00953 to 0.00874, saving model to weight_MobileNet.hdf5\n",
      "Epoch 59/100\n",
      "4340/4340 [==============================] - ETA: 15s - loss: 0.0218 - acc: 0.98 - ETA: 17s - loss: 0.0145 - acc: 0.99 - ETA: 15s - loss: 0.0120 - acc: 0.99 - ETA: 15s - loss: 0.0113 - acc: 0.99 - ETA: 14s - loss: 0.0113 - acc: 0.99 - ETA: 14s - loss: 0.0106 - acc: 0.99 - ETA: 13s - loss: 0.0117 - acc: 0.99 - ETA: 13s - loss: 0.0123 - acc: 0.99 - ETA: 12s - loss: 0.0117 - acc: 0.99 - ETA: 12s - loss: 0.0123 - acc: 0.99 - ETA: 11s - loss: 0.0133 - acc: 0.99 - ETA: 11s - loss: 0.0134 - acc: 0.99 - ETA: 10s - loss: 0.0137 - acc: 0.99 - ETA: 10s - loss: 0.0150 - acc: 0.99 - ETA: 9s - loss: 0.0147 - acc: 0.9964 - ETA: 9s - loss: 0.0142 - acc: 0.996 - ETA: 8s - loss: 0.0138 - acc: 0.996 - ETA: 8s - loss: 0.0134 - acc: 0.997 - ETA: 7s - loss: 0.0131 - acc: 0.997 - ETA: 7s - loss: 0.0128 - acc: 0.997 - ETA: 6s - loss: 0.0128 - acc: 0.997 - ETA: 6s - loss: 0.0131 - acc: 0.997 - ETA: 5s - loss: 0.0129 - acc: 0.997 - ETA: 5s - loss: 0.0126 - acc: 0.997 - ETA: 4s - loss: 0.0123 - acc: 0.997 - ETA: 4s - loss: 0.0119 - acc: 0.997 - ETA: 3s - loss: 0.0119 - acc: 0.998 - ETA: 3s - loss: 0.0119 - acc: 0.997 - ETA: 2s - loss: 0.0118 - acc: 0.997 - ETA: 2s - loss: 0.0116 - acc: 0.997 - ETA: 1s - loss: 0.0114 - acc: 0.998 - ETA: 1s - loss: 0.0114 - acc: 0.997 - ETA: 0s - loss: 0.0118 - acc: 0.997 - 20s 5ms/step - loss: 0.0116 - acc: 0.9977 - val_loss: 0.0121 - val_acc: 0.9945\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00874\n",
      "Epoch 60/100\n",
      "4340/4340 [==============================] - ETA: 18s - loss: 0.0063 - acc: 1.00 - ETA: 32s - loss: 0.0083 - acc: 1.00 - ETA: 28s - loss: 0.0095 - acc: 1.00 - ETA: 24s - loss: 0.0094 - acc: 1.00 - ETA: 24s - loss: 0.0083 - acc: 1.00 - ETA: 21s - loss: 0.0096 - acc: 0.99 - ETA: 19s - loss: 0.0103 - acc: 0.99 - ETA: 18s - loss: 0.0104 - acc: 0.99 - ETA: 17s - loss: 0.0098 - acc: 0.99 - ETA: 17s - loss: 0.0099 - acc: 0.99 - ETA: 15s - loss: 0.0094 - acc: 0.99 - ETA: 14s - loss: 0.0089 - acc: 0.99 - ETA: 14s - loss: 0.0086 - acc: 0.99 - ETA: 13s - loss: 0.0089 - acc: 0.99 - ETA: 13s - loss: 0.0097 - acc: 0.99 - ETA: 12s - loss: 0.0101 - acc: 0.99 - ETA: 11s - loss: 0.0097 - acc: 0.99 - ETA: 10s - loss: 0.0094 - acc: 0.99 - ETA: 9s - loss: 0.0094 - acc: 0.9988 - ETA: 9s - loss: 0.0096 - acc: 0.998 - ETA: 8s - loss: 0.0100 - acc: 0.998 - ETA: 7s - loss: 0.0106 - acc: 0.998 - ETA: 7s - loss: 0.0106 - acc: 0.998 - ETA: 6s - loss: 0.0105 - acc: 0.998 - ETA: 5s - loss: 0.0113 - acc: 0.998 - ETA: 5s - loss: 0.0112 - acc: 0.998 - ETA: 4s - loss: 0.0112 - acc: 0.998 - ETA: 4s - loss: 0.0112 - acc: 0.998 - ETA: 3s - loss: 0.0112 - acc: 0.998 - ETA: 2s - loss: 0.0112 - acc: 0.998 - ETA: 2s - loss: 0.0111 - acc: 0.998 - ETA: 1s - loss: 0.0109 - acc: 0.998 - ETA: 0s - loss: 0.0108 - acc: 0.998 - 26s 6ms/step - loss: 0.0107 - acc: 0.9986 - val_loss: 0.0086 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00874 to 0.00861, saving model to weight_MobileNet.hdf5\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4340/4340 [==============================] - ETA: 16s - loss: 0.0202 - acc: 0.99 - ETA: 20s - loss: 0.0183 - acc: 0.98 - ETA: 18s - loss: 0.0191 - acc: 0.98 - ETA: 19s - loss: 0.0160 - acc: 0.99 - ETA: 19s - loss: 0.0167 - acc: 0.99 - ETA: 17s - loss: 0.0149 - acc: 0.99 - ETA: 17s - loss: 0.0140 - acc: 0.99 - ETA: 16s - loss: 0.0148 - acc: 0.99 - ETA: 15s - loss: 0.0149 - acc: 0.99 - ETA: 15s - loss: 0.0151 - acc: 0.99 - ETA: 14s - loss: 0.0141 - acc: 0.99 - ETA: 14s - loss: 0.0138 - acc: 0.99 - ETA: 13s - loss: 0.0139 - acc: 0.99 - ETA: 12s - loss: 0.0133 - acc: 0.99 - ETA: 11s - loss: 0.0131 - acc: 0.99 - ETA: 11s - loss: 0.0139 - acc: 0.99 - ETA: 10s - loss: 0.0135 - acc: 0.99 - ETA: 9s - loss: 0.0133 - acc: 0.9961 - ETA: 9s - loss: 0.0129 - acc: 0.996 - ETA: 8s - loss: 0.0133 - acc: 0.995 - ETA: 8s - loss: 0.0140 - acc: 0.995 - ETA: 7s - loss: 0.0140 - acc: 0.995 - ETA: 6s - loss: 0.0144 - acc: 0.994 - ETA: 6s - loss: 0.0144 - acc: 0.994 - ETA: 5s - loss: 0.0143 - acc: 0.995 - ETA: 5s - loss: 0.0141 - acc: 0.995 - ETA: 4s - loss: 0.0140 - acc: 0.995 - ETA: 3s - loss: 0.0139 - acc: 0.995 - ETA: 3s - loss: 0.0141 - acc: 0.995 - ETA: 2s - loss: 0.0139 - acc: 0.995 - ETA: 1s - loss: 0.0140 - acc: 0.995 - ETA: 1s - loss: 0.0139 - acc: 0.995 - ETA: 0s - loss: 0.0136 - acc: 0.995 - 24s 5ms/step - loss: 0.0140 - acc: 0.9954 - val_loss: 0.0088 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00861\n",
      "Epoch 62/100\n",
      "4340/4340 [==============================] - ETA: 13s - loss: 0.0143 - acc: 1.00 - ETA: 19s - loss: 0.0132 - acc: 1.00 - ETA: 19s - loss: 0.0119 - acc: 1.00 - ETA: 17s - loss: 0.0112 - acc: 1.00 - ETA: 17s - loss: 0.0111 - acc: 1.00 - ETA: 18s - loss: 0.0117 - acc: 1.00 - ETA: 16s - loss: 0.0113 - acc: 1.00 - ETA: 15s - loss: 0.0113 - acc: 1.00 - ETA: 14s - loss: 0.0109 - acc: 1.00 - ETA: 14s - loss: 0.0101 - acc: 1.00 - ETA: 14s - loss: 0.0098 - acc: 1.00 - ETA: 13s - loss: 0.0098 - acc: 1.00 - ETA: 13s - loss: 0.0111 - acc: 0.99 - ETA: 13s - loss: 0.0111 - acc: 0.99 - ETA: 12s - loss: 0.0113 - acc: 0.99 - ETA: 12s - loss: 0.0110 - acc: 0.99 - ETA: 11s - loss: 0.0117 - acc: 0.99 - ETA: 11s - loss: 0.0113 - acc: 0.99 - ETA: 10s - loss: 0.0109 - acc: 0.99 - ETA: 9s - loss: 0.0106 - acc: 0.9988 - ETA: 9s - loss: 0.0106 - acc: 0.998 - ETA: 8s - loss: 0.0104 - acc: 0.998 - ETA: 7s - loss: 0.0102 - acc: 0.999 - ETA: 7s - loss: 0.0100 - acc: 0.999 - ETA: 6s - loss: 0.0097 - acc: 0.999 - ETA: 5s - loss: 0.0099 - acc: 0.998 - ETA: 4s - loss: 0.0098 - acc: 0.998 - ETA: 4s - loss: 0.0097 - acc: 0.998 - ETA: 3s - loss: 0.0097 - acc: 0.998 - ETA: 2s - loss: 0.0095 - acc: 0.999 - ETA: 2s - loss: 0.0093 - acc: 0.999 - ETA: 1s - loss: 0.0095 - acc: 0.998 - ETA: 0s - loss: 0.0094 - acc: 0.998 - 27s 6ms/step - loss: 0.0092 - acc: 0.9988 - val_loss: 0.0079 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00861 to 0.00792, saving model to weight_MobileNet.hdf5\n",
      "Epoch 63/100\n",
      "4340/4340 [==============================] - ETA: 15s - loss: 0.0051 - acc: 1.00 - ETA: 21s - loss: 0.0060 - acc: 1.00 - ETA: 21s - loss: 0.0087 - acc: 0.99 - ETA: 21s - loss: 0.0105 - acc: 0.99 - ETA: 22s - loss: 0.0090 - acc: 0.99 - ETA: 19s - loss: 0.0084 - acc: 0.99 - ETA: 18s - loss: 0.0080 - acc: 0.99 - ETA: 17s - loss: 0.0083 - acc: 0.99 - ETA: 17s - loss: 0.0083 - acc: 0.99 - ETA: 16s - loss: 0.0079 - acc: 0.99 - ETA: 15s - loss: 0.0078 - acc: 0.99 - ETA: 15s - loss: 0.0080 - acc: 0.99 - ETA: 14s - loss: 0.0078 - acc: 0.99 - ETA: 14s - loss: 0.0085 - acc: 0.99 - ETA: 13s - loss: 0.0083 - acc: 0.99 - ETA: 12s - loss: 0.0082 - acc: 0.99 - ETA: 11s - loss: 0.0083 - acc: 0.99 - ETA: 10s - loss: 0.0086 - acc: 0.99 - ETA: 9s - loss: 0.0084 - acc: 0.9984 - ETA: 9s - loss: 0.0082 - acc: 0.998 - ETA: 8s - loss: 0.0085 - acc: 0.998 - ETA: 7s - loss: 0.0085 - acc: 0.998 - ETA: 6s - loss: 0.0082 - acc: 0.998 - ETA: 6s - loss: 0.0082 - acc: 0.998 - ETA: 5s - loss: 0.0082 - acc: 0.998 - ETA: 5s - loss: 0.0082 - acc: 0.998 - ETA: 4s - loss: 0.0083 - acc: 0.998 - ETA: 3s - loss: 0.0083 - acc: 0.998 - ETA: 3s - loss: 0.0082 - acc: 0.998 - ETA: 2s - loss: 0.0087 - acc: 0.998 - ETA: 1s - loss: 0.0089 - acc: 0.998 - ETA: 1s - loss: 0.0088 - acc: 0.998 - ETA: 0s - loss: 0.0098 - acc: 0.997 - 23s 5ms/step - loss: 0.0097 - acc: 0.9977 - val_loss: 0.0163 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00792\n",
      "Epoch 64/100\n",
      "4340/4340 [==============================] - ETA: 20s - loss: 0.0074 - acc: 1.00 - ETA: 16s - loss: 0.0078 - acc: 1.00 - ETA: 15s - loss: 0.0067 - acc: 1.00 - ETA: 14s - loss: 0.0064 - acc: 1.00 - ETA: 14s - loss: 0.0061 - acc: 1.00 - ETA: 17s - loss: 0.0061 - acc: 1.00 - ETA: 17s - loss: 0.0056 - acc: 1.00 - ETA: 15s - loss: 0.0054 - acc: 1.00 - ETA: 15s - loss: 0.0073 - acc: 0.99 - ETA: 15s - loss: 0.0075 - acc: 0.99 - ETA: 14s - loss: 0.0073 - acc: 0.99 - ETA: 13s - loss: 0.0072 - acc: 0.99 - ETA: 12s - loss: 0.0070 - acc: 0.99 - ETA: 12s - loss: 0.0071 - acc: 0.99 - ETA: 11s - loss: 0.0072 - acc: 0.99 - ETA: 10s - loss: 0.0070 - acc: 0.99 - ETA: 10s - loss: 0.0069 - acc: 0.99 - ETA: 10s - loss: 0.0069 - acc: 0.99 - ETA: 9s - loss: 0.0068 - acc: 0.9992 - ETA: 8s - loss: 0.0068 - acc: 0.999 - ETA: 8s - loss: 0.0068 - acc: 0.999 - ETA: 7s - loss: 0.0070 - acc: 0.999 - ETA: 7s - loss: 0.0070 - acc: 0.999 - ETA: 6s - loss: 0.0070 - acc: 0.999 - ETA: 5s - loss: 0.0072 - acc: 0.999 - ETA: 5s - loss: 0.0072 - acc: 0.999 - ETA: 4s - loss: 0.0070 - acc: 0.999 - ETA: 3s - loss: 0.0072 - acc: 0.999 - ETA: 3s - loss: 0.0072 - acc: 0.999 - ETA: 2s - loss: 0.0071 - acc: 0.999 - ETA: 1s - loss: 0.0070 - acc: 0.999 - ETA: 1s - loss: 0.0069 - acc: 0.999 - ETA: 0s - loss: 0.0068 - acc: 0.999 - 23s 5ms/step - loss: 0.0067 - acc: 0.9995 - val_loss: 0.0076 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00792 to 0.00764, saving model to weight_MobileNet.hdf5\n",
      "Epoch 65/100\n",
      "4340/4340 [==============================] - ETA: 17s - loss: 0.0013 - acc: 1.00 - ETA: 16s - loss: 0.0037 - acc: 1.00 - ETA: 15s - loss: 0.0035 - acc: 1.00 - ETA: 18s - loss: 0.0055 - acc: 1.00 - ETA: 19s - loss: 0.0056 - acc: 1.00 - ETA: 17s - loss: 0.0069 - acc: 0.99 - ETA: 17s - loss: 0.0079 - acc: 0.99 - ETA: 15s - loss: 0.0075 - acc: 0.99 - ETA: 15s - loss: 0.0073 - acc: 0.99 - ETA: 15s - loss: 0.0076 - acc: 0.99 - ETA: 14s - loss: 0.0072 - acc: 0.99 - ETA: 14s - loss: 0.0072 - acc: 0.99 - ETA: 13s - loss: 0.0074 - acc: 0.99 - ETA: 12s - loss: 0.0079 - acc: 0.99 - ETA: 12s - loss: 0.0079 - acc: 0.99 - ETA: 12s - loss: 0.0083 - acc: 0.99 - ETA: 11s - loss: 0.0080 - acc: 0.99 - ETA: 11s - loss: 0.0077 - acc: 0.99 - ETA: 10s - loss: 0.0078 - acc: 0.99 - ETA: 9s - loss: 0.0079 - acc: 0.9992 - ETA: 8s - loss: 0.0079 - acc: 0.999 - ETA: 8s - loss: 0.0080 - acc: 0.999 - ETA: 7s - loss: 0.0082 - acc: 0.999 - ETA: 6s - loss: 0.0081 - acc: 0.999 - ETA: 5s - loss: 0.0081 - acc: 0.999 - ETA: 5s - loss: 0.0086 - acc: 0.999 - ETA: 4s - loss: 0.0090 - acc: 0.999 - ETA: 3s - loss: 0.0090 - acc: 0.999 - ETA: 3s - loss: 0.0093 - acc: 0.998 - ETA: 2s - loss: 0.0098 - acc: 0.998 - ETA: 1s - loss: 0.0098 - acc: 0.998 - ETA: 1s - loss: 0.0098 - acc: 0.998 - ETA: 0s - loss: 0.0098 - acc: 0.998 - 24s 6ms/step - loss: 0.0096 - acc: 0.9988 - val_loss: 0.0093 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00764\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4340/4340 [==============================] - ETA: 20s - loss: 0.0277 - acc: 0.99 - ETA: 20s - loss: 0.0166 - acc: 0.99 - ETA: 18s - loss: 0.0114 - acc: 0.99 - ETA: 19s - loss: 0.0136 - acc: 0.99 - ETA: 17s - loss: 0.0121 - acc: 0.99 - ETA: 16s - loss: 0.0129 - acc: 0.99 - ETA: 16s - loss: 0.0124 - acc: 0.99 - ETA: 14s - loss: 0.0115 - acc: 0.99 - ETA: 14s - loss: 0.0107 - acc: 0.99 - ETA: 13s - loss: 0.0100 - acc: 0.99 - ETA: 13s - loss: 0.0111 - acc: 0.99 - ETA: 12s - loss: 0.0107 - acc: 0.99 - ETA: 11s - loss: 0.0104 - acc: 0.99 - ETA: 10s - loss: 0.0101 - acc: 0.99 - ETA: 10s - loss: 0.0100 - acc: 0.99 - ETA: 10s - loss: 0.0101 - acc: 0.99 - ETA: 9s - loss: 0.0099 - acc: 0.9986 - ETA: 9s - loss: 0.0095 - acc: 0.998 - ETA: 8s - loss: 0.0106 - acc: 0.998 - ETA: 8s - loss: 0.0109 - acc: 0.998 - ETA: 8s - loss: 0.0107 - acc: 0.998 - ETA: 7s - loss: 0.0103 - acc: 0.998 - ETA: 6s - loss: 0.0104 - acc: 0.998 - ETA: 6s - loss: 0.0102 - acc: 0.998 - ETA: 5s - loss: 0.0101 - acc: 0.998 - ETA: 5s - loss: 0.0101 - acc: 0.998 - ETA: 4s - loss: 0.0103 - acc: 0.998 - ETA: 3s - loss: 0.0101 - acc: 0.998 - ETA: 3s - loss: 0.0100 - acc: 0.998 - ETA: 2s - loss: 0.0098 - acc: 0.998 - ETA: 1s - loss: 0.0099 - acc: 0.998 - ETA: 1s - loss: 0.0098 - acc: 0.998 - ETA: 0s - loss: 0.0097 - acc: 0.998 - 22s 5ms/step - loss: 0.0095 - acc: 0.9984 - val_loss: 0.0086 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00764\n",
      "Epoch 67/100\n",
      "4340/4340 [==============================] - ETA: 13s - loss: 0.0047 - acc: 1.00 - ETA: 16s - loss: 0.0064 - acc: 1.00 - ETA: 16s - loss: 0.0055 - acc: 1.00 - ETA: 15s - loss: 0.0121 - acc: 0.99 - ETA: 14s - loss: 0.0104 - acc: 0.99 - ETA: 14s - loss: 0.0103 - acc: 0.99 - ETA: 13s - loss: 0.0099 - acc: 0.99 - ETA: 13s - loss: 0.0107 - acc: 0.99 - ETA: 12s - loss: 0.0110 - acc: 0.99 - ETA: 12s - loss: 0.0105 - acc: 0.99 - ETA: 11s - loss: 0.0117 - acc: 0.99 - ETA: 11s - loss: 0.0114 - acc: 0.99 - ETA: 10s - loss: 0.0119 - acc: 0.99 - ETA: 10s - loss: 0.0117 - acc: 0.99 - ETA: 9s - loss: 0.0114 - acc: 0.9969 - ETA: 9s - loss: 0.0110 - acc: 0.997 - ETA: 8s - loss: 0.0111 - acc: 0.997 - ETA: 8s - loss: 0.0109 - acc: 0.997 - ETA: 8s - loss: 0.0105 - acc: 0.997 - ETA: 7s - loss: 0.0103 - acc: 0.997 - ETA: 6s - loss: 0.0100 - acc: 0.997 - ETA: 6s - loss: 0.0099 - acc: 0.997 - ETA: 5s - loss: 0.0098 - acc: 0.998 - ETA: 5s - loss: 0.0095 - acc: 0.998 - ETA: 4s - loss: 0.0096 - acc: 0.998 - ETA: 4s - loss: 0.0093 - acc: 0.998 - ETA: 3s - loss: 0.0095 - acc: 0.998 - ETA: 3s - loss: 0.0094 - acc: 0.998 - ETA: 2s - loss: 0.0095 - acc: 0.998 - ETA: 2s - loss: 0.0093 - acc: 0.998 - ETA: 1s - loss: 0.0092 - acc: 0.998 - ETA: 1s - loss: 0.0093 - acc: 0.998 - ETA: 0s - loss: 0.0092 - acc: 0.998 - 21s 5ms/step - loss: 0.0091 - acc: 0.9984 - val_loss: 0.0085 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00764\n",
      "Epoch 68/100\n",
      "4340/4340 [==============================] - ETA: 18s - loss: 0.0044 - acc: 1.00 - ETA: 15s - loss: 0.0085 - acc: 1.00 - ETA: 18s - loss: 0.0088 - acc: 1.00 - ETA: 19s - loss: 0.0077 - acc: 1.00 - ETA: 20s - loss: 0.0070 - acc: 1.00 - ETA: 18s - loss: 0.0062 - acc: 1.00 - ETA: 20s - loss: 0.0059 - acc: 1.00 - ETA: 20s - loss: 0.0086 - acc: 0.99 - ETA: 18s - loss: 0.0080 - acc: 0.99 - ETA: 17s - loss: 0.0087 - acc: 0.99 - ETA: 16s - loss: 0.0087 - acc: 0.99 - ETA: 15s - loss: 0.0088 - acc: 0.99 - ETA: 14s - loss: 0.0093 - acc: 0.99 - ETA: 14s - loss: 0.0093 - acc: 0.99 - ETA: 13s - loss: 0.0088 - acc: 0.99 - ETA: 12s - loss: 0.0087 - acc: 0.99 - ETA: 12s - loss: 0.0085 - acc: 0.99 - ETA: 11s - loss: 0.0083 - acc: 0.99 - ETA: 10s - loss: 0.0083 - acc: 0.99 - ETA: 10s - loss: 0.0082 - acc: 0.99 - ETA: 9s - loss: 0.0080 - acc: 0.9989 - ETA: 8s - loss: 0.0082 - acc: 0.998 - ETA: 8s - loss: 0.0081 - acc: 0.999 - ETA: 7s - loss: 0.0080 - acc: 0.999 - ETA: 6s - loss: 0.0079 - acc: 0.999 - ETA: 5s - loss: 0.0078 - acc: 0.999 - ETA: 5s - loss: 0.0079 - acc: 0.999 - ETA: 4s - loss: 0.0077 - acc: 0.999 - ETA: 3s - loss: 0.0078 - acc: 0.998 - ETA: 2s - loss: 0.0077 - acc: 0.999 - ETA: 2s - loss: 0.0076 - acc: 0.999 - ETA: 1s - loss: 0.0075 - acc: 0.999 - ETA: 0s - loss: 0.0076 - acc: 0.999 - 27s 6ms/step - loss: 0.0078 - acc: 0.9988 - val_loss: 0.0093 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00764\n",
      "Epoch 69/100\n",
      "4340/4340 [==============================] - ETA: 30s - loss: 0.0083 - acc: 1.00 - ETA: 28s - loss: 0.0095 - acc: 1.00 - ETA: 25s - loss: 0.0097 - acc: 1.00 - ETA: 22s - loss: 0.0090 - acc: 1.00 - ETA: 22s - loss: 0.0094 - acc: 1.00 - ETA: 23s - loss: 0.0080 - acc: 1.00 - ETA: 22s - loss: 0.0119 - acc: 0.99 - ETA: 21s - loss: 0.0133 - acc: 0.99 - ETA: 20s - loss: 0.0125 - acc: 0.99 - ETA: 19s - loss: 0.0117 - acc: 0.99 - ETA: 18s - loss: 0.0110 - acc: 0.99 - ETA: 17s - loss: 0.0105 - acc: 0.99 - ETA: 17s - loss: 0.0099 - acc: 0.99 - ETA: 16s - loss: 0.0101 - acc: 0.99 - ETA: 14s - loss: 0.0097 - acc: 0.99 - ETA: 13s - loss: 0.0102 - acc: 0.99 - ETA: 12s - loss: 0.0107 - acc: 0.99 - ETA: 11s - loss: 0.0104 - acc: 0.99 - ETA: 11s - loss: 0.0103 - acc: 0.99 - ETA: 10s - loss: 0.0100 - acc: 0.99 - ETA: 9s - loss: 0.0099 - acc: 0.9978 - ETA: 8s - loss: 0.0097 - acc: 0.997 - ETA: 8s - loss: 0.0095 - acc: 0.998 - ETA: 7s - loss: 0.0092 - acc: 0.998 - ETA: 6s - loss: 0.0089 - acc: 0.998 - ETA: 6s - loss: 0.0088 - acc: 0.998 - ETA: 5s - loss: 0.0087 - acc: 0.998 - ETA: 4s - loss: 0.0086 - acc: 0.998 - ETA: 3s - loss: 0.0087 - acc: 0.998 - ETA: 2s - loss: 0.0087 - acc: 0.998 - ETA: 2s - loss: 0.0085 - acc: 0.998 - ETA: 1s - loss: 0.0084 - acc: 0.998 - ETA: 0s - loss: 0.0083 - acc: 0.998 - 28s 6ms/step - loss: 0.0086 - acc: 0.9986 - val_loss: 0.0101 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00764\n",
      "Epoch 70/100\n",
      "4340/4340 [==============================] - ETA: 26s - loss: 0.0049 - acc: 1.00 - ETA: 25s - loss: 0.0059 - acc: 1.00 - ETA: 25s - loss: 0.0052 - acc: 1.00 - ETA: 26s - loss: 0.0059 - acc: 1.00 - ETA: 27s - loss: 0.0084 - acc: 0.99 - ETA: 28s - loss: 0.0079 - acc: 0.99 - ETA: 27s - loss: 0.0070 - acc: 0.99 - ETA: 26s - loss: 0.0069 - acc: 0.99 - ETA: 26s - loss: 0.0070 - acc: 0.99 - ETA: 25s - loss: 0.0066 - acc: 0.99 - ETA: 24s - loss: 0.0063 - acc: 0.99 - ETA: 24s - loss: 0.0064 - acc: 0.99 - ETA: 23s - loss: 0.0066 - acc: 0.99 - ETA: 23s - loss: 0.0063 - acc: 0.99 - ETA: 22s - loss: 0.0063 - acc: 0.99 - ETA: 21s - loss: 0.0064 - acc: 0.99 - ETA: 20s - loss: 0.0071 - acc: 0.99 - ETA: 18s - loss: 0.0070 - acc: 0.99 - ETA: 17s - loss: 0.0069 - acc: 0.99 - ETA: 15s - loss: 0.0067 - acc: 0.99 - ETA: 14s - loss: 0.0066 - acc: 0.99 - ETA: 12s - loss: 0.0065 - acc: 0.99 - ETA: 11s - loss: 0.0069 - acc: 0.99 - ETA: 10s - loss: 0.0068 - acc: 0.99 - ETA: 9s - loss: 0.0067 - acc: 0.9994 - ETA: 7s - loss: 0.0066 - acc: 0.999 - ETA: 6s - loss: 0.0066 - acc: 0.999 - ETA: 5s - loss: 0.0069 - acc: 0.999 - ETA: 4s - loss: 0.0074 - acc: 0.998 - ETA: 3s - loss: 0.0073 - acc: 0.999 - ETA: 2s - loss: 0.0072 - acc: 0.999 - ETA: 1s - loss: 0.0071 - acc: 0.999 - ETA: 0s - loss: 0.0073 - acc: 0.999 - 33s 8ms/step - loss: 0.0072 - acc: 0.9991 - val_loss: 0.0085 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00764\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4340/4340 [==============================] - ETA: 16s - loss: 0.0074 - acc: 1.00 - ETA: 17s - loss: 0.0137 - acc: 0.99 - ETA: 16s - loss: 0.0094 - acc: 0.99 - ETA: 16s - loss: 0.0087 - acc: 0.99 - ETA: 16s - loss: 0.0074 - acc: 0.99 - ETA: 15s - loss: 0.0079 - acc: 0.99 - ETA: 15s - loss: 0.0073 - acc: 0.99 - ETA: 14s - loss: 0.0086 - acc: 0.99 - ETA: 14s - loss: 0.0088 - acc: 0.99 - ETA: 14s - loss: 0.0084 - acc: 0.99 - ETA: 13s - loss: 0.0078 - acc: 0.99 - ETA: 13s - loss: 0.0084 - acc: 0.99 - ETA: 12s - loss: 0.0098 - acc: 0.99 - ETA: 12s - loss: 0.0099 - acc: 0.99 - ETA: 11s - loss: 0.0099 - acc: 0.99 - ETA: 11s - loss: 0.0095 - acc: 0.99 - ETA: 10s - loss: 0.0096 - acc: 0.99 - ETA: 9s - loss: 0.0093 - acc: 0.9987 - ETA: 9s - loss: 0.0092 - acc: 0.998 - ETA: 8s - loss: 0.0099 - acc: 0.998 - ETA: 8s - loss: 0.0105 - acc: 0.997 - ETA: 7s - loss: 0.0110 - acc: 0.997 - ETA: 7s - loss: 0.0106 - acc: 0.997 - ETA: 6s - loss: 0.0106 - acc: 0.997 - ETA: 5s - loss: 0.0106 - acc: 0.997 - ETA: 5s - loss: 0.0104 - acc: 0.997 - ETA: 4s - loss: 0.0103 - acc: 0.998 - ETA: 4s - loss: 0.0106 - acc: 0.998 - ETA: 3s - loss: 0.0105 - acc: 0.998 - ETA: 2s - loss: 0.0106 - acc: 0.998 - ETA: 2s - loss: 0.0104 - acc: 0.998 - ETA: 1s - loss: 0.0101 - acc: 0.998 - ETA: 0s - loss: 0.0103 - acc: 0.998 - 27s 6ms/step - loss: 0.0102 - acc: 0.9982 - val_loss: 0.0151 - val_acc: 0.9945\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00764\n",
      "Epoch 72/100\n",
      "4340/4340 [==============================] - ETA: 23s - loss: 0.0176 - acc: 1.00 - ETA: 20s - loss: 0.0135 - acc: 1.00 - ETA: 19s - loss: 0.0108 - acc: 1.00 - ETA: 17s - loss: 0.0091 - acc: 1.00 - ETA: 17s - loss: 0.0090 - acc: 1.00 - ETA: 16s - loss: 0.0101 - acc: 0.99 - ETA: 15s - loss: 0.0094 - acc: 0.99 - ETA: 15s - loss: 0.0116 - acc: 0.99 - ETA: 15s - loss: 0.0110 - acc: 0.99 - ETA: 15s - loss: 0.0126 - acc: 0.99 - ETA: 14s - loss: 0.0119 - acc: 0.99 - ETA: 13s - loss: 0.0113 - acc: 0.99 - ETA: 13s - loss: 0.0116 - acc: 0.99 - ETA: 12s - loss: 0.0117 - acc: 0.99 - ETA: 12s - loss: 0.0115 - acc: 0.99 - ETA: 11s - loss: 0.0114 - acc: 0.99 - ETA: 10s - loss: 0.0109 - acc: 0.99 - ETA: 10s - loss: 0.0105 - acc: 0.99 - ETA: 9s - loss: 0.0103 - acc: 0.9975 - ETA: 8s - loss: 0.0101 - acc: 0.997 - ETA: 8s - loss: 0.0102 - acc: 0.997 - ETA: 8s - loss: 0.0105 - acc: 0.997 - ETA: 7s - loss: 0.0107 - acc: 0.997 - ETA: 7s - loss: 0.0107 - acc: 0.997 - ETA: 6s - loss: 0.0106 - acc: 0.997 - ETA: 5s - loss: 0.0105 - acc: 0.997 - ETA: 4s - loss: 0.0105 - acc: 0.998 - ETA: 4s - loss: 0.0102 - acc: 0.998 - ETA: 3s - loss: 0.0099 - acc: 0.998 - ETA: 2s - loss: 0.0098 - acc: 0.998 - ETA: 2s - loss: 0.0098 - acc: 0.998 - ETA: 1s - loss: 0.0097 - acc: 0.998 - ETA: 0s - loss: 0.0096 - acc: 0.998 - 25s 6ms/step - loss: 0.0093 - acc: 0.9982 - val_loss: 0.0100 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00764\n",
      "Epoch 73/100\n",
      "4340/4340 [==============================] - ETA: 16s - loss: 0.0040 - acc: 1.00 - ETA: 18s - loss: 0.0040 - acc: 1.00 - ETA: 17s - loss: 0.0050 - acc: 1.00 - ETA: 18s - loss: 0.0054 - acc: 1.00 - ETA: 17s - loss: 0.0054 - acc: 1.00 - ETA: 16s - loss: 0.0055 - acc: 1.00 - ETA: 16s - loss: 0.0052 - acc: 1.00 - ETA: 15s - loss: 0.0055 - acc: 1.00 - ETA: 15s - loss: 0.0056 - acc: 1.00 - ETA: 14s - loss: 0.0058 - acc: 1.00 - ETA: 13s - loss: 0.0055 - acc: 1.00 - ETA: 13s - loss: 0.0054 - acc: 1.00 - ETA: 13s - loss: 0.0055 - acc: 1.00 - ETA: 12s - loss: 0.0057 - acc: 1.00 - ETA: 12s - loss: 0.0058 - acc: 1.00 - ETA: 12s - loss: 0.0056 - acc: 1.00 - ETA: 11s - loss: 0.0055 - acc: 1.00 - ETA: 10s - loss: 0.0054 - acc: 1.00 - ETA: 10s - loss: 0.0052 - acc: 1.00 - ETA: 9s - loss: 0.0052 - acc: 1.0000 - ETA: 8s - loss: 0.0054 - acc: 1.000 - ETA: 8s - loss: 0.0053 - acc: 1.000 - ETA: 7s - loss: 0.0052 - acc: 1.000 - ETA: 6s - loss: 0.0052 - acc: 1.000 - ETA: 5s - loss: 0.0051 - acc: 1.000 - ETA: 5s - loss: 0.0055 - acc: 0.999 - ETA: 4s - loss: 0.0057 - acc: 0.999 - ETA: 3s - loss: 0.0056 - acc: 0.999 - ETA: 3s - loss: 0.0057 - acc: 0.999 - ETA: 2s - loss: 0.0056 - acc: 0.999 - ETA: 1s - loss: 0.0056 - acc: 0.999 - ETA: 1s - loss: 0.0055 - acc: 0.999 - ETA: 0s - loss: 0.0054 - acc: 0.999 - 24s 6ms/step - loss: 0.0053 - acc: 0.9998 - val_loss: 0.0110 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00764\n",
      "Epoch 74/100\n",
      "4340/4340 [==============================] - ETA: 18s - loss: 0.0023 - acc: 1.00 - ETA: 17s - loss: 0.0054 - acc: 1.00 - ETA: 18s - loss: 0.0046 - acc: 1.00 - ETA: 17s - loss: 0.0061 - acc: 1.00 - ETA: 16s - loss: 0.0056 - acc: 1.00 - ETA: 16s - loss: 0.0083 - acc: 0.99 - ETA: 15s - loss: 0.0087 - acc: 0.99 - ETA: 15s - loss: 0.0097 - acc: 0.99 - ETA: 15s - loss: 0.0101 - acc: 0.99 - ETA: 14s - loss: 0.0097 - acc: 0.99 - ETA: 14s - loss: 0.0100 - acc: 0.99 - ETA: 13s - loss: 0.0095 - acc: 0.99 - ETA: 12s - loss: 0.0092 - acc: 0.99 - ETA: 12s - loss: 0.0090 - acc: 0.99 - ETA: 11s - loss: 0.0088 - acc: 0.99 - ETA: 11s - loss: 0.0096 - acc: 0.99 - ETA: 10s - loss: 0.0092 - acc: 0.99 - ETA: 9s - loss: 0.0090 - acc: 0.9974 - ETA: 9s - loss: 0.0086 - acc: 0.997 - ETA: 8s - loss: 0.0087 - acc: 0.997 - ETA: 8s - loss: 0.0084 - acc: 0.997 - ETA: 7s - loss: 0.0084 - acc: 0.997 - ETA: 6s - loss: 0.0084 - acc: 0.997 - ETA: 6s - loss: 0.0091 - acc: 0.997 - ETA: 5s - loss: 0.0093 - acc: 0.996 - ETA: 4s - loss: 0.0092 - acc: 0.997 - ETA: 4s - loss: 0.0090 - acc: 0.997 - ETA: 3s - loss: 0.0090 - acc: 0.997 - ETA: 3s - loss: 0.0090 - acc: 0.997 - ETA: 2s - loss: 0.0087 - acc: 0.997 - ETA: 1s - loss: 0.0086 - acc: 0.997 - ETA: 1s - loss: 0.0086 - acc: 0.997 - ETA: 0s - loss: 0.0084 - acc: 0.997 - 23s 5ms/step - loss: 0.0083 - acc: 0.9977 - val_loss: 0.0098 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00764\n",
      "Epoch 75/100\n",
      "4340/4340 [==============================] - ETA: 23s - loss: 0.0047 - acc: 1.00 - ETA: 19s - loss: 0.0044 - acc: 1.00 - ETA: 19s - loss: 0.0081 - acc: 0.99 - ETA: 19s - loss: 0.0071 - acc: 0.99 - ETA: 22s - loss: 0.0075 - acc: 0.99 - ETA: 22s - loss: 0.0066 - acc: 0.99 - ETA: 20s - loss: 0.0074 - acc: 0.99 - ETA: 19s - loss: 0.0067 - acc: 0.99 - ETA: 19s - loss: 0.0066 - acc: 0.99 - ETA: 18s - loss: 0.0071 - acc: 0.99 - ETA: 16s - loss: 0.0075 - acc: 0.99 - ETA: 15s - loss: 0.0075 - acc: 0.99 - ETA: 14s - loss: 0.0075 - acc: 0.99 - ETA: 14s - loss: 0.0074 - acc: 0.99 - ETA: 13s - loss: 0.0075 - acc: 0.99 - ETA: 12s - loss: 0.0073 - acc: 0.99 - ETA: 11s - loss: 0.0070 - acc: 0.99 - ETA: 11s - loss: 0.0067 - acc: 0.99 - ETA: 10s - loss: 0.0065 - acc: 0.99 - ETA: 10s - loss: 0.0064 - acc: 0.99 - ETA: 9s - loss: 0.0062 - acc: 0.9993 - ETA: 8s - loss: 0.0064 - acc: 0.999 - ETA: 7s - loss: 0.0065 - acc: 0.999 - ETA: 7s - loss: 0.0067 - acc: 0.999 - ETA: 6s - loss: 0.0066 - acc: 0.999 - ETA: 5s - loss: 0.0066 - acc: 0.999 - ETA: 4s - loss: 0.0066 - acc: 0.999 - ETA: 4s - loss: 0.0064 - acc: 0.999 - ETA: 3s - loss: 0.0063 - acc: 0.999 - ETA: 2s - loss: 0.0063 - acc: 0.999 - ETA: 2s - loss: 0.0063 - acc: 0.999 - ETA: 1s - loss: 0.0062 - acc: 0.999 - ETA: 0s - loss: 0.0072 - acc: 0.999 - 25s 6ms/step - loss: 0.0071 - acc: 0.9991 - val_loss: 0.0126 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00764\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4340/4340 [==============================] - ETA: 21s - loss: 0.0015 - acc: 1.00 - ETA: 23s - loss: 0.0035 - acc: 1.00 - ETA: 22s - loss: 0.0030 - acc: 1.00 - ETA: 22s - loss: 0.0048 - acc: 0.99 - ETA: 21s - loss: 0.0045 - acc: 0.99 - ETA: 20s - loss: 0.0049 - acc: 0.99 - ETA: 19s - loss: 0.0049 - acc: 0.99 - ETA: 18s - loss: 0.0055 - acc: 0.99 - ETA: 17s - loss: 0.0054 - acc: 0.99 - ETA: 16s - loss: 0.0050 - acc: 0.99 - ETA: 15s - loss: 0.0049 - acc: 0.99 - ETA: 16s - loss: 0.0048 - acc: 0.99 - ETA: 15s - loss: 0.0049 - acc: 0.99 - ETA: 14s - loss: 0.0053 - acc: 0.99 - ETA: 13s - loss: 0.0054 - acc: 0.99 - ETA: 12s - loss: 0.0061 - acc: 0.99 - ETA: 11s - loss: 0.0060 - acc: 0.99 - ETA: 10s - loss: 0.0071 - acc: 0.99 - ETA: 10s - loss: 0.0071 - acc: 0.99 - ETA: 9s - loss: 0.0069 - acc: 0.9980 - ETA: 8s - loss: 0.0068 - acc: 0.998 - ETA: 7s - loss: 0.0066 - acc: 0.998 - ETA: 7s - loss: 0.0067 - acc: 0.998 - ETA: 6s - loss: 0.0066 - acc: 0.998 - ETA: 5s - loss: 0.0065 - acc: 0.998 - ETA: 5s - loss: 0.0064 - acc: 0.998 - ETA: 4s - loss: 0.0063 - acc: 0.998 - ETA: 3s - loss: 0.0062 - acc: 0.998 - ETA: 3s - loss: 0.0061 - acc: 0.998 - ETA: 2s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - 25s 6ms/step - loss: 0.0062 - acc: 0.9984 - val_loss: 0.0094 - val_acc: 0.9945\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00764\n",
      "Epoch 77/100\n",
      "4340/4340 [==============================] - ETA: 20s - loss: 0.0110 - acc: 1.00 - ETA: 18s - loss: 0.0079 - acc: 1.00 - ETA: 19s - loss: 0.0103 - acc: 0.99 - ETA: 19s - loss: 0.0092 - acc: 0.99 - ETA: 19s - loss: 0.0102 - acc: 0.99 - ETA: 17s - loss: 0.0090 - acc: 0.99 - ETA: 17s - loss: 0.0080 - acc: 0.99 - ETA: 16s - loss: 0.0073 - acc: 0.99 - ETA: 16s - loss: 0.0071 - acc: 0.99 - ETA: 15s - loss: 0.0069 - acc: 0.99 - ETA: 14s - loss: 0.0065 - acc: 0.99 - ETA: 14s - loss: 0.0068 - acc: 0.99 - ETA: 13s - loss: 0.0068 - acc: 0.99 - ETA: 13s - loss: 0.0065 - acc: 0.99 - ETA: 12s - loss: 0.0066 - acc: 0.99 - ETA: 11s - loss: 0.0063 - acc: 0.99 - ETA: 11s - loss: 0.0062 - acc: 0.99 - ETA: 10s - loss: 0.0060 - acc: 0.99 - ETA: 10s - loss: 0.0058 - acc: 0.99 - ETA: 10s - loss: 0.0057 - acc: 0.99 - ETA: 10s - loss: 0.0055 - acc: 0.99 - ETA: 9s - loss: 0.0066 - acc: 0.9989 - ETA: 8s - loss: 0.0065 - acc: 0.999 - ETA: 7s - loss: 0.0064 - acc: 0.999 - ETA: 7s - loss: 0.0063 - acc: 0.999 - ETA: 6s - loss: 0.0063 - acc: 0.999 - ETA: 5s - loss: 0.0064 - acc: 0.999 - ETA: 4s - loss: 0.0070 - acc: 0.998 - ETA: 3s - loss: 0.0073 - acc: 0.998 - ETA: 3s - loss: 0.0073 - acc: 0.998 - ETA: 2s - loss: 0.0073 - acc: 0.998 - ETA: 1s - loss: 0.0072 - acc: 0.998 - ETA: 0s - loss: 0.0072 - acc: 0.998 - 30s 7ms/step - loss: 0.0072 - acc: 0.9988 - val_loss: 0.0077 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00764\n",
      "Epoch 78/100\n",
      "4340/4340 [==============================] - ETA: 16s - loss: 0.0057 - acc: 1.00 - ETA: 17s - loss: 0.0042 - acc: 1.00 - ETA: 18s - loss: 0.0063 - acc: 1.00 - ETA: 17s - loss: 0.0054 - acc: 1.00 - ETA: 18s - loss: 0.0063 - acc: 0.99 - ETA: 17s - loss: 0.0057 - acc: 0.99 - ETA: 16s - loss: 0.0054 - acc: 0.99 - ETA: 15s - loss: 0.0052 - acc: 0.99 - ETA: 15s - loss: 0.0052 - acc: 0.99 - ETA: 14s - loss: 0.0062 - acc: 0.99 - ETA: 14s - loss: 0.0062 - acc: 0.99 - ETA: 14s - loss: 0.0058 - acc: 0.99 - ETA: 14s - loss: 0.0057 - acc: 0.99 - ETA: 13s - loss: 0.0056 - acc: 0.99 - ETA: 13s - loss: 0.0053 - acc: 0.99 - ETA: 12s - loss: 0.0056 - acc: 0.99 - ETA: 12s - loss: 0.0060 - acc: 0.99 - ETA: 11s - loss: 0.0060 - acc: 0.99 - ETA: 10s - loss: 0.0063 - acc: 0.99 - ETA: 10s - loss: 0.0062 - acc: 0.99 - ETA: 9s - loss: 0.0063 - acc: 0.9989 - ETA: 8s - loss: 0.0061 - acc: 0.998 - ETA: 7s - loss: 0.0059 - acc: 0.999 - ETA: 7s - loss: 0.0058 - acc: 0.999 - ETA: 6s - loss: 0.0058 - acc: 0.999 - ETA: 5s - loss: 0.0061 - acc: 0.998 - ETA: 4s - loss: 0.0060 - acc: 0.998 - ETA: 4s - loss: 0.0060 - acc: 0.998 - ETA: 3s - loss: 0.0059 - acc: 0.998 - ETA: 2s - loss: 0.0059 - acc: 0.999 - ETA: 2s - loss: 0.0058 - acc: 0.999 - ETA: 1s - loss: 0.0059 - acc: 0.999 - ETA: 0s - loss: 0.0060 - acc: 0.999 - 26s 6ms/step - loss: 0.0059 - acc: 0.9991 - val_loss: 0.0094 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00764\n",
      "Epoch 79/100\n",
      "4340/4340 [==============================] - ETA: 20s - loss: 0.0074 - acc: 0.99 - ETA: 19s - loss: 0.0073 - acc: 0.99 - ETA: 21s - loss: 0.0077 - acc: 0.99 - ETA: 19s - loss: 0.0077 - acc: 0.99 - ETA: 19s - loss: 0.0063 - acc: 0.99 - ETA: 18s - loss: 0.0056 - acc: 0.99 - ETA: 18s - loss: 0.0059 - acc: 0.99 - ETA: 17s - loss: 0.0057 - acc: 0.99 - ETA: 16s - loss: 0.0058 - acc: 0.99 - ETA: 16s - loss: 0.0055 - acc: 0.99 - ETA: 15s - loss: 0.0054 - acc: 0.99 - ETA: 15s - loss: 0.0072 - acc: 0.99 - ETA: 14s - loss: 0.0070 - acc: 0.99 - ETA: 13s - loss: 0.0067 - acc: 0.99 - ETA: 12s - loss: 0.0068 - acc: 0.99 - ETA: 11s - loss: 0.0070 - acc: 0.99 - ETA: 11s - loss: 0.0068 - acc: 0.99 - ETA: 10s - loss: 0.0067 - acc: 0.99 - ETA: 9s - loss: 0.0065 - acc: 0.9992 - ETA: 9s - loss: 0.0064 - acc: 0.999 - ETA: 8s - loss: 0.0063 - acc: 0.999 - ETA: 7s - loss: 0.0064 - acc: 0.999 - ETA: 7s - loss: 0.0064 - acc: 0.999 - ETA: 6s - loss: 0.0063 - acc: 0.999 - ETA: 5s - loss: 0.0063 - acc: 0.999 - ETA: 5s - loss: 0.0064 - acc: 0.999 - ETA: 4s - loss: 0.0064 - acc: 0.999 - ETA: 3s - loss: 0.0063 - acc: 0.999 - ETA: 3s - loss: 0.0062 - acc: 0.999 - ETA: 2s - loss: 0.0061 - acc: 0.999 - ETA: 1s - loss: 0.0064 - acc: 0.999 - ETA: 1s - loss: 0.0064 - acc: 0.999 - ETA: 0s - loss: 0.0063 - acc: 0.999 - 25s 6ms/step - loss: 0.0063 - acc: 0.9993 - val_loss: 0.0105 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00764\n",
      "Epoch 80/100\n",
      "4340/4340 [==============================] - ETA: 24s - loss: 0.0074 - acc: 1.00 - ETA: 19s - loss: 0.0071 - acc: 1.00 - ETA: 18s - loss: 0.0060 - acc: 1.00 - ETA: 17s - loss: 0.0051 - acc: 1.00 - ETA: 17s - loss: 0.0063 - acc: 1.00 - ETA: 17s - loss: 0.0069 - acc: 1.00 - ETA: 17s - loss: 0.0065 - acc: 1.00 - ETA: 16s - loss: 0.0063 - acc: 1.00 - ETA: 16s - loss: 0.0063 - acc: 1.00 - ETA: 15s - loss: 0.0059 - acc: 1.00 - ETA: 14s - loss: 0.0056 - acc: 1.00 - ETA: 14s - loss: 0.0055 - acc: 1.00 - ETA: 13s - loss: 0.0052 - acc: 1.00 - ETA: 12s - loss: 0.0051 - acc: 1.00 - ETA: 12s - loss: 0.0051 - acc: 1.00 - ETA: 11s - loss: 0.0050 - acc: 1.00 - ETA: 10s - loss: 0.0055 - acc: 0.99 - ETA: 10s - loss: 0.0056 - acc: 0.99 - ETA: 9s - loss: 0.0059 - acc: 0.9992 - ETA: 8s - loss: 0.0059 - acc: 0.999 - ETA: 8s - loss: 0.0057 - acc: 0.999 - ETA: 7s - loss: 0.0057 - acc: 0.999 - ETA: 7s - loss: 0.0056 - acc: 0.999 - ETA: 6s - loss: 0.0056 - acc: 0.999 - ETA: 6s - loss: 0.0055 - acc: 0.999 - ETA: 5s - loss: 0.0053 - acc: 0.999 - ETA: 4s - loss: 0.0054 - acc: 0.999 - ETA: 4s - loss: 0.0054 - acc: 0.999 - ETA: 3s - loss: 0.0055 - acc: 0.999 - ETA: 2s - loss: 0.0056 - acc: 0.999 - ETA: 1s - loss: 0.0055 - acc: 0.999 - ETA: 1s - loss: 0.0054 - acc: 0.999 - ETA: 0s - loss: 0.0055 - acc: 0.999 - 27s 6ms/step - loss: 0.0056 - acc: 0.9995 - val_loss: 0.0082 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00764\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4340/4340 [==============================] - ETA: 16s - loss: 0.0038 - acc: 1.00 - ETA: 16s - loss: 0.0026 - acc: 1.00 - ETA: 15s - loss: 0.0026 - acc: 1.00 - ETA: 17s - loss: 0.0043 - acc: 1.00 - ETA: 17s - loss: 0.0044 - acc: 1.00 - ETA: 16s - loss: 0.0042 - acc: 1.00 - ETA: 16s - loss: 0.0038 - acc: 1.00 - ETA: 16s - loss: 0.0037 - acc: 1.00 - ETA: 15s - loss: 0.0052 - acc: 0.99 - ETA: 14s - loss: 0.0052 - acc: 0.99 - ETA: 14s - loss: 0.0049 - acc: 0.99 - ETA: 13s - loss: 0.0046 - acc: 0.99 - ETA: 13s - loss: 0.0046 - acc: 0.99 - ETA: 12s - loss: 0.0044 - acc: 0.99 - ETA: 12s - loss: 0.0043 - acc: 0.99 - ETA: 11s - loss: 0.0043 - acc: 0.99 - ETA: 10s - loss: 0.0042 - acc: 0.99 - ETA: 10s - loss: 0.0041 - acc: 0.99 - ETA: 9s - loss: 0.0041 - acc: 0.9996 - ETA: 8s - loss: 0.0040 - acc: 0.999 - ETA: 8s - loss: 0.0041 - acc: 0.999 - ETA: 7s - loss: 0.0040 - acc: 0.999 - ETA: 6s - loss: 0.0046 - acc: 0.999 - ETA: 6s - loss: 0.0047 - acc: 0.999 - ETA: 5s - loss: 0.0046 - acc: 0.999 - ETA: 5s - loss: 0.0046 - acc: 0.999 - ETA: 4s - loss: 0.0045 - acc: 0.999 - ETA: 3s - loss: 0.0046 - acc: 0.999 - ETA: 3s - loss: 0.0046 - acc: 0.999 - ETA: 2s - loss: 0.0045 - acc: 0.999 - ETA: 1s - loss: 0.0048 - acc: 0.999 - ETA: 1s - loss: 0.0048 - acc: 0.999 - ETA: 0s - loss: 0.0047 - acc: 0.999 - 23s 5ms/step - loss: 0.0047 - acc: 0.9993 - val_loss: 0.0111 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00764\n",
      "Epoch 82/100\n",
      "4340/4340 [==============================] - ETA: 17s - loss: 0.0045 - acc: 1.00 - ETA: 16s - loss: 0.0071 - acc: 1.00 - ETA: 15s - loss: 0.0052 - acc: 1.00 - ETA: 16s - loss: 0.0058 - acc: 1.00 - ETA: 15s - loss: 0.0127 - acc: 0.99 - ETA: 15s - loss: 0.0111 - acc: 0.99 - ETA: 14s - loss: 0.0099 - acc: 0.99 - ETA: 15s - loss: 0.0093 - acc: 0.99 - ETA: 14s - loss: 0.0101 - acc: 0.99 - ETA: 14s - loss: 0.0095 - acc: 0.99 - ETA: 13s - loss: 0.0094 - acc: 0.99 - ETA: 12s - loss: 0.0092 - acc: 0.99 - ETA: 14s - loss: 0.0086 - acc: 0.99 - ETA: 13s - loss: 0.0086 - acc: 0.99 - ETA: 12s - loss: 0.0084 - acc: 0.99 - ETA: 11s - loss: 0.0082 - acc: 0.99 - ETA: 11s - loss: 0.0079 - acc: 0.99 - ETA: 10s - loss: 0.0076 - acc: 0.99 - ETA: 9s - loss: 0.0073 - acc: 0.9992 - ETA: 9s - loss: 0.0071 - acc: 0.999 - ETA: 8s - loss: 0.0069 - acc: 0.999 - ETA: 7s - loss: 0.0071 - acc: 0.998 - ETA: 7s - loss: 0.0069 - acc: 0.999 - ETA: 6s - loss: 0.0069 - acc: 0.999 - ETA: 5s - loss: 0.0068 - acc: 0.999 - ETA: 5s - loss: 0.0066 - acc: 0.999 - ETA: 4s - loss: 0.0065 - acc: 0.999 - ETA: 3s - loss: 0.0069 - acc: 0.998 - ETA: 3s - loss: 0.0068 - acc: 0.998 - ETA: 2s - loss: 0.0067 - acc: 0.999 - ETA: 1s - loss: 0.0066 - acc: 0.999 - ETA: 1s - loss: 0.0066 - acc: 0.999 - ETA: 0s - loss: 0.0065 - acc: 0.999 - 23s 5ms/step - loss: 0.0065 - acc: 0.9991 - val_loss: 0.0093 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00764\n",
      "Epoch 83/100\n",
      "4340/4340 [==============================] - ETA: 16s - loss: 0.0060 - acc: 1.00 - ETA: 16s - loss: 0.0042 - acc: 1.00 - ETA: 15s - loss: 0.0037 - acc: 1.00 - ETA: 15s - loss: 0.0033 - acc: 1.00 - ETA: 16s - loss: 0.0048 - acc: 1.00 - ETA: 16s - loss: 0.0050 - acc: 1.00 - ETA: 15s - loss: 0.0051 - acc: 1.00 - ETA: 15s - loss: 0.0049 - acc: 1.00 - ETA: 14s - loss: 0.0047 - acc: 1.00 - ETA: 14s - loss: 0.0046 - acc: 1.00 - ETA: 14s - loss: 0.0044 - acc: 1.00 - ETA: 13s - loss: 0.0043 - acc: 1.00 - ETA: 12s - loss: 0.0052 - acc: 0.99 - ETA: 12s - loss: 0.0057 - acc: 0.99 - ETA: 11s - loss: 0.0058 - acc: 0.99 - ETA: 11s - loss: 0.0057 - acc: 0.99 - ETA: 10s - loss: 0.0054 - acc: 0.99 - ETA: 9s - loss: 0.0054 - acc: 0.9996 - ETA: 9s - loss: 0.0058 - acc: 0.999 - ETA: 8s - loss: 0.0058 - acc: 0.999 - ETA: 8s - loss: 0.0056 - acc: 0.999 - ETA: 7s - loss: 0.0056 - acc: 0.999 - ETA: 6s - loss: 0.0056 - acc: 0.999 - ETA: 6s - loss: 0.0058 - acc: 0.999 - ETA: 5s - loss: 0.0060 - acc: 0.999 - ETA: 4s - loss: 0.0060 - acc: 0.999 - ETA: 4s - loss: 0.0064 - acc: 0.998 - ETA: 3s - loss: 0.0063 - acc: 0.998 - ETA: 3s - loss: 0.0062 - acc: 0.998 - ETA: 2s - loss: 0.0065 - acc: 0.998 - ETA: 1s - loss: 0.0065 - acc: 0.998 - ETA: 1s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - 23s 5ms/step - loss: 0.0065 - acc: 0.9988 - val_loss: 0.0106 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00764\n",
      "Epoch 84/100\n",
      "4340/4340 [==============================] - ETA: 24s - loss: 0.0142 - acc: 1.00 - ETA: 23s - loss: 0.0089 - acc: 1.00 - ETA: 19s - loss: 0.0099 - acc: 0.99 - ETA: 19s - loss: 0.0085 - acc: 0.99 - ETA: 18s - loss: 0.0073 - acc: 0.99 - ETA: 17s - loss: 0.0064 - acc: 0.99 - ETA: 17s - loss: 0.0064 - acc: 0.99 - ETA: 15s - loss: 0.0059 - acc: 0.99 - ETA: 15s - loss: 0.0064 - acc: 0.99 - ETA: 14s - loss: 0.0063 - acc: 0.99 - ETA: 13s - loss: 0.0059 - acc: 0.99 - ETA: 13s - loss: 0.0059 - acc: 0.99 - ETA: 12s - loss: 0.0056 - acc: 0.99 - ETA: 12s - loss: 0.0059 - acc: 0.99 - ETA: 11s - loss: 0.0057 - acc: 0.99 - ETA: 10s - loss: 0.0054 - acc: 0.99 - ETA: 10s - loss: 0.0055 - acc: 0.99 - ETA: 9s - loss: 0.0053 - acc: 0.9996 - ETA: 9s - loss: 0.0051 - acc: 0.999 - ETA: 8s - loss: 0.0050 - acc: 0.999 - ETA: 7s - loss: 0.0050 - acc: 0.999 - ETA: 7s - loss: 0.0050 - acc: 0.999 - ETA: 6s - loss: 0.0050 - acc: 0.999 - ETA: 6s - loss: 0.0050 - acc: 0.999 - ETA: 5s - loss: 0.0049 - acc: 0.999 - ETA: 4s - loss: 0.0048 - acc: 0.999 - ETA: 4s - loss: 0.0048 - acc: 0.999 - ETA: 3s - loss: 0.0047 - acc: 0.999 - ETA: 3s - loss: 0.0046 - acc: 0.999 - ETA: 2s - loss: 0.0046 - acc: 0.999 - ETA: 1s - loss: 0.0046 - acc: 0.999 - ETA: 1s - loss: 0.0049 - acc: 0.999 - ETA: 0s - loss: 0.0048 - acc: 0.999 - 24s 5ms/step - loss: 0.0047 - acc: 0.9995 - val_loss: 0.0091 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00764\n",
      "Epoch 85/100\n",
      "4340/4340 [==============================] - ETA: 24s - loss: 0.0158 - acc: 0.99 - ETA: 19s - loss: 0.0098 - acc: 0.99 - ETA: 19s - loss: 0.0069 - acc: 0.99 - ETA: 20s - loss: 0.0095 - acc: 0.99 - ETA: 18s - loss: 0.0081 - acc: 0.99 - ETA: 18s - loss: 0.0079 - acc: 0.99 - ETA: 18s - loss: 0.0080 - acc: 0.99 - ETA: 16s - loss: 0.0080 - acc: 0.99 - ETA: 16s - loss: 0.0075 - acc: 0.99 - ETA: 15s - loss: 0.0069 - acc: 0.99 - ETA: 15s - loss: 0.0065 - acc: 0.99 - ETA: 14s - loss: 0.0068 - acc: 0.99 - ETA: 13s - loss: 0.0065 - acc: 0.99 - ETA: 12s - loss: 0.0063 - acc: 0.99 - ETA: 12s - loss: 0.0060 - acc: 0.99 - ETA: 11s - loss: 0.0059 - acc: 0.99 - ETA: 10s - loss: 0.0068 - acc: 0.99 - ETA: 10s - loss: 0.0065 - acc: 0.99 - ETA: 9s - loss: 0.0063 - acc: 0.9988 - ETA: 8s - loss: 0.0062 - acc: 0.998 - ETA: 8s - loss: 0.0060 - acc: 0.998 - ETA: 7s - loss: 0.0060 - acc: 0.998 - ETA: 6s - loss: 0.0060 - acc: 0.999 - ETA: 6s - loss: 0.0059 - acc: 0.999 - ETA: 5s - loss: 0.0056 - acc: 0.999 - ETA: 5s - loss: 0.0056 - acc: 0.999 - ETA: 4s - loss: 0.0056 - acc: 0.999 - ETA: 3s - loss: 0.0057 - acc: 0.999 - ETA: 3s - loss: 0.0057 - acc: 0.999 - ETA: 2s - loss: 0.0057 - acc: 0.999 - ETA: 1s - loss: 0.0056 - acc: 0.999 - ETA: 1s - loss: 0.0058 - acc: 0.999 - ETA: 0s - loss: 0.0058 - acc: 0.999 - 24s 6ms/step - loss: 0.0058 - acc: 0.9993 - val_loss: 0.0085 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00764\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4340/4340 [==============================] - ETA: 21s - loss: 0.0028 - acc: 1.00 - ETA: 23s - loss: 0.0056 - acc: 0.99 - ETA: 23s - loss: 0.0041 - acc: 0.99 - ETA: 20s - loss: 0.0038 - acc: 0.99 - ETA: 19s - loss: 0.0037 - acc: 0.99 - ETA: 18s - loss: 0.0035 - acc: 0.99 - ETA: 18s - loss: 0.0047 - acc: 0.99 - ETA: 18s - loss: 0.0046 - acc: 0.99 - ETA: 18s - loss: 0.0067 - acc: 0.99 - ETA: 17s - loss: 0.0067 - acc: 0.99 - ETA: 17s - loss: 0.0064 - acc: 0.99 - ETA: 17s - loss: 0.0062 - acc: 0.99 - ETA: 16s - loss: 0.0060 - acc: 0.99 - ETA: 15s - loss: 0.0074 - acc: 0.99 - ETA: 15s - loss: 0.0085 - acc: 0.99 - ETA: 14s - loss: 0.0083 - acc: 0.99 - ETA: 13s - loss: 0.0081 - acc: 0.99 - ETA: 13s - loss: 0.0083 - acc: 0.99 - ETA: 12s - loss: 0.0081 - acc: 0.99 - ETA: 11s - loss: 0.0079 - acc: 0.99 - ETA: 10s - loss: 0.0081 - acc: 0.99 - ETA: 9s - loss: 0.0079 - acc: 0.9975 - ETA: 8s - loss: 0.0078 - acc: 0.997 - ETA: 7s - loss: 0.0075 - acc: 0.997 - ETA: 6s - loss: 0.0073 - acc: 0.997 - ETA: 6s - loss: 0.0072 - acc: 0.997 - ETA: 5s - loss: 0.0072 - acc: 0.998 - ETA: 4s - loss: 0.0071 - acc: 0.998 - ETA: 3s - loss: 0.0069 - acc: 0.998 - ETA: 2s - loss: 0.0069 - acc: 0.998 - ETA: 2s - loss: 0.0068 - acc: 0.998 - ETA: 1s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - 27s 6ms/step - loss: 0.0064 - acc: 0.9984 - val_loss: 0.0118 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00764\n",
      "Epoch 87/100\n",
      "4340/4340 [==============================] - ETA: 16s - loss: 0.0053 - acc: 1.00 - ETA: 16s - loss: 0.0058 - acc: 1.00 - ETA: 18s - loss: 0.0043 - acc: 1.00 - ETA: 16s - loss: 0.0040 - acc: 1.00 - ETA: 16s - loss: 0.0036 - acc: 1.00 - ETA: 15s - loss: 0.0033 - acc: 1.00 - ETA: 15s - loss: 0.0034 - acc: 1.00 - ETA: 14s - loss: 0.0031 - acc: 1.00 - ETA: 14s - loss: 0.0031 - acc: 1.00 - ETA: 14s - loss: 0.0031 - acc: 1.00 - ETA: 13s - loss: 0.0033 - acc: 1.00 - ETA: 12s - loss: 0.0033 - acc: 1.00 - ETA: 12s - loss: 0.0033 - acc: 1.00 - ETA: 11s - loss: 0.0033 - acc: 1.00 - ETA: 11s - loss: 0.0032 - acc: 1.00 - ETA: 11s - loss: 0.0031 - acc: 1.00 - ETA: 10s - loss: 0.0032 - acc: 1.00 - ETA: 10s - loss: 0.0031 - acc: 1.00 - ETA: 9s - loss: 0.0031 - acc: 1.0000 - ETA: 8s - loss: 0.0032 - acc: 1.000 - ETA: 8s - loss: 0.0037 - acc: 0.999 - ETA: 7s - loss: 0.0039 - acc: 0.999 - ETA: 6s - loss: 0.0044 - acc: 0.999 - ETA: 6s - loss: 0.0044 - acc: 0.999 - ETA: 5s - loss: 0.0045 - acc: 0.999 - ETA: 4s - loss: 0.0044 - acc: 0.999 - ETA: 4s - loss: 0.0044 - acc: 0.999 - ETA: 3s - loss: 0.0044 - acc: 0.999 - ETA: 3s - loss: 0.0043 - acc: 0.999 - ETA: 2s - loss: 0.0043 - acc: 0.999 - ETA: 1s - loss: 0.0042 - acc: 0.999 - ETA: 1s - loss: 0.0041 - acc: 0.999 - ETA: 0s - loss: 0.0042 - acc: 0.999 - 23s 5ms/step - loss: 0.0042 - acc: 0.9995 - val_loss: 0.0130 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00764\n",
      "Epoch 88/100\n",
      "4340/4340 [==============================] - ETA: 16s - loss: 0.0066 - acc: 1.00 - ETA: 22s - loss: 0.0063 - acc: 1.00 - ETA: 22s - loss: 0.0055 - acc: 1.00 - ETA: 20s - loss: 0.0051 - acc: 1.00 - ETA: 18s - loss: 0.0057 - acc: 1.00 - ETA: 19s - loss: 0.0053 - acc: 1.00 - ETA: 19s - loss: 0.0053 - acc: 1.00 - ETA: 19s - loss: 0.0051 - acc: 1.00 - ETA: 17s - loss: 0.0047 - acc: 1.00 - ETA: 17s - loss: 0.0045 - acc: 1.00 - ETA: 16s - loss: 0.0045 - acc: 1.00 - ETA: 15s - loss: 0.0049 - acc: 1.00 - ETA: 14s - loss: 0.0051 - acc: 1.00 - ETA: 14s - loss: 0.0052 - acc: 1.00 - ETA: 13s - loss: 0.0052 - acc: 1.00 - ETA: 12s - loss: 0.0057 - acc: 0.99 - ETA: 11s - loss: 0.0057 - acc: 0.99 - ETA: 10s - loss: 0.0056 - acc: 0.99 - ETA: 10s - loss: 0.0059 - acc: 0.99 - ETA: 9s - loss: 0.0057 - acc: 0.9992 - ETA: 8s - loss: 0.0059 - acc: 0.999 - ETA: 8s - loss: 0.0059 - acc: 0.999 - ETA: 7s - loss: 0.0059 - acc: 0.999 - ETA: 6s - loss: 0.0063 - acc: 0.999 - ETA: 5s - loss: 0.0072 - acc: 0.998 - ETA: 5s - loss: 0.0070 - acc: 0.998 - ETA: 4s - loss: 0.0068 - acc: 0.998 - ETA: 4s - loss: 0.0067 - acc: 0.998 - ETA: 3s - loss: 0.0066 - acc: 0.998 - ETA: 2s - loss: 0.0065 - acc: 0.999 - ETA: 1s - loss: 0.0064 - acc: 0.999 - ETA: 1s - loss: 0.0063 - acc: 0.999 - ETA: 0s - loss: 0.0062 - acc: 0.999 - 25s 6ms/step - loss: 0.0062 - acc: 0.9991 - val_loss: 0.0097 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00764\n",
      "Epoch 89/100\n",
      "4340/4340 [==============================] - ETA: 16s - loss: 0.0066 - acc: 1.00 - ETA: 18s - loss: 0.0067 - acc: 1.00 - ETA: 19s - loss: 0.0060 - acc: 1.00 - ETA: 17s - loss: 0.0051 - acc: 1.00 - ETA: 18s - loss: 0.0049 - acc: 1.00 - ETA: 18s - loss: 0.0063 - acc: 0.99 - ETA: 17s - loss: 0.0059 - acc: 0.99 - ETA: 16s - loss: 0.0058 - acc: 0.99 - ETA: 15s - loss: 0.0055 - acc: 0.99 - ETA: 15s - loss: 0.0055 - acc: 0.99 - ETA: 15s - loss: 0.0054 - acc: 0.99 - ETA: 14s - loss: 0.0052 - acc: 0.99 - ETA: 14s - loss: 0.0050 - acc: 0.99 - ETA: 13s - loss: 0.0048 - acc: 0.99 - ETA: 12s - loss: 0.0050 - acc: 0.99 - ETA: 12s - loss: 0.0048 - acc: 0.99 - ETA: 11s - loss: 0.0049 - acc: 0.99 - ETA: 10s - loss: 0.0049 - acc: 0.99 - ETA: 10s - loss: 0.0047 - acc: 0.99 - ETA: 9s - loss: 0.0046 - acc: 0.9996 - ETA: 8s - loss: 0.0045 - acc: 0.999 - ETA: 7s - loss: 0.0044 - acc: 0.999 - ETA: 7s - loss: 0.0045 - acc: 0.999 - ETA: 6s - loss: 0.0046 - acc: 0.999 - ETA: 5s - loss: 0.0045 - acc: 0.999 - ETA: 5s - loss: 0.0045 - acc: 0.999 - ETA: 4s - loss: 0.0045 - acc: 0.999 - ETA: 3s - loss: 0.0045 - acc: 0.999 - ETA: 3s - loss: 0.0045 - acc: 0.999 - ETA: 2s - loss: 0.0046 - acc: 0.999 - ETA: 1s - loss: 0.0045 - acc: 0.999 - ETA: 1s - loss: 0.0044 - acc: 0.999 - ETA: 0s - loss: 0.0044 - acc: 0.999 - 27s 6ms/step - loss: 0.0043 - acc: 0.9998 - val_loss: 0.0090 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00764\n",
      "Epoch 90/100\n",
      "4340/4340 [==============================] - ETA: 18s - loss: 0.0078 - acc: 0.99 - ETA: 19s - loss: 0.0055 - acc: 0.99 - ETA: 18s - loss: 0.0047 - acc: 0.99 - ETA: 17s - loss: 0.0039 - acc: 0.99 - ETA: 17s - loss: 0.0036 - acc: 0.99 - ETA: 16s - loss: 0.0037 - acc: 0.99 - ETA: 16s - loss: 0.0040 - acc: 0.99 - ETA: 16s - loss: 0.0038 - acc: 0.99 - ETA: 16s - loss: 0.0037 - acc: 0.99 - ETA: 15s - loss: 0.0035 - acc: 0.99 - ETA: 14s - loss: 0.0037 - acc: 0.99 - ETA: 13s - loss: 0.0040 - acc: 0.99 - ETA: 13s - loss: 0.0039 - acc: 0.99 - ETA: 13s - loss: 0.0039 - acc: 0.99 - ETA: 12s - loss: 0.0039 - acc: 0.99 - ETA: 11s - loss: 0.0042 - acc: 0.99 - ETA: 11s - loss: 0.0040 - acc: 0.99 - ETA: 10s - loss: 0.0039 - acc: 0.99 - ETA: 10s - loss: 0.0039 - acc: 0.99 - ETA: 9s - loss: 0.0045 - acc: 0.9992 - ETA: 8s - loss: 0.0043 - acc: 0.999 - ETA: 8s - loss: 0.0044 - acc: 0.999 - ETA: 7s - loss: 0.0042 - acc: 0.999 - ETA: 6s - loss: 0.0043 - acc: 0.999 - ETA: 5s - loss: 0.0042 - acc: 0.999 - ETA: 5s - loss: 0.0042 - acc: 0.999 - ETA: 4s - loss: 0.0043 - acc: 0.999 - ETA: 3s - loss: 0.0043 - acc: 0.999 - ETA: 3s - loss: 0.0043 - acc: 0.999 - ETA: 2s - loss: 0.0043 - acc: 0.999 - ETA: 1s - loss: 0.0044 - acc: 0.999 - ETA: 1s - loss: 0.0044 - acc: 0.999 - ETA: 0s - loss: 0.0044 - acc: 0.999 - 24s 6ms/step - loss: 0.0045 - acc: 0.9995 - val_loss: 0.0129 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00764\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4340/4340 [==============================] - ETA: 25s - loss: 0.0016 - acc: 1.00 - ETA: 23s - loss: 0.0015 - acc: 1.00 - ETA: 21s - loss: 0.0030 - acc: 1.00 - ETA: 21s - loss: 0.0025 - acc: 1.00 - ETA: 21s - loss: 0.0023 - acc: 1.00 - ETA: 19s - loss: 0.0022 - acc: 1.00 - ETA: 19s - loss: 0.0047 - acc: 0.99 - ETA: 18s - loss: 0.0074 - acc: 0.99 - ETA: 17s - loss: 0.0068 - acc: 0.99 - ETA: 17s - loss: 0.0067 - acc: 0.99 - ETA: 16s - loss: 0.0063 - acc: 0.99 - ETA: 15s - loss: 0.0059 - acc: 0.99 - ETA: 14s - loss: 0.0063 - acc: 0.99 - ETA: 13s - loss: 0.0059 - acc: 0.99 - ETA: 13s - loss: 0.0059 - acc: 0.99 - ETA: 12s - loss: 0.0056 - acc: 0.99 - ETA: 11s - loss: 0.0060 - acc: 0.99 - ETA: 11s - loss: 0.0057 - acc: 0.99 - ETA: 10s - loss: 0.0055 - acc: 0.99 - ETA: 9s - loss: 0.0054 - acc: 0.9984 - ETA: 9s - loss: 0.0065 - acc: 0.997 - ETA: 8s - loss: 0.0064 - acc: 0.997 - ETA: 7s - loss: 0.0063 - acc: 0.998 - ETA: 7s - loss: 0.0063 - acc: 0.998 - ETA: 6s - loss: 0.0062 - acc: 0.998 - ETA: 5s - loss: 0.0060 - acc: 0.998 - ETA: 4s - loss: 0.0060 - acc: 0.998 - ETA: 4s - loss: 0.0061 - acc: 0.998 - ETA: 3s - loss: 0.0061 - acc: 0.998 - ETA: 2s - loss: 0.0060 - acc: 0.998 - ETA: 2s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.998 - 26s 6ms/step - loss: 0.0057 - acc: 0.9986 - val_loss: 0.0095 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00764\n",
      "Epoch 92/100\n",
      "4340/4340 [==============================] - ETA: 20s - loss: 0.0062 - acc: 1.00 - ETA: 21s - loss: 0.0036 - acc: 1.00 - ETA: 18s - loss: 0.0044 - acc: 1.00 - ETA: 19s - loss: 0.0039 - acc: 1.00 - ETA: 19s - loss: 0.0034 - acc: 1.00 - ETA: 19s - loss: 0.0032 - acc: 1.00 - ETA: 18s - loss: 0.0030 - acc: 1.00 - ETA: 17s - loss: 0.0029 - acc: 1.00 - ETA: 16s - loss: 0.0044 - acc: 0.99 - ETA: 15s - loss: 0.0043 - acc: 0.99 - ETA: 17s - loss: 0.0043 - acc: 0.99 - ETA: 15s - loss: 0.0042 - acc: 0.99 - ETA: 15s - loss: 0.0042 - acc: 0.99 - ETA: 14s - loss: 0.0048 - acc: 0.99 - ETA: 13s - loss: 0.0046 - acc: 0.99 - ETA: 12s - loss: 0.0044 - acc: 0.99 - ETA: 12s - loss: 0.0042 - acc: 0.99 - ETA: 11s - loss: 0.0041 - acc: 0.99 - ETA: 10s - loss: 0.0040 - acc: 0.99 - ETA: 9s - loss: 0.0040 - acc: 0.9996 - ETA: 8s - loss: 0.0039 - acc: 0.999 - ETA: 8s - loss: 0.0040 - acc: 0.999 - ETA: 7s - loss: 0.0039 - acc: 0.999 - ETA: 6s - loss: 0.0040 - acc: 0.999 - ETA: 6s - loss: 0.0039 - acc: 0.999 - ETA: 5s - loss: 0.0038 - acc: 0.999 - ETA: 4s - loss: 0.0039 - acc: 0.999 - ETA: 3s - loss: 0.0039 - acc: 0.999 - ETA: 3s - loss: 0.0038 - acc: 0.999 - ETA: 2s - loss: 0.0039 - acc: 0.999 - ETA: 1s - loss: 0.0038 - acc: 0.999 - ETA: 1s - loss: 0.0038 - acc: 0.999 - ETA: 0s - loss: 0.0041 - acc: 0.999 - 25s 6ms/step - loss: 0.0040 - acc: 0.9993 - val_loss: 0.0109 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00764\n",
      "Epoch 93/100\n",
      "4340/4340 [==============================] - ETA: 24s - loss: 0.0045 - acc: 1.00 - ETA: 24s - loss: 0.0067 - acc: 0.99 - ETA: 21s - loss: 0.0061 - acc: 0.99 - ETA: 20s - loss: 0.0052 - acc: 0.99 - ETA: 19s - loss: 0.0053 - acc: 0.99 - ETA: 18s - loss: 0.0052 - acc: 0.99 - ETA: 17s - loss: 0.0046 - acc: 0.99 - ETA: 16s - loss: 0.0058 - acc: 0.99 - ETA: 16s - loss: 0.0053 - acc: 0.99 - ETA: 15s - loss: 0.0048 - acc: 0.99 - ETA: 14s - loss: 0.0048 - acc: 0.99 - ETA: 14s - loss: 0.0046 - acc: 0.99 - ETA: 13s - loss: 0.0046 - acc: 0.99 - ETA: 12s - loss: 0.0046 - acc: 0.99 - ETA: 11s - loss: 0.0044 - acc: 0.99 - ETA: 11s - loss: 0.0042 - acc: 0.99 - ETA: 10s - loss: 0.0045 - acc: 0.99 - ETA: 10s - loss: 0.0045 - acc: 0.99 - ETA: 9s - loss: 0.0045 - acc: 0.9992 - ETA: 8s - loss: 0.0046 - acc: 0.999 - ETA: 8s - loss: 0.0044 - acc: 0.999 - ETA: 7s - loss: 0.0047 - acc: 0.999 - ETA: 6s - loss: 0.0046 - acc: 0.999 - ETA: 6s - loss: 0.0046 - acc: 0.999 - ETA: 5s - loss: 0.0044 - acc: 0.999 - ETA: 5s - loss: 0.0043 - acc: 0.999 - ETA: 4s - loss: 0.0043 - acc: 0.999 - ETA: 3s - loss: 0.0043 - acc: 0.999 - ETA: 3s - loss: 0.0042 - acc: 0.999 - ETA: 2s - loss: 0.0041 - acc: 0.999 - ETA: 1s - loss: 0.0041 - acc: 0.999 - ETA: 1s - loss: 0.0040 - acc: 0.999 - ETA: 0s - loss: 0.0039 - acc: 0.999 - 24s 6ms/step - loss: 0.0039 - acc: 0.9995 - val_loss: 0.0086 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00764\n",
      "Epoch 94/100\n",
      "4340/4340 [==============================] - ETA: 24s - loss: 0.0032 - acc: 1.00 - ETA: 22s - loss: 0.0030 - acc: 1.00 - ETA: 22s - loss: 0.0074 - acc: 0.99 - ETA: 20s - loss: 0.0058 - acc: 0.99 - ETA: 19s - loss: 0.0059 - acc: 0.99 - ETA: 18s - loss: 0.0057 - acc: 0.99 - ETA: 18s - loss: 0.0056 - acc: 0.99 - ETA: 17s - loss: 0.0053 - acc: 0.99 - ETA: 17s - loss: 0.0050 - acc: 0.99 - ETA: 16s - loss: 0.0055 - acc: 0.99 - ETA: 16s - loss: 0.0051 - acc: 0.99 - ETA: 15s - loss: 0.0055 - acc: 0.99 - ETA: 14s - loss: 0.0053 - acc: 0.99 - ETA: 13s - loss: 0.0055 - acc: 0.99 - ETA: 12s - loss: 0.0053 - acc: 0.99 - ETA: 12s - loss: 0.0050 - acc: 0.99 - ETA: 11s - loss: 0.0048 - acc: 0.99 - ETA: 10s - loss: 0.0059 - acc: 0.99 - ETA: 10s - loss: 0.0057 - acc: 0.99 - ETA: 9s - loss: 0.0071 - acc: 0.9984 - ETA: 8s - loss: 0.0070 - acc: 0.998 - ETA: 7s - loss: 0.0067 - acc: 0.998 - ETA: 7s - loss: 0.0066 - acc: 0.998 - ETA: 6s - loss: 0.0071 - acc: 0.998 - ETA: 5s - loss: 0.0073 - acc: 0.998 - ETA: 5s - loss: 0.0072 - acc: 0.998 - ETA: 4s - loss: 0.0070 - acc: 0.998 - ETA: 3s - loss: 0.0070 - acc: 0.998 - ETA: 3s - loss: 0.0070 - acc: 0.998 - ETA: 2s - loss: 0.0070 - acc: 0.997 - ETA: 2s - loss: 0.0069 - acc: 0.998 - ETA: 1s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0071 - acc: 0.997 - 26s 6ms/step - loss: 0.0071 - acc: 0.9979 - val_loss: 0.0082 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00764\n",
      "Epoch 95/100\n",
      "4340/4340 [==============================] - ETA: 18s - loss: 0.0282 - acc: 0.99 - ETA: 18s - loss: 0.0192 - acc: 0.99 - ETA: 18s - loss: 0.0131 - acc: 0.99 - ETA: 17s - loss: 0.0128 - acc: 0.99 - ETA: 17s - loss: 0.0125 - acc: 0.99 - ETA: 18s - loss: 0.0116 - acc: 0.99 - ETA: 17s - loss: 0.0109 - acc: 0.99 - ETA: 17s - loss: 0.0099 - acc: 0.99 - ETA: 16s - loss: 0.0092 - acc: 0.99 - ETA: 15s - loss: 0.0088 - acc: 0.99 - ETA: 14s - loss: 0.0083 - acc: 0.99 - ETA: 14s - loss: 0.0079 - acc: 0.99 - ETA: 13s - loss: 0.0079 - acc: 0.99 - ETA: 13s - loss: 0.0090 - acc: 0.99 - ETA: 12s - loss: 0.0090 - acc: 0.99 - ETA: 12s - loss: 0.0088 - acc: 0.99 - ETA: 11s - loss: 0.0085 - acc: 0.99 - ETA: 10s - loss: 0.0082 - acc: 0.99 - ETA: 10s - loss: 0.0080 - acc: 0.99 - ETA: 9s - loss: 0.0077 - acc: 0.9980 - ETA: 9s - loss: 0.0074 - acc: 0.998 - ETA: 8s - loss: 0.0072 - acc: 0.998 - ETA: 7s - loss: 0.0071 - acc: 0.998 - ETA: 7s - loss: 0.0069 - acc: 0.998 - ETA: 6s - loss: 0.0067 - acc: 0.998 - ETA: 5s - loss: 0.0065 - acc: 0.998 - ETA: 4s - loss: 0.0063 - acc: 0.998 - ETA: 4s - loss: 0.0062 - acc: 0.998 - ETA: 3s - loss: 0.0061 - acc: 0.998 - ETA: 2s - loss: 0.0061 - acc: 0.998 - ETA: 2s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.998 - 26s 6ms/step - loss: 0.0058 - acc: 0.9988 - val_loss: 0.0089 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00764\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4340/4340 [==============================] - ETA: 20s - loss: 0.0044 - acc: 1.00 - ETA: 19s - loss: 0.0050 - acc: 1.00 - ETA: 18s - loss: 0.0040 - acc: 1.00 - ETA: 18s - loss: 0.0039 - acc: 1.00 - ETA: 17s - loss: 0.0048 - acc: 1.00 - ETA: 17s - loss: 0.0042 - acc: 1.00 - ETA: 17s - loss: 0.0038 - acc: 1.00 - ETA: 16s - loss: 0.0034 - acc: 1.00 - ETA: 15s - loss: 0.0053 - acc: 0.99 - ETA: 15s - loss: 0.0052 - acc: 0.99 - ETA: 14s - loss: 0.0049 - acc: 0.99 - ETA: 14s - loss: 0.0050 - acc: 0.99 - ETA: 13s - loss: 0.0050 - acc: 0.99 - ETA: 12s - loss: 0.0048 - acc: 0.99 - ETA: 12s - loss: 0.0048 - acc: 0.99 - ETA: 11s - loss: 0.0047 - acc: 0.99 - ETA: 11s - loss: 0.0046 - acc: 0.99 - ETA: 10s - loss: 0.0047 - acc: 0.99 - ETA: 9s - loss: 0.0045 - acc: 0.9996 - ETA: 9s - loss: 0.0044 - acc: 0.999 - ETA: 8s - loss: 0.0043 - acc: 0.999 - ETA: 7s - loss: 0.0047 - acc: 0.999 - ETA: 7s - loss: 0.0046 - acc: 0.999 - ETA: 6s - loss: 0.0046 - acc: 0.999 - ETA: 5s - loss: 0.0047 - acc: 0.999 - ETA: 5s - loss: 0.0046 - acc: 0.999 - ETA: 4s - loss: 0.0054 - acc: 0.999 - ETA: 3s - loss: 0.0052 - acc: 0.999 - ETA: 3s - loss: 0.0051 - acc: 0.999 - ETA: 2s - loss: 0.0055 - acc: 0.999 - ETA: 1s - loss: 0.0054 - acc: 0.999 - ETA: 1s - loss: 0.0053 - acc: 0.999 - ETA: 0s - loss: 0.0053 - acc: 0.999 - 24s 6ms/step - loss: 0.0052 - acc: 0.9991 - val_loss: 0.0073 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00764 to 0.00733, saving model to weight_MobileNet.hdf5\n",
      "Epoch 97/100\n",
      "4340/4340 [==============================] - ETA: 44s - loss: 0.0090 - acc: 1.00 - ETA: 40s - loss: 0.0061 - acc: 1.00 - ETA: 40s - loss: 0.0050 - acc: 1.00 - ETA: 37s - loss: 0.0039 - acc: 1.00 - ETA: 39s - loss: 0.0039 - acc: 1.00 - ETA: 37s - loss: 0.0036 - acc: 1.00 - ETA: 35s - loss: 0.0037 - acc: 1.00 - ETA: 34s - loss: 0.0037 - acc: 1.00 - ETA: 33s - loss: 0.0035 - acc: 1.00 - ETA: 31s - loss: 0.0035 - acc: 1.00 - ETA: 29s - loss: 0.0047 - acc: 0.99 - ETA: 28s - loss: 0.0047 - acc: 0.99 - ETA: 27s - loss: 0.0047 - acc: 0.99 - ETA: 25s - loss: 0.0045 - acc: 0.99 - ETA: 24s - loss: 0.0043 - acc: 0.99 - ETA: 23s - loss: 0.0043 - acc: 0.99 - ETA: 21s - loss: 0.0042 - acc: 0.99 - ETA: 20s - loss: 0.0041 - acc: 0.99 - ETA: 19s - loss: 0.0040 - acc: 0.99 - ETA: 17s - loss: 0.0047 - acc: 0.99 - ETA: 16s - loss: 0.0046 - acc: 0.99 - ETA: 14s - loss: 0.0048 - acc: 0.99 - ETA: 13s - loss: 0.0046 - acc: 0.99 - ETA: 11s - loss: 0.0045 - acc: 0.99 - ETA: 10s - loss: 0.0045 - acc: 0.99 - ETA: 9s - loss: 0.0045 - acc: 0.9991 - ETA: 7s - loss: 0.0045 - acc: 0.999 - ETA: 6s - loss: 0.0044 - acc: 0.999 - ETA: 5s - loss: 0.0043 - acc: 0.999 - ETA: 4s - loss: 0.0043 - acc: 0.999 - ETA: 3s - loss: 0.0042 - acc: 0.999 - ETA: 2s - loss: 0.0042 - acc: 0.999 - ETA: 1s - loss: 0.0041 - acc: 0.999 - 41s 9ms/step - loss: 0.0040 - acc: 0.9993 - val_loss: 0.0064 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00733 to 0.00644, saving model to weight_MobileNet.hdf5\n",
      "Epoch 98/100\n",
      "4340/4340 [==============================] - ETA: 15s - loss: 0.0018 - acc: 1.00 - ETA: 18s - loss: 0.0019 - acc: 1.00 - ETA: 17s - loss: 0.0019 - acc: 1.00 - ETA: 17s - loss: 0.0021 - acc: 1.00 - ETA: 18s - loss: 0.0019 - acc: 1.00 - ETA: 17s - loss: 0.0025 - acc: 1.00 - ETA: 16s - loss: 0.0023 - acc: 1.00 - ETA: 16s - loss: 0.0025 - acc: 1.00 - ETA: 16s - loss: 0.0029 - acc: 1.00 - ETA: 16s - loss: 0.0030 - acc: 1.00 - ETA: 15s - loss: 0.0028 - acc: 1.00 - ETA: 15s - loss: 0.0032 - acc: 0.99 - ETA: 14s - loss: 0.0032 - acc: 0.99 - ETA: 13s - loss: 0.0033 - acc: 0.99 - ETA: 12s - loss: 0.0035 - acc: 0.99 - ETA: 12s - loss: 0.0034 - acc: 0.99 - ETA: 11s - loss: 0.0032 - acc: 0.99 - ETA: 10s - loss: 0.0033 - acc: 0.99 - ETA: 10s - loss: 0.0032 - acc: 0.99 - ETA: 9s - loss: 0.0031 - acc: 0.9996 - ETA: 8s - loss: 0.0032 - acc: 0.999 - ETA: 8s - loss: 0.0032 - acc: 0.999 - ETA: 7s - loss: 0.0031 - acc: 0.999 - ETA: 6s - loss: 0.0030 - acc: 0.999 - ETA: 6s - loss: 0.0030 - acc: 0.999 - ETA: 5s - loss: 0.0029 - acc: 0.999 - ETA: 4s - loss: 0.0033 - acc: 0.999 - ETA: 4s - loss: 0.0035 - acc: 0.999 - ETA: 3s - loss: 0.0035 - acc: 0.999 - ETA: 2s - loss: 0.0037 - acc: 0.999 - ETA: 2s - loss: 0.0036 - acc: 0.999 - ETA: 1s - loss: 0.0044 - acc: 0.999 - ETA: 0s - loss: 0.0049 - acc: 0.998 - 25s 6ms/step - loss: 0.0048 - acc: 0.9988 - val_loss: 0.0148 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00644\n",
      "Epoch 99/100\n",
      "4340/4340 [==============================] - ETA: 22s - loss: 9.9463e-04 - acc: 1.00 - ETA: 20s - loss: 0.0045 - acc: 1.0000   - ETA: 22s - loss: 0.0044 - acc: 1.00 - ETA: 23s - loss: 0.0042 - acc: 1.00 - ETA: 22s - loss: 0.0053 - acc: 0.99 - ETA: 21s - loss: 0.0047 - acc: 0.99 - ETA: 19s - loss: 0.0044 - acc: 0.99 - ETA: 19s - loss: 0.0044 - acc: 0.99 - ETA: 17s - loss: 0.0046 - acc: 0.99 - ETA: 16s - loss: 0.0043 - acc: 0.99 - ETA: 16s - loss: 0.0045 - acc: 0.99 - ETA: 15s - loss: 0.0043 - acc: 0.99 - ETA: 14s - loss: 0.0041 - acc: 0.99 - ETA: 14s - loss: 0.0039 - acc: 0.99 - ETA: 13s - loss: 0.0039 - acc: 0.99 - ETA: 12s - loss: 0.0038 - acc: 0.99 - ETA: 11s - loss: 0.0037 - acc: 0.99 - ETA: 11s - loss: 0.0038 - acc: 0.99 - ETA: 10s - loss: 0.0037 - acc: 0.99 - ETA: 9s - loss: 0.0036 - acc: 0.9996 - ETA: 9s - loss: 0.0036 - acc: 0.999 - ETA: 8s - loss: 0.0036 - acc: 0.999 - ETA: 7s - loss: 0.0036 - acc: 0.999 - ETA: 7s - loss: 0.0036 - acc: 0.999 - ETA: 6s - loss: 0.0036 - acc: 0.999 - ETA: 5s - loss: 0.0034 - acc: 0.999 - ETA: 5s - loss: 0.0036 - acc: 0.999 - ETA: 4s - loss: 0.0036 - acc: 0.999 - ETA: 3s - loss: 0.0035 - acc: 0.999 - ETA: 3s - loss: 0.0035 - acc: 0.999 - ETA: 2s - loss: 0.0035 - acc: 0.999 - ETA: 1s - loss: 0.0036 - acc: 0.999 - ETA: 0s - loss: 0.0035 - acc: 0.999 - 29s 7ms/step - loss: 0.0035 - acc: 0.9998 - val_loss: 0.0068 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00644\n",
      "Epoch 100/100\n",
      "4340/4340 [==============================] - ETA: 23s - loss: 0.0016 - acc: 1.00 - ETA: 24s - loss: 0.0016 - acc: 1.00 - ETA: 24s - loss: 0.0022 - acc: 1.00 - ETA: 23s - loss: 0.0032 - acc: 1.00 - ETA: 22s - loss: 0.0029 - acc: 1.00 - ETA: 21s - loss: 0.0028 - acc: 1.00 - ETA: 20s - loss: 0.0025 - acc: 1.00 - ETA: 19s - loss: 0.0028 - acc: 1.00 - ETA: 19s - loss: 0.0027 - acc: 1.00 - ETA: 18s - loss: 0.0027 - acc: 1.00 - ETA: 17s - loss: 0.0026 - acc: 1.00 - ETA: 16s - loss: 0.0025 - acc: 1.00 - ETA: 15s - loss: 0.0024 - acc: 1.00 - ETA: 15s - loss: 0.0031 - acc: 0.99 - ETA: 14s - loss: 0.0031 - acc: 0.99 - ETA: 14s - loss: 0.0030 - acc: 0.99 - ETA: 13s - loss: 0.0029 - acc: 0.99 - ETA: 12s - loss: 0.0030 - acc: 0.99 - ETA: 12s - loss: 0.0032 - acc: 0.99 - ETA: 11s - loss: 0.0033 - acc: 0.99 - ETA: 10s - loss: 0.0033 - acc: 0.99 - ETA: 9s - loss: 0.0048 - acc: 0.9993 - ETA: 8s - loss: 0.0047 - acc: 0.999 - ETA: 7s - loss: 0.0046 - acc: 0.999 - ETA: 7s - loss: 0.0047 - acc: 0.999 - ETA: 6s - loss: 0.0046 - acc: 0.999 - ETA: 5s - loss: 0.0046 - acc: 0.999 - ETA: 4s - loss: 0.0045 - acc: 0.999 - ETA: 4s - loss: 0.0045 - acc: 0.999 - ETA: 3s - loss: 0.0045 - acc: 0.999 - ETA: 2s - loss: 0.0045 - acc: 0.999 - ETA: 1s - loss: 0.0043 - acc: 0.999 - ETA: 0s - loss: 0.0043 - acc: 0.999 - 33s 7ms/step - loss: 0.0043 - acc: 0.9995 - val_loss: 0.0095 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00644\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[mcp_save], batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
