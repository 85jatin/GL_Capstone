{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video Classification Model : https://towardsdatascience.com/transfer-learning-using-mobilenet-and-keras-c75daf7ff299"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1:  import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, InputLayer, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications import MobileNet\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from IPython.display import Image\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Read each frame and their corresponding tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A_OffSide_shot1.mp4_frame0.jpg</td>\n",
       "      <td>OffSide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A_OffSide_shot1.mp4_frame1.jpg</td>\n",
       "      <td>OffSide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A_OffSide_shot1.mp4_frame10.jpg</td>\n",
       "      <td>OffSide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A_OffSide_shot1.mp4_frame11.jpg</td>\n",
       "      <td>OffSide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>A_OffSide_shot1.mp4_frame12.jpg</td>\n",
       "      <td>OffSide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             image    class\n",
       "0   A_OffSide_shot1.mp4_frame0.jpg  OffSide\n",
       "1   A_OffSide_shot1.mp4_frame1.jpg  OffSide\n",
       "2  A_OffSide_shot1.mp4_frame10.jpg  OffSide\n",
       "3  A_OffSide_shot1.mp4_frame11.jpg  OffSide\n",
       "4  A_OffSide_shot1.mp4_frame12.jpg  OffSide"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train_new.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Read the frames and then store them as a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2809/2809 [00:19<00:00, 144.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2809, 224, 224, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an empty list\n",
    "train_image = []\n",
    "\n",
    "# for loop to read and store frames\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    # loading the image and keeping the target size as (224,224,3)\n",
    "    img = image.load_img('train_1/'+train['image'][i], target_size=(224,224,3))\n",
    "    # converting it to array\n",
    "    img = image.img_to_array(img)\n",
    "    # normalizing the pixel value\n",
    "    img = img/255\n",
    "    # appending the image to the train_image list\n",
    "    train_image.append(img)\n",
    "    \n",
    "# converting the list to numpy array\n",
    "X = np.array(train_image)\n",
    "\n",
    "# shape of the array\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: Create the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the target\n",
    "y = train['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training and validation set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: Create 4 different columns in the target, one for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dummies of target variable for train and validation set\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_test = pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OffSide     1689\n",
       "Legside      665\n",
       "Straight     455\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: Use the MobileNet pre-trained model to create the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now re-use MobileNet as it is quite lightweight (17Mb), freeze the base layers and lets add and train the top few layers (https://machinethink.net/blog/compressing-deep-neural-nets/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin_Thakkar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras_applications\\mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jatin_Thakkar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Jatin_Thakkar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Jatin_Thakkar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Jatin_Thakkar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Jatin_Thakkar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Jatin_Thakkar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#imports the mobilenet model and discards the last 1000 neuron layer.\n",
    "base_model = MobileNet(weights='imagenet',include_top=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: Extract features from this pre-trained model for our training and validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2247, 7, 7, 1024)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting features for training frames\n",
    "X_train = base_model.predict(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(562, 7, 7, 1024)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting features for validation frames\n",
    "X_test = base_model.predict(X_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8: reshape the images into a single dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the training as well as validation frames in single dimension\n",
    "X_train = X_train.reshape(2247, 7*7*1024)\n",
    "X_test = X_test.reshape(562, 7*7*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 9: Normalize the pixel values (between 0 and 1, helps the model to converge faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the pixel values\n",
    "max = X_train.max()\n",
    "X_train = X_train/max\n",
    "X_test = X_test/max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 10: Create the architecture of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2247, 50176)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of images\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jatin_Thakkar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "#defining the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu', input_shape=(50176,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 11: Train our model using the training frames and validate using validation frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to save the weights of best model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "mcp_save = ModelCheckpoint('weight_MobileNet.hdf5', save_best_only=True, monitor='val_loss', mode='min', verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving Generalization Performance by Switching from Adam to SGD\n",
    "(https://towardsdatascience.com/normalized-direction-preserving-adam-switching-from-adam-to-sgd-and-nesterov-momentum-adam-with-460be5ddf686)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jatin_Thakkar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compiling the model\n",
    "from keras.optimizers import SGD\n",
    "opt = SGD()\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1024)              51381248  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 52,070,659\n",
      "Trainable params: 52,070,659\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jatin_Thakkar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 2247 samples, validate on 562 samples\n",
      "Epoch 1/50\n",
      "2247/2247 [==============================] - ETA: 1:31 - loss: 1.1993 - acc: 0.375 - ETA: 49s - loss: 1.1919 - acc: 0.375 - ETA: 36s - loss: 1.1376 - acc: 0.43 - ETA: 28s - loss: 1.1282 - acc: 0.46 - ETA: 24s - loss: 1.1165 - acc: 0.47 - ETA: 22s - loss: 1.1200 - acc: 0.48 - ETA: 19s - loss: 1.1161 - acc: 0.50 - ETA: 17s - loss: 1.1006 - acc: 0.51 - ETA: 16s - loss: 1.1038 - acc: 0.51 - ETA: 15s - loss: 1.1114 - acc: 0.51 - ETA: 14s - loss: 1.1021 - acc: 0.51 - ETA: 13s - loss: 1.0931 - acc: 0.52 - ETA: 12s - loss: 1.0855 - acc: 0.52 - ETA: 11s - loss: 1.0897 - acc: 0.52 - ETA: 10s - loss: 1.0935 - acc: 0.51 - ETA: 10s - loss: 1.0813 - acc: 0.52 - ETA: 9s - loss: 1.0705 - acc: 0.5303 - ETA: 8s - loss: 1.0712 - acc: 0.530 - ETA: 8s - loss: 1.0721 - acc: 0.528 - ETA: 7s - loss: 1.0683 - acc: 0.532 - ETA: 7s - loss: 1.0633 - acc: 0.532 - ETA: 6s - loss: 1.0616 - acc: 0.538 - ETA: 5s - loss: 1.0559 - acc: 0.540 - ETA: 5s - loss: 1.0502 - acc: 0.543 - ETA: 4s - loss: 1.0495 - acc: 0.540 - ETA: 4s - loss: 1.0521 - acc: 0.540 - ETA: 3s - loss: 1.0468 - acc: 0.543 - ETA: 3s - loss: 1.0448 - acc: 0.545 - ETA: 2s - loss: 1.0402 - acc: 0.546 - ETA: 2s - loss: 1.0405 - acc: 0.547 - ETA: 1s - loss: 1.0450 - acc: 0.543 - ETA: 1s - loss: 1.0443 - acc: 0.544 - ETA: 0s - loss: 1.0462 - acc: 0.543 - ETA: 0s - loss: 1.0449 - acc: 0.542 - ETA: 0s - loss: 1.0406 - acc: 0.546 - 25s 11ms/step - loss: 1.0401 - acc: 0.5465 - val_loss: 0.8649 - val_acc: 0.6014\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.86489, saving model to weight_MobileNet.hdf5\n",
      "Epoch 2/50\n",
      "2247/2247 [==============================] - ETA: 45s - loss: 0.8816 - acc: 0.57 - ETA: 38s - loss: 0.9196 - acc: 0.53 - ETA: 34s - loss: 0.9728 - acc: 0.53 - ETA: 41s - loss: 0.9609 - acc: 0.54 - ETA: 38s - loss: 0.9587 - acc: 0.53 - ETA: 35s - loss: 0.9414 - acc: 0.54 - ETA: 31s - loss: 0.9422 - acc: 0.55 - ETA: 28s - loss: 0.9304 - acc: 0.56 - ETA: 26s - loss: 0.9404 - acc: 0.56 - ETA: 25s - loss: 0.9327 - acc: 0.57 - ETA: 23s - loss: 0.9258 - acc: 0.58 - ETA: 21s - loss: 0.9232 - acc: 0.58 - ETA: 19s - loss: 0.9373 - acc: 0.57 - ETA: 18s - loss: 0.9453 - acc: 0.57 - ETA: 17s - loss: 0.9507 - acc: 0.57 - ETA: 15s - loss: 0.9605 - acc: 0.56 - ETA: 14s - loss: 0.9586 - acc: 0.56 - ETA: 13s - loss: 0.9604 - acc: 0.56 - ETA: 12s - loss: 0.9642 - acc: 0.55 - ETA: 11s - loss: 0.9634 - acc: 0.55 - ETA: 10s - loss: 0.9686 - acc: 0.55 - ETA: 9s - loss: 0.9685 - acc: 0.5547 - ETA: 9s - loss: 0.9668 - acc: 0.555 - ETA: 8s - loss: 0.9628 - acc: 0.556 - ETA: 7s - loss: 0.9660 - acc: 0.555 - ETA: 6s - loss: 0.9630 - acc: 0.556 - ETA: 5s - loss: 0.9677 - acc: 0.555 - ETA: 5s - loss: 0.9612 - acc: 0.559 - ETA: 4s - loss: 0.9575 - acc: 0.561 - ETA: 3s - loss: 0.9530 - acc: 0.564 - ETA: 2s - loss: 0.9479 - acc: 0.568 - ETA: 2s - loss: 0.9501 - acc: 0.567 - ETA: 1s - loss: 0.9522 - acc: 0.567 - ETA: 0s - loss: 0.9483 - acc: 0.569 - ETA: 0s - loss: 0.9477 - acc: 0.569 - 25s 11ms/step - loss: 0.9481 - acc: 0.5696 - val_loss: 0.8102 - val_acc: 0.6423\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.86489 to 0.81023, saving model to weight_MobileNet.hdf5\n",
      "Epoch 3/50\n",
      "2247/2247 [==============================] - ETA: 18s - loss: 0.8609 - acc: 0.60 - ETA: 15s - loss: 0.9883 - acc: 0.57 - ETA: 16s - loss: 0.8818 - acc: 0.62 - ETA: 15s - loss: 0.9626 - acc: 0.57 - ETA: 14s - loss: 0.9648 - acc: 0.58 - ETA: 13s - loss: 0.9593 - acc: 0.58 - ETA: 13s - loss: 0.9589 - acc: 0.58 - ETA: 12s - loss: 0.9583 - acc: 0.58 - ETA: 11s - loss: 0.9426 - acc: 0.58 - ETA: 11s - loss: 0.9454 - acc: 0.58 - ETA: 10s - loss: 0.9383 - acc: 0.58 - ETA: 10s - loss: 0.9462 - acc: 0.58 - ETA: 10s - loss: 0.9458 - acc: 0.58 - ETA: 9s - loss: 0.9390 - acc: 0.5871 - ETA: 9s - loss: 0.9274 - acc: 0.592 - ETA: 8s - loss: 0.9299 - acc: 0.591 - ETA: 8s - loss: 0.9375 - acc: 0.584 - ETA: 8s - loss: 0.9424 - acc: 0.579 - ETA: 7s - loss: 0.9390 - acc: 0.582 - ETA: 7s - loss: 0.9352 - acc: 0.582 - ETA: 6s - loss: 0.9311 - acc: 0.584 - ETA: 6s - loss: 0.9263 - acc: 0.588 - ETA: 5s - loss: 0.9376 - acc: 0.583 - ETA: 5s - loss: 0.9366 - acc: 0.583 - ETA: 4s - loss: 0.9316 - acc: 0.586 - ETA: 4s - loss: 0.9260 - acc: 0.587 - ETA: 3s - loss: 0.9216 - acc: 0.589 - ETA: 3s - loss: 0.9170 - acc: 0.591 - ETA: 2s - loss: 0.9172 - acc: 0.590 - ETA: 2s - loss: 0.9149 - acc: 0.590 - ETA: 1s - loss: 0.9161 - acc: 0.590 - ETA: 1s - loss: 0.9193 - acc: 0.591 - ETA: 0s - loss: 0.9226 - acc: 0.590 - ETA: 0s - loss: 0.9194 - acc: 0.591 - ETA: 0s - loss: 0.9197 - acc: 0.591 - 17s 8ms/step - loss: 0.9190 - acc: 0.5915 - val_loss: 0.7692 - val_acc: 0.7295\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.81023 to 0.76923, saving model to weight_MobileNet.hdf5\n",
      "Epoch 4/50\n",
      "2247/2247 [==============================] - ETA: 12s - loss: 1.0858 - acc: 0.46 - ETA: 12s - loss: 0.9801 - acc: 0.53 - ETA: 11s - loss: 0.9231 - acc: 0.54 - ETA: 12s - loss: 0.9213 - acc: 0.56 - ETA: 11s - loss: 0.8882 - acc: 0.58 - ETA: 11s - loss: 0.8779 - acc: 0.58 - ETA: 11s - loss: 0.8805 - acc: 0.58 - ETA: 11s - loss: 0.8863 - acc: 0.58 - ETA: 10s - loss: 0.8953 - acc: 0.57 - ETA: 10s - loss: 0.9018 - acc: 0.57 - ETA: 9s - loss: 0.9000 - acc: 0.5753 - ETA: 9s - loss: 0.8791 - acc: 0.592 - ETA: 9s - loss: 0.8870 - acc: 0.590 - ETA: 8s - loss: 0.8777 - acc: 0.597 - ETA: 8s - loss: 0.8798 - acc: 0.594 - ETA: 8s - loss: 0.8844 - acc: 0.588 - ETA: 7s - loss: 0.8803 - acc: 0.592 - ETA: 7s - loss: 0.8747 - acc: 0.594 - ETA: 6s - loss: 0.8805 - acc: 0.592 - ETA: 6s - loss: 0.8794 - acc: 0.596 - ETA: 5s - loss: 0.8728 - acc: 0.599 - ETA: 5s - loss: 0.8656 - acc: 0.606 - ETA: 5s - loss: 0.8577 - acc: 0.610 - ETA: 4s - loss: 0.8552 - acc: 0.613 - ETA: 4s - loss: 0.8584 - acc: 0.611 - ETA: 3s - loss: 0.8497 - acc: 0.615 - ETA: 3s - loss: 0.8500 - acc: 0.615 - ETA: 3s - loss: 0.8503 - acc: 0.614 - ETA: 2s - loss: 0.8526 - acc: 0.616 - ETA: 2s - loss: 0.8527 - acc: 0.616 - ETA: 1s - loss: 0.8467 - acc: 0.622 - ETA: 1s - loss: 0.8460 - acc: 0.621 - ETA: 0s - loss: 0.8457 - acc: 0.620 - ETA: 0s - loss: 0.8440 - acc: 0.619 - ETA: 0s - loss: 0.8436 - acc: 0.620 - 17s 8ms/step - loss: 0.8425 - acc: 0.6213 - val_loss: 0.6880 - val_acc: 0.7633\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.76923 to 0.68795, saving model to weight_MobileNet.hdf5\n",
      "Epoch 5/50\n",
      "2247/2247 [==============================] - ETA: 12s - loss: 0.8412 - acc: 0.64 - ETA: 13s - loss: 0.8487 - acc: 0.55 - ETA: 13s - loss: 0.8198 - acc: 0.58 - ETA: 14s - loss: 0.7998 - acc: 0.61 - ETA: 16s - loss: 0.8041 - acc: 0.60 - ETA: 16s - loss: 0.7793 - acc: 0.62 - ETA: 15s - loss: 0.7807 - acc: 0.63 - ETA: 15s - loss: 0.7869 - acc: 0.63 - ETA: 14s - loss: 0.7756 - acc: 0.64 - ETA: 14s - loss: 0.7715 - acc: 0.64 - ETA: 13s - loss: 0.7660 - acc: 0.65 - ETA: 13s - loss: 0.7573 - acc: 0.65 - ETA: 12s - loss: 0.7543 - acc: 0.65 - ETA: 11s - loss: 0.7656 - acc: 0.66 - ETA: 10s - loss: 0.7645 - acc: 0.65 - ETA: 10s - loss: 0.7719 - acc: 0.65 - ETA: 9s - loss: 0.7722 - acc: 0.6498 - ETA: 9s - loss: 0.7674 - acc: 0.654 - ETA: 8s - loss: 0.7568 - acc: 0.662 - ETA: 7s - loss: 0.7587 - acc: 0.660 - ETA: 7s - loss: 0.7547 - acc: 0.664 - ETA: 6s - loss: 0.7623 - acc: 0.662 - ETA: 6s - loss: 0.7585 - acc: 0.667 - ETA: 5s - loss: 0.7577 - acc: 0.664 - ETA: 5s - loss: 0.7549 - acc: 0.666 - ETA: 4s - loss: 0.7564 - acc: 0.663 - ETA: 4s - loss: 0.7550 - acc: 0.666 - ETA: 3s - loss: 0.7529 - acc: 0.668 - ETA: 3s - loss: 0.7517 - acc: 0.669 - ETA: 2s - loss: 0.7551 - acc: 0.665 - ETA: 2s - loss: 0.7522 - acc: 0.667 - ETA: 1s - loss: 0.7499 - acc: 0.668 - ETA: 1s - loss: 0.7500 - acc: 0.667 - ETA: 0s - loss: 0.7486 - acc: 0.667 - ETA: 0s - loss: 0.7461 - acc: 0.667 - 19s 8ms/step - loss: 0.7459 - acc: 0.6671 - val_loss: 0.6277 - val_acc: 0.7509\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00005: val_loss improved from 0.68795 to 0.62770, saving model to weight_MobileNet.hdf5\n",
      "Epoch 6/50\n",
      "2247/2247 [==============================] - ETA: 15s - loss: 0.7758 - acc: 0.67 - ETA: 14s - loss: 0.7704 - acc: 0.64 - ETA: 13s - loss: 0.7371 - acc: 0.67 - ETA: 13s - loss: 0.7417 - acc: 0.66 - ETA: 13s - loss: 0.7630 - acc: 0.65 - ETA: 13s - loss: 0.7447 - acc: 0.65 - ETA: 13s - loss: 0.7380 - acc: 0.66 - ETA: 13s - loss: 0.7317 - acc: 0.66 - ETA: 13s - loss: 0.7349 - acc: 0.67 - ETA: 13s - loss: 0.7238 - acc: 0.68 - ETA: 12s - loss: 0.7147 - acc: 0.68 - ETA: 12s - loss: 0.7080 - acc: 0.69 - ETA: 11s - loss: 0.7115 - acc: 0.68 - ETA: 10s - loss: 0.7014 - acc: 0.69 - ETA: 10s - loss: 0.7140 - acc: 0.68 - ETA: 9s - loss: 0.7210 - acc: 0.6875 - ETA: 9s - loss: 0.7109 - acc: 0.695 - ETA: 8s - loss: 0.7111 - acc: 0.693 - ETA: 8s - loss: 0.7143 - acc: 0.692 - ETA: 7s - loss: 0.7146 - acc: 0.689 - ETA: 7s - loss: 0.7165 - acc: 0.689 - ETA: 6s - loss: 0.7086 - acc: 0.692 - ETA: 6s - loss: 0.7065 - acc: 0.691 - ETA: 5s - loss: 0.7072 - acc: 0.691 - ETA: 4s - loss: 0.7062 - acc: 0.692 - ETA: 4s - loss: 0.7021 - acc: 0.693 - ETA: 3s - loss: 0.7016 - acc: 0.693 - ETA: 3s - loss: 0.7055 - acc: 0.691 - ETA: 2s - loss: 0.7035 - acc: 0.695 - ETA: 2s - loss: 0.7000 - acc: 0.696 - ETA: 1s - loss: 0.7010 - acc: 0.696 - ETA: 1s - loss: 0.7014 - acc: 0.695 - ETA: 0s - loss: 0.6979 - acc: 0.698 - ETA: 0s - loss: 0.6957 - acc: 0.700 - ETA: 0s - loss: 0.6920 - acc: 0.701 - 18s 8ms/step - loss: 0.6921 - acc: 0.7023 - val_loss: 0.5270 - val_acc: 0.8185\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62770 to 0.52697, saving model to weight_MobileNet.hdf5\n",
      "Epoch 7/50\n",
      "2247/2247 [==============================] - ETA: 16s - loss: 0.8125 - acc: 0.60 - ETA: 16s - loss: 0.7085 - acc: 0.68 - ETA: 15s - loss: 0.6995 - acc: 0.68 - ETA: 15s - loss: 0.6949 - acc: 0.68 - ETA: 14s - loss: 0.6878 - acc: 0.69 - ETA: 13s - loss: 0.6805 - acc: 0.70 - ETA: 13s - loss: 0.6719 - acc: 0.70 - ETA: 12s - loss: 0.6509 - acc: 0.71 - ETA: 12s - loss: 0.6666 - acc: 0.71 - ETA: 11s - loss: 0.6692 - acc: 0.70 - ETA: 11s - loss: 0.6598 - acc: 0.71 - ETA: 10s - loss: 0.6663 - acc: 0.70 - ETA: 10s - loss: 0.6606 - acc: 0.71 - ETA: 9s - loss: 0.6610 - acc: 0.7154 - ETA: 9s - loss: 0.6585 - acc: 0.718 - ETA: 8s - loss: 0.6501 - acc: 0.722 - ETA: 8s - loss: 0.6492 - acc: 0.721 - ETA: 7s - loss: 0.6450 - acc: 0.721 - ETA: 7s - loss: 0.6476 - acc: 0.720 - ETA: 6s - loss: 0.6452 - acc: 0.723 - ETA: 6s - loss: 0.6413 - acc: 0.724 - ETA: 6s - loss: 0.6395 - acc: 0.724 - ETA: 5s - loss: 0.6374 - acc: 0.723 - ETA: 5s - loss: 0.6387 - acc: 0.723 - ETA: 4s - loss: 0.6366 - acc: 0.726 - ETA: 4s - loss: 0.6401 - acc: 0.723 - ETA: 3s - loss: 0.6388 - acc: 0.724 - ETA: 3s - loss: 0.6344 - acc: 0.727 - ETA: 2s - loss: 0.6334 - acc: 0.728 - ETA: 2s - loss: 0.6339 - acc: 0.728 - ETA: 1s - loss: 0.6329 - acc: 0.727 - ETA: 1s - loss: 0.6293 - acc: 0.731 - ETA: 0s - loss: 0.6279 - acc: 0.732 - ETA: 0s - loss: 0.6246 - acc: 0.734 - ETA: 0s - loss: 0.6244 - acc: 0.733 - 17s 8ms/step - loss: 0.6228 - acc: 0.7343 - val_loss: 0.4271 - val_acc: 0.8577\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.52697 to 0.42714, saving model to weight_MobileNet.hdf5\n",
      "Epoch 8/50\n",
      "2247/2247 [==============================] - ETA: 18s - loss: 0.5740 - acc: 0.76 - ETA: 15s - loss: 0.5829 - acc: 0.75 - ETA: 13s - loss: 0.5694 - acc: 0.75 - ETA: 12s - loss: 0.5587 - acc: 0.73 - ETA: 12s - loss: 0.5855 - acc: 0.73 - ETA: 12s - loss: 0.5802 - acc: 0.73 - ETA: 11s - loss: 0.5895 - acc: 0.72 - ETA: 11s - loss: 0.5613 - acc: 0.74 - ETA: 11s - loss: 0.5630 - acc: 0.74 - ETA: 10s - loss: 0.5606 - acc: 0.75 - ETA: 10s - loss: 0.5647 - acc: 0.74 - ETA: 10s - loss: 0.5613 - acc: 0.75 - ETA: 10s - loss: 0.5551 - acc: 0.75 - ETA: 9s - loss: 0.5532 - acc: 0.7623 - ETA: 9s - loss: 0.5592 - acc: 0.757 - ETA: 8s - loss: 0.5570 - acc: 0.756 - ETA: 8s - loss: 0.5597 - acc: 0.755 - ETA: 7s - loss: 0.5600 - acc: 0.756 - ETA: 7s - loss: 0.5550 - acc: 0.761 - ETA: 6s - loss: 0.5542 - acc: 0.761 - ETA: 6s - loss: 0.5551 - acc: 0.757 - ETA: 5s - loss: 0.5523 - acc: 0.759 - ETA: 5s - loss: 0.5507 - acc: 0.762 - ETA: 4s - loss: 0.5484 - acc: 0.765 - ETA: 4s - loss: 0.5431 - acc: 0.767 - ETA: 4s - loss: 0.5428 - acc: 0.767 - ETA: 3s - loss: 0.5507 - acc: 0.766 - ETA: 3s - loss: 0.5434 - acc: 0.771 - ETA: 3s - loss: 0.5432 - acc: 0.771 - ETA: 2s - loss: 0.5415 - acc: 0.769 - ETA: 2s - loss: 0.5374 - acc: 0.771 - ETA: 1s - loss: 0.5355 - acc: 0.772 - ETA: 1s - loss: 0.5366 - acc: 0.772 - ETA: 0s - loss: 0.5364 - acc: 0.771 - ETA: 0s - loss: 0.5341 - acc: 0.772 - 19s 9ms/step - loss: 0.5335 - acc: 0.7730 - val_loss: 0.4208 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.42714 to 0.42079, saving model to weight_MobileNet.hdf5\n",
      "Epoch 9/50\n",
      "2247/2247 [==============================] - ETA: 14s - loss: 0.6459 - acc: 0.68 - ETA: 16s - loss: 0.5808 - acc: 0.71 - ETA: 15s - loss: 0.5543 - acc: 0.73 - ETA: 16s - loss: 0.5115 - acc: 0.76 - ETA: 15s - loss: 0.5134 - acc: 0.76 - ETA: 14s - loss: 0.5039 - acc: 0.78 - ETA: 14s - loss: 0.5003 - acc: 0.78 - ETA: 13s - loss: 0.5245 - acc: 0.77 - ETA: 12s - loss: 0.5044 - acc: 0.78 - ETA: 11s - loss: 0.5059 - acc: 0.78 - ETA: 11s - loss: 0.5036 - acc: 0.78 - ETA: 10s - loss: 0.4916 - acc: 0.79 - ETA: 10s - loss: 0.4989 - acc: 0.78 - ETA: 10s - loss: 0.4912 - acc: 0.79 - ETA: 9s - loss: 0.4952 - acc: 0.7896 - ETA: 8s - loss: 0.4991 - acc: 0.790 - ETA: 8s - loss: 0.4968 - acc: 0.792 - ETA: 7s - loss: 0.5056 - acc: 0.790 - ETA: 7s - loss: 0.5044 - acc: 0.792 - ETA: 7s - loss: 0.5050 - acc: 0.791 - ETA: 6s - loss: 0.5059 - acc: 0.789 - ETA: 6s - loss: 0.4982 - acc: 0.793 - ETA: 5s - loss: 0.4919 - acc: 0.796 - ETA: 5s - loss: 0.4981 - acc: 0.794 - ETA: 4s - loss: 0.5000 - acc: 0.795 - ETA: 4s - loss: 0.4952 - acc: 0.797 - ETA: 3s - loss: 0.4890 - acc: 0.801 - ETA: 3s - loss: 0.4889 - acc: 0.801 - ETA: 2s - loss: 0.4857 - acc: 0.802 - ETA: 2s - loss: 0.4826 - acc: 0.802 - ETA: 1s - loss: 0.4787 - acc: 0.806 - ETA: 1s - loss: 0.4795 - acc: 0.806 - ETA: 0s - loss: 0.4745 - acc: 0.807 - ETA: 0s - loss: 0.4720 - acc: 0.809 - ETA: 0s - loss: 0.4711 - acc: 0.809 - 17s 8ms/step - loss: 0.4702 - acc: 0.8100 - val_loss: 0.2885 - val_acc: 0.8790\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.42079 to 0.28853, saving model to weight_MobileNet.hdf5\n",
      "Epoch 10/50\n",
      "2247/2247 [==============================] - ETA: 12s - loss: 0.4272 - acc: 0.79 - ETA: 15s - loss: 0.4189 - acc: 0.82 - ETA: 14s - loss: 0.4486 - acc: 0.80 - ETA: 14s - loss: 0.4726 - acc: 0.80 - ETA: 13s - loss: 0.4503 - acc: 0.80 - ETA: 13s - loss: 0.4254 - acc: 0.82 - ETA: 12s - loss: 0.4070 - acc: 0.83 - ETA: 12s - loss: 0.4206 - acc: 0.82 - ETA: 11s - loss: 0.4163 - acc: 0.83 - ETA: 11s - loss: 0.4140 - acc: 0.83 - ETA: 10s - loss: 0.4180 - acc: 0.82 - ETA: 10s - loss: 0.4133 - acc: 0.82 - ETA: 10s - loss: 0.4041 - acc: 0.83 - ETA: 9s - loss: 0.4007 - acc: 0.8326 - ETA: 8s - loss: 0.3987 - acc: 0.834 - ETA: 8s - loss: 0.3974 - acc: 0.835 - ETA: 7s - loss: 0.4045 - acc: 0.830 - ETA: 7s - loss: 0.4038 - acc: 0.830 - ETA: 7s - loss: 0.4032 - acc: 0.833 - ETA: 6s - loss: 0.4016 - acc: 0.833 - ETA: 6s - loss: 0.3991 - acc: 0.834 - ETA: 5s - loss: 0.4007 - acc: 0.835 - ETA: 5s - loss: 0.3993 - acc: 0.834 - ETA: 4s - loss: 0.4021 - acc: 0.833 - ETA: 4s - loss: 0.3994 - acc: 0.834 - ETA: 4s - loss: 0.3996 - acc: 0.834 - ETA: 3s - loss: 0.3990 - acc: 0.835 - ETA: 3s - loss: 0.3992 - acc: 0.835 - ETA: 2s - loss: 0.3954 - acc: 0.838 - ETA: 2s - loss: 0.3933 - acc: 0.840 - ETA: 1s - loss: 0.3943 - acc: 0.839 - ETA: 1s - loss: 0.3930 - acc: 0.839 - ETA: 0s - loss: 0.3904 - acc: 0.842 - ETA: 0s - loss: 0.3914 - acc: 0.842 - ETA: 0s - loss: 0.3938 - acc: 0.841 - 17s 8ms/step - loss: 0.3933 - acc: 0.8411 - val_loss: 0.2299 - val_acc: 0.9484\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.28853 to 0.22994, saving model to weight_MobileNet.hdf5\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2247/2247 [==============================] - ETA: 12s - loss: 0.2792 - acc: 0.90 - ETA: 16s - loss: 0.3013 - acc: 0.89 - ETA: 15s - loss: 0.3085 - acc: 0.88 - ETA: 13s - loss: 0.3242 - acc: 0.87 - ETA: 13s - loss: 0.3425 - acc: 0.86 - ETA: 12s - loss: 0.3449 - acc: 0.85 - ETA: 11s - loss: 0.3489 - acc: 0.85 - ETA: 11s - loss: 0.3598 - acc: 0.84 - ETA: 11s - loss: 0.3632 - acc: 0.84 - ETA: 10s - loss: 0.3580 - acc: 0.85 - ETA: 10s - loss: 0.3583 - acc: 0.85 - ETA: 10s - loss: 0.3575 - acc: 0.85 - ETA: 9s - loss: 0.3485 - acc: 0.8606 - ETA: 9s - loss: 0.3532 - acc: 0.857 - ETA: 9s - loss: 0.3497 - acc: 0.860 - ETA: 8s - loss: 0.3565 - acc: 0.857 - ETA: 8s - loss: 0.3578 - acc: 0.856 - ETA: 8s - loss: 0.3594 - acc: 0.855 - ETA: 7s - loss: 0.3604 - acc: 0.852 - ETA: 7s - loss: 0.3563 - acc: 0.855 - ETA: 6s - loss: 0.3512 - acc: 0.857 - ETA: 6s - loss: 0.3464 - acc: 0.860 - ETA: 5s - loss: 0.3428 - acc: 0.864 - ETA: 5s - loss: 0.3443 - acc: 0.864 - ETA: 4s - loss: 0.3515 - acc: 0.861 - ETA: 4s - loss: 0.3539 - acc: 0.861 - ETA: 3s - loss: 0.3517 - acc: 0.864 - ETA: 3s - loss: 0.3512 - acc: 0.864 - ETA: 2s - loss: 0.3496 - acc: 0.865 - ETA: 2s - loss: 0.3479 - acc: 0.865 - ETA: 1s - loss: 0.3462 - acc: 0.866 - ETA: 1s - loss: 0.3430 - acc: 0.868 - ETA: 0s - loss: 0.3423 - acc: 0.869 - ETA: 0s - loss: 0.3378 - acc: 0.870 - ETA: 0s - loss: 0.3360 - acc: 0.871 - 17s 8ms/step - loss: 0.3358 - acc: 0.8714 - val_loss: 0.4759 - val_acc: 0.7740\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.22994\n",
      "Epoch 12/50\n",
      "2247/2247 [==============================] - ETA: 13s - loss: 0.8678 - acc: 0.73 - ETA: 12s - loss: 0.6389 - acc: 0.78 - ETA: 13s - loss: 0.5637 - acc: 0.80 - ETA: 13s - loss: 0.4974 - acc: 0.83 - ETA: 12s - loss: 0.4846 - acc: 0.82 - ETA: 12s - loss: 0.4713 - acc: 0.83 - ETA: 11s - loss: 0.4345 - acc: 0.84 - ETA: 11s - loss: 0.4049 - acc: 0.85 - ETA: 11s - loss: 0.3886 - acc: 0.86 - ETA: 10s - loss: 0.3853 - acc: 0.86 - ETA: 10s - loss: 0.3818 - acc: 0.86 - ETA: 9s - loss: 0.3782 - acc: 0.8685 - ETA: 9s - loss: 0.3808 - acc: 0.864 - ETA: 8s - loss: 0.3814 - acc: 0.865 - ETA: 8s - loss: 0.3700 - acc: 0.871 - ETA: 7s - loss: 0.3568 - acc: 0.876 - ETA: 7s - loss: 0.3594 - acc: 0.874 - ETA: 7s - loss: 0.3555 - acc: 0.874 - ETA: 6s - loss: 0.3570 - acc: 0.873 - ETA: 6s - loss: 0.3551 - acc: 0.875 - ETA: 6s - loss: 0.3524 - acc: 0.875 - ETA: 6s - loss: 0.3485 - acc: 0.877 - ETA: 5s - loss: 0.3453 - acc: 0.878 - ETA: 5s - loss: 0.3368 - acc: 0.882 - ETA: 4s - loss: 0.3331 - acc: 0.884 - ETA: 4s - loss: 0.3326 - acc: 0.884 - ETA: 4s - loss: 0.3317 - acc: 0.882 - ETA: 3s - loss: 0.3269 - acc: 0.884 - ETA: 3s - loss: 0.3252 - acc: 0.885 - ETA: 2s - loss: 0.3198 - acc: 0.888 - ETA: 2s - loss: 0.3161 - acc: 0.889 - ETA: 1s - loss: 0.3141 - acc: 0.889 - ETA: 1s - loss: 0.3112 - acc: 0.891 - ETA: 0s - loss: 0.3095 - acc: 0.892 - ETA: 0s - loss: 0.3084 - acc: 0.892 - 24s 11ms/step - loss: 0.3075 - acc: 0.8932 - val_loss: 0.1466 - val_acc: 0.9591\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.22994 to 0.14660, saving model to weight_MobileNet.hdf5\n",
      "Epoch 13/50\n",
      "2247/2247 [==============================] - ETA: 25s - loss: 0.3004 - acc: 0.85 - ETA: 18s - loss: 0.2592 - acc: 0.89 - ETA: 16s - loss: 0.2593 - acc: 0.90 - ETA: 15s - loss: 0.2681 - acc: 0.89 - ETA: 14s - loss: 0.2710 - acc: 0.90 - ETA: 13s - loss: 0.2539 - acc: 0.90 - ETA: 13s - loss: 0.2371 - acc: 0.91 - ETA: 12s - loss: 0.2438 - acc: 0.91 - ETA: 12s - loss: 0.2346 - acc: 0.91 - ETA: 11s - loss: 0.2414 - acc: 0.91 - ETA: 11s - loss: 0.2380 - acc: 0.91 - ETA: 10s - loss: 0.2398 - acc: 0.91 - ETA: 10s - loss: 0.2376 - acc: 0.91 - ETA: 9s - loss: 0.2345 - acc: 0.9141 - ETA: 9s - loss: 0.2342 - acc: 0.915 - ETA: 8s - loss: 0.2361 - acc: 0.916 - ETA: 8s - loss: 0.2337 - acc: 0.918 - ETA: 7s - loss: 0.2341 - acc: 0.918 - ETA: 7s - loss: 0.2333 - acc: 0.919 - ETA: 6s - loss: 0.2311 - acc: 0.920 - ETA: 6s - loss: 0.2275 - acc: 0.922 - ETA: 5s - loss: 0.2292 - acc: 0.921 - ETA: 5s - loss: 0.2295 - acc: 0.921 - ETA: 5s - loss: 0.2303 - acc: 0.919 - ETA: 4s - loss: 0.2268 - acc: 0.920 - ETA: 4s - loss: 0.2242 - acc: 0.922 - ETA: 3s - loss: 0.2259 - acc: 0.922 - ETA: 3s - loss: 0.2251 - acc: 0.923 - ETA: 2s - loss: 0.2230 - acc: 0.925 - ETA: 2s - loss: 0.2240 - acc: 0.925 - ETA: 1s - loss: 0.2252 - acc: 0.925 - ETA: 1s - loss: 0.2242 - acc: 0.924 - ETA: 1s - loss: 0.2225 - acc: 0.924 - ETA: 0s - loss: 0.2228 - acc: 0.924 - ETA: 0s - loss: 0.2206 - acc: 0.925 - 18s 8ms/step - loss: 0.2202 - acc: 0.9261 - val_loss: 0.0994 - val_acc: 0.9715\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.14660 to 0.09941, saving model to weight_MobileNet.hdf5\n",
      "Epoch 14/50\n",
      "2247/2247 [==============================] - ETA: 15s - loss: 0.2527 - acc: 0.93 - ETA: 15s - loss: 0.2390 - acc: 0.94 - ETA: 17s - loss: 0.2451 - acc: 0.92 - ETA: 15s - loss: 0.2189 - acc: 0.94 - ETA: 14s - loss: 0.2139 - acc: 0.94 - ETA: 13s - loss: 0.1949 - acc: 0.95 - ETA: 13s - loss: 0.1969 - acc: 0.94 - ETA: 12s - loss: 0.2027 - acc: 0.94 - ETA: 12s - loss: 0.2017 - acc: 0.94 - ETA: 12s - loss: 0.2073 - acc: 0.93 - ETA: 11s - loss: 0.2087 - acc: 0.93 - ETA: 11s - loss: 0.1995 - acc: 0.94 - ETA: 10s - loss: 0.1979 - acc: 0.94 - ETA: 10s - loss: 0.1933 - acc: 0.94 - ETA: 9s - loss: 0.1943 - acc: 0.9396 - ETA: 9s - loss: 0.1907 - acc: 0.940 - ETA: 8s - loss: 0.1929 - acc: 0.940 - ETA: 7s - loss: 0.1939 - acc: 0.940 - ETA: 7s - loss: 0.1928 - acc: 0.940 - ETA: 7s - loss: 0.1950 - acc: 0.939 - ETA: 6s - loss: 0.1954 - acc: 0.937 - ETA: 6s - loss: 0.1952 - acc: 0.937 - ETA: 5s - loss: 0.1916 - acc: 0.938 - ETA: 5s - loss: 0.1902 - acc: 0.938 - ETA: 4s - loss: 0.1902 - acc: 0.938 - ETA: 4s - loss: 0.1917 - acc: 0.936 - ETA: 3s - loss: 0.1921 - acc: 0.936 - ETA: 3s - loss: 0.1884 - acc: 0.937 - ETA: 2s - loss: 0.1873 - acc: 0.937 - ETA: 2s - loss: 0.1884 - acc: 0.937 - ETA: 1s - loss: 0.1878 - acc: 0.938 - ETA: 1s - loss: 0.1884 - acc: 0.937 - ETA: 1s - loss: 0.1886 - acc: 0.938 - ETA: 0s - loss: 0.1864 - acc: 0.938 - ETA: 0s - loss: 0.1876 - acc: 0.937 - 18s 8ms/step - loss: 0.1882 - acc: 0.9377 - val_loss: 0.0842 - val_acc: 0.9698\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.09941 to 0.08416, saving model to weight_MobileNet.hdf5\n",
      "Epoch 15/50\n",
      "2247/2247 [==============================] - ETA: 12s - loss: 0.1699 - acc: 0.93 - ETA: 16s - loss: 0.2276 - acc: 0.92 - ETA: 15s - loss: 0.1906 - acc: 0.95 - ETA: 14s - loss: 0.1912 - acc: 0.94 - ETA: 14s - loss: 0.1756 - acc: 0.95 - ETA: 14s - loss: 0.1651 - acc: 0.95 - ETA: 13s - loss: 0.1602 - acc: 0.95 - ETA: 13s - loss: 0.1622 - acc: 0.95 - ETA: 12s - loss: 0.1608 - acc: 0.96 - ETA: 11s - loss: 0.1565 - acc: 0.96 - ETA: 11s - loss: 0.1542 - acc: 0.96 - ETA: 10s - loss: 0.1542 - acc: 0.96 - ETA: 10s - loss: 0.1589 - acc: 0.96 - ETA: 9s - loss: 0.1659 - acc: 0.9565 - ETA: 9s - loss: 0.1633 - acc: 0.958 - ETA: 9s - loss: 0.1629 - acc: 0.959 - ETA: 8s - loss: 0.1585 - acc: 0.960 - ETA: 8s - loss: 0.1627 - acc: 0.956 - ETA: 7s - loss: 0.1632 - acc: 0.956 - ETA: 7s - loss: 0.1641 - acc: 0.956 - ETA: 6s - loss: 0.1631 - acc: 0.954 - ETA: 6s - loss: 0.1616 - acc: 0.954 - ETA: 5s - loss: 0.1628 - acc: 0.953 - ETA: 5s - loss: 0.1621 - acc: 0.954 - ETA: 4s - loss: 0.1603 - acc: 0.954 - ETA: 4s - loss: 0.1574 - acc: 0.955 - ETA: 3s - loss: 0.1557 - acc: 0.956 - ETA: 3s - loss: 0.1552 - acc: 0.956 - ETA: 2s - loss: 0.1538 - acc: 0.956 - ETA: 2s - loss: 0.1520 - acc: 0.957 - ETA: 1s - loss: 0.1520 - acc: 0.956 - ETA: 1s - loss: 0.1516 - acc: 0.956 - ETA: 1s - loss: 0.1515 - acc: 0.955 - ETA: 0s - loss: 0.1500 - acc: 0.956 - ETA: 0s - loss: 0.1489 - acc: 0.956 - 18s 8ms/step - loss: 0.1493 - acc: 0.9564 - val_loss: 0.0746 - val_acc: 0.9786\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.08416 to 0.07464, saving model to weight_MobileNet.hdf5\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2247/2247 [==============================] - ETA: 19s - loss: 0.2700 - acc: 0.84 - ETA: 16s - loss: 0.1750 - acc: 0.91 - ETA: 15s - loss: 0.2199 - acc: 0.91 - ETA: 13s - loss: 0.2256 - acc: 0.91 - ETA: 13s - loss: 0.2082 - acc: 0.91 - ETA: 13s - loss: 0.2086 - acc: 0.92 - ETA: 12s - loss: 0.1886 - acc: 0.93 - ETA: 12s - loss: 0.1849 - acc: 0.93 - ETA: 11s - loss: 0.1890 - acc: 0.93 - ETA: 11s - loss: 0.1839 - acc: 0.93 - ETA: 11s - loss: 0.1782 - acc: 0.94 - ETA: 10s - loss: 0.1732 - acc: 0.94 - ETA: 10s - loss: 0.1710 - acc: 0.94 - ETA: 9s - loss: 0.1709 - acc: 0.9420 - ETA: 9s - loss: 0.1721 - acc: 0.942 - ETA: 8s - loss: 0.1692 - acc: 0.944 - ETA: 8s - loss: 0.1650 - acc: 0.944 - ETA: 7s - loss: 0.1608 - acc: 0.947 - ETA: 7s - loss: 0.1618 - acc: 0.949 - ETA: 6s - loss: 0.1608 - acc: 0.949 - ETA: 6s - loss: 0.1592 - acc: 0.950 - ETA: 5s - loss: 0.1566 - acc: 0.952 - ETA: 5s - loss: 0.1540 - acc: 0.953 - ETA: 4s - loss: 0.1528 - acc: 0.954 - ETA: 4s - loss: 0.1509 - acc: 0.954 - ETA: 3s - loss: 0.1483 - acc: 0.954 - ETA: 3s - loss: 0.1465 - acc: 0.956 - ETA: 3s - loss: 0.1443 - acc: 0.957 - ETA: 2s - loss: 0.1428 - acc: 0.956 - ETA: 2s - loss: 0.1439 - acc: 0.956 - ETA: 1s - loss: 0.1435 - acc: 0.957 - ETA: 1s - loss: 0.1420 - acc: 0.958 - ETA: 0s - loss: 0.1409 - acc: 0.958 - ETA: 0s - loss: 0.1394 - acc: 0.959 - ETA: 0s - loss: 0.1404 - acc: 0.958 - 17s 7ms/step - loss: 0.1402 - acc: 0.9591 - val_loss: 0.0442 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.07464 to 0.04416, saving model to weight_MobileNet.hdf5\n",
      "Epoch 17/50\n",
      "2247/2247 [==============================] - ETA: 11s - loss: 0.0969 - acc: 0.96 - ETA: 12s - loss: 0.1068 - acc: 0.96 - ETA: 12s - loss: 0.1050 - acc: 0.97 - ETA: 11s - loss: 0.1082 - acc: 0.97 - ETA: 12s - loss: 0.1233 - acc: 0.96 - ETA: 12s - loss: 0.1252 - acc: 0.96 - ETA: 12s - loss: 0.1236 - acc: 0.96 - ETA: 11s - loss: 0.1285 - acc: 0.95 - ETA: 11s - loss: 0.1225 - acc: 0.96 - ETA: 11s - loss: 0.1193 - acc: 0.96 - ETA: 10s - loss: 0.1215 - acc: 0.96 - ETA: 10s - loss: 0.1180 - acc: 0.96 - ETA: 9s - loss: 0.1159 - acc: 0.9639 - ETA: 9s - loss: 0.1150 - acc: 0.964 - ETA: 8s - loss: 0.1141 - acc: 0.963 - ETA: 8s - loss: 0.1114 - acc: 0.963 - ETA: 8s - loss: 0.1130 - acc: 0.961 - ETA: 7s - loss: 0.1123 - acc: 0.962 - ETA: 7s - loss: 0.1118 - acc: 0.963 - ETA: 6s - loss: 0.1088 - acc: 0.964 - ETA: 6s - loss: 0.1077 - acc: 0.964 - ETA: 6s - loss: 0.1074 - acc: 0.965 - ETA: 5s - loss: 0.1078 - acc: 0.964 - ETA: 5s - loss: 0.1062 - acc: 0.965 - ETA: 4s - loss: 0.1072 - acc: 0.965 - ETA: 4s - loss: 0.1068 - acc: 0.965 - ETA: 3s - loss: 0.1054 - acc: 0.967 - ETA: 3s - loss: 0.1063 - acc: 0.966 - ETA: 2s - loss: 0.1058 - acc: 0.967 - ETA: 2s - loss: 0.1049 - acc: 0.967 - ETA: 1s - loss: 0.1047 - acc: 0.968 - ETA: 1s - loss: 0.1051 - acc: 0.968 - ETA: 0s - loss: 0.1043 - acc: 0.968 - ETA: 0s - loss: 0.1046 - acc: 0.968 - ETA: 0s - loss: 0.1048 - acc: 0.968 - 17s 8ms/step - loss: 0.1046 - acc: 0.9688 - val_loss: 0.0312 - val_acc: 0.9911\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.04416 to 0.03117, saving model to weight_MobileNet.hdf5\n",
      "Epoch 18/50\n",
      "2247/2247 [==============================] - ETA: 12s - loss: 0.0644 - acc: 1.00 - ETA: 12s - loss: 0.0714 - acc: 0.99 - ETA: 13s - loss: 0.0760 - acc: 0.98 - ETA: 12s - loss: 0.0743 - acc: 0.98 - ETA: 12s - loss: 0.0759 - acc: 0.98 - ETA: 12s - loss: 0.0784 - acc: 0.98 - ETA: 11s - loss: 0.0847 - acc: 0.98 - ETA: 11s - loss: 0.0846 - acc: 0.98 - ETA: 11s - loss: 0.0862 - acc: 0.97 - ETA: 10s - loss: 0.0847 - acc: 0.97 - ETA: 10s - loss: 0.0860 - acc: 0.97 - ETA: 9s - loss: 0.0847 - acc: 0.9766 - ETA: 9s - loss: 0.0847 - acc: 0.977 - ETA: 9s - loss: 0.0840 - acc: 0.977 - ETA: 8s - loss: 0.0839 - acc: 0.977 - ETA: 8s - loss: 0.0811 - acc: 0.978 - ETA: 7s - loss: 0.0792 - acc: 0.979 - ETA: 7s - loss: 0.0787 - acc: 0.979 - ETA: 6s - loss: 0.0780 - acc: 0.980 - ETA: 6s - loss: 0.0805 - acc: 0.979 - ETA: 6s - loss: 0.0826 - acc: 0.979 - ETA: 5s - loss: 0.0809 - acc: 0.980 - ETA: 5s - loss: 0.0825 - acc: 0.980 - ETA: 4s - loss: 0.0822 - acc: 0.980 - ETA: 4s - loss: 0.0825 - acc: 0.979 - ETA: 3s - loss: 0.0826 - acc: 0.979 - ETA: 3s - loss: 0.0822 - acc: 0.979 - ETA: 3s - loss: 0.0818 - acc: 0.979 - ETA: 2s - loss: 0.0840 - acc: 0.978 - ETA: 2s - loss: 0.0842 - acc: 0.978 - ETA: 1s - loss: 0.0848 - acc: 0.978 - ETA: 1s - loss: 0.0849 - acc: 0.977 - ETA: 0s - loss: 0.0841 - acc: 0.977 - ETA: 0s - loss: 0.0835 - acc: 0.977 - ETA: 0s - loss: 0.0830 - acc: 0.977 - 17s 7ms/step - loss: 0.0829 - acc: 0.9777 - val_loss: 0.0272 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.03117 to 0.02723, saving model to weight_MobileNet.hdf5\n",
      "Epoch 19/50\n",
      "2247/2247 [==============================] - ETA: 12s - loss: 0.0588 - acc: 1.00 - ETA: 12s - loss: 0.0738 - acc: 0.98 - ETA: 12s - loss: 0.0929 - acc: 0.97 - ETA: 12s - loss: 0.1081 - acc: 0.97 - ETA: 12s - loss: 0.1096 - acc: 0.96 - ETA: 11s - loss: 0.1017 - acc: 0.97 - ETA: 11s - loss: 0.1011 - acc: 0.97 - ETA: 11s - loss: 0.1038 - acc: 0.97 - ETA: 11s - loss: 0.0994 - acc: 0.97 - ETA: 10s - loss: 0.1052 - acc: 0.97 - ETA: 10s - loss: 0.0997 - acc: 0.97 - ETA: 9s - loss: 0.0997 - acc: 0.9740 - ETA: 9s - loss: 0.1002 - acc: 0.972 - ETA: 9s - loss: 0.0984 - acc: 0.971 - ETA: 8s - loss: 0.0945 - acc: 0.972 - ETA: 8s - loss: 0.0945 - acc: 0.972 - ETA: 7s - loss: 0.0929 - acc: 0.974 - ETA: 7s - loss: 0.0938 - acc: 0.974 - ETA: 7s - loss: 0.0925 - acc: 0.974 - ETA: 6s - loss: 0.0912 - acc: 0.975 - ETA: 6s - loss: 0.0889 - acc: 0.976 - ETA: 5s - loss: 0.0875 - acc: 0.977 - ETA: 5s - loss: 0.0865 - acc: 0.976 - ETA: 4s - loss: 0.0856 - acc: 0.977 - ETA: 4s - loss: 0.0847 - acc: 0.978 - ETA: 4s - loss: 0.0842 - acc: 0.978 - ETA: 3s - loss: 0.0852 - acc: 0.977 - ETA: 3s - loss: 0.0834 - acc: 0.978 - ETA: 2s - loss: 0.0823 - acc: 0.978 - ETA: 2s - loss: 0.0812 - acc: 0.978 - ETA: 1s - loss: 0.0795 - acc: 0.978 - ETA: 1s - loss: 0.0795 - acc: 0.978 - ETA: 0s - loss: 0.0787 - acc: 0.978 - ETA: 0s - loss: 0.0789 - acc: 0.977 - ETA: 0s - loss: 0.0796 - acc: 0.977 - 17s 8ms/step - loss: 0.0820 - acc: 0.9769 - val_loss: 0.5042 - val_acc: 0.7865\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.02723\n",
      "Epoch 20/50\n",
      "2247/2247 [==============================] - ETA: 18s - loss: 0.8094 - acc: 0.71 - ETA: 15s - loss: 0.4951 - acc: 0.82 - ETA: 14s - loss: 0.4528 - acc: 0.83 - ETA: 14s - loss: 0.4143 - acc: 0.84 - ETA: 13s - loss: 0.3668 - acc: 0.85 - ETA: 13s - loss: 0.3265 - acc: 0.87 - ETA: 12s - loss: 0.3260 - acc: 0.87 - ETA: 12s - loss: 0.3075 - acc: 0.88 - ETA: 11s - loss: 0.2923 - acc: 0.89 - ETA: 11s - loss: 0.2884 - acc: 0.89 - ETA: 10s - loss: 0.2758 - acc: 0.89 - ETA: 10s - loss: 0.2587 - acc: 0.90 - ETA: 9s - loss: 0.2530 - acc: 0.9075 - ETA: 9s - loss: 0.2386 - acc: 0.914 - ETA: 9s - loss: 0.2255 - acc: 0.919 - ETA: 8s - loss: 0.2156 - acc: 0.923 - ETA: 8s - loss: 0.2088 - acc: 0.925 - ETA: 7s - loss: 0.2052 - acc: 0.926 - ETA: 7s - loss: 0.1999 - acc: 0.928 - ETA: 6s - loss: 0.1958 - acc: 0.931 - ETA: 6s - loss: 0.1914 - acc: 0.933 - ETA: 5s - loss: 0.1917 - acc: 0.932 - ETA: 5s - loss: 0.1873 - acc: 0.934 - ETA: 4s - loss: 0.1873 - acc: 0.935 - ETA: 4s - loss: 0.1866 - acc: 0.934 - ETA: 3s - loss: 0.1836 - acc: 0.935 - ETA: 3s - loss: 0.1803 - acc: 0.936 - ETA: 3s - loss: 0.1791 - acc: 0.937 - ETA: 2s - loss: 0.1775 - acc: 0.938 - ETA: 2s - loss: 0.1741 - acc: 0.939 - ETA: 1s - loss: 0.1712 - acc: 0.940 - ETA: 1s - loss: 0.1681 - acc: 0.941 - ETA: 0s - loss: 0.1652 - acc: 0.943 - ETA: 0s - loss: 0.1616 - acc: 0.944 - ETA: 0s - loss: 0.1583 - acc: 0.946 - 18s 8ms/step - loss: 0.1580 - acc: 0.9466 - val_loss: 0.0313 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.02723\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2247/2247 [==============================] - ETA: 12s - loss: 0.1256 - acc: 0.96 - ETA: 12s - loss: 0.1725 - acc: 0.93 - ETA: 11s - loss: 0.1468 - acc: 0.95 - ETA: 11s - loss: 0.1252 - acc: 0.95 - ETA: 11s - loss: 0.1190 - acc: 0.95 - ETA: 11s - loss: 0.1139 - acc: 0.96 - ETA: 11s - loss: 0.1122 - acc: 0.96 - ETA: 10s - loss: 0.1132 - acc: 0.96 - ETA: 10s - loss: 0.1142 - acc: 0.96 - ETA: 9s - loss: 0.1097 - acc: 0.9656 - ETA: 9s - loss: 0.1034 - acc: 0.968 - ETA: 9s - loss: 0.0988 - acc: 0.970 - ETA: 9s - loss: 0.0957 - acc: 0.972 - ETA: 8s - loss: 0.0920 - acc: 0.974 - ETA: 8s - loss: 0.0916 - acc: 0.975 - ETA: 7s - loss: 0.0934 - acc: 0.974 - ETA: 7s - loss: 0.0936 - acc: 0.974 - ETA: 6s - loss: 0.0937 - acc: 0.974 - ETA: 6s - loss: 0.0914 - acc: 0.975 - ETA: 6s - loss: 0.0924 - acc: 0.975 - ETA: 5s - loss: 0.0903 - acc: 0.976 - ETA: 5s - loss: 0.0884 - acc: 0.977 - ETA: 5s - loss: 0.0879 - acc: 0.977 - ETA: 4s - loss: 0.0869 - acc: 0.977 - ETA: 4s - loss: 0.0843 - acc: 0.978 - ETA: 3s - loss: 0.0845 - acc: 0.978 - ETA: 3s - loss: 0.0829 - acc: 0.979 - ETA: 2s - loss: 0.0812 - acc: 0.979 - ETA: 2s - loss: 0.0811 - acc: 0.979 - ETA: 2s - loss: 0.0817 - acc: 0.979 - ETA: 1s - loss: 0.0811 - acc: 0.978 - ETA: 1s - loss: 0.0802 - acc: 0.979 - ETA: 0s - loss: 0.0791 - acc: 0.979 - ETA: 0s - loss: 0.0800 - acc: 0.979 - ETA: 0s - loss: 0.0800 - acc: 0.979 - 17s 7ms/step - loss: 0.0802 - acc: 0.9800 - val_loss: 0.0568 - val_acc: 0.9786\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.02723\n",
      "Epoch 22/50\n",
      "2247/2247 [==============================] - ETA: 17s - loss: 0.1906 - acc: 0.90 - ETA: 15s - loss: 0.1857 - acc: 0.92 - ETA: 14s - loss: 0.1962 - acc: 0.92 - ETA: 14s - loss: 0.1779 - acc: 0.92 - ETA: 13s - loss: 0.1780 - acc: 0.93 - ETA: 12s - loss: 0.1587 - acc: 0.94 - ETA: 12s - loss: 0.1449 - acc: 0.95 - ETA: 11s - loss: 0.1400 - acc: 0.95 - ETA: 11s - loss: 0.1297 - acc: 0.96 - ETA: 11s - loss: 0.1263 - acc: 0.96 - ETA: 10s - loss: 0.1206 - acc: 0.96 - ETA: 10s - loss: 0.1164 - acc: 0.96 - ETA: 9s - loss: 0.1114 - acc: 0.9663 - ETA: 9s - loss: 0.1143 - acc: 0.964 - ETA: 8s - loss: 0.1105 - acc: 0.964 - ETA: 8s - loss: 0.1071 - acc: 0.966 - ETA: 7s - loss: 0.1056 - acc: 0.968 - ETA: 7s - loss: 0.1034 - acc: 0.969 - ETA: 6s - loss: 0.1051 - acc: 0.970 - ETA: 6s - loss: 0.1047 - acc: 0.970 - ETA: 5s - loss: 0.1026 - acc: 0.971 - ETA: 5s - loss: 0.0994 - acc: 0.973 - ETA: 5s - loss: 0.0973 - acc: 0.974 - ETA: 4s - loss: 0.0964 - acc: 0.974 - ETA: 4s - loss: 0.0945 - acc: 0.974 - ETA: 3s - loss: 0.0941 - acc: 0.974 - ETA: 3s - loss: 0.0937 - acc: 0.975 - ETA: 2s - loss: 0.0925 - acc: 0.975 - ETA: 2s - loss: 0.0903 - acc: 0.976 - ETA: 2s - loss: 0.0891 - acc: 0.976 - ETA: 1s - loss: 0.0877 - acc: 0.976 - ETA: 1s - loss: 0.0884 - acc: 0.976 - ETA: 0s - loss: 0.0870 - acc: 0.976 - ETA: 0s - loss: 0.0868 - acc: 0.976 - ETA: 0s - loss: 0.0866 - acc: 0.976 - 16s 7ms/step - loss: 0.0864 - acc: 0.9764 - val_loss: 0.0181 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.02723 to 0.01814, saving model to weight_MobileNet.hdf5\n",
      "Epoch 23/50\n",
      "2247/2247 [==============================] - ETA: 12s - loss: 0.0901 - acc: 0.96 - ETA: 12s - loss: 0.0934 - acc: 0.97 - ETA: 11s - loss: 0.0904 - acc: 0.97 - ETA: 11s - loss: 0.0836 - acc: 0.97 - ETA: 11s - loss: 0.0762 - acc: 0.97 - ETA: 10s - loss: 0.0687 - acc: 0.98 - ETA: 10s - loss: 0.0645 - acc: 0.98 - ETA: 10s - loss: 0.0651 - acc: 0.98 - ETA: 10s - loss: 0.0685 - acc: 0.98 - ETA: 9s - loss: 0.0670 - acc: 0.9828 - ETA: 9s - loss: 0.0641 - acc: 0.984 - ETA: 8s - loss: 0.0615 - acc: 0.985 - ETA: 8s - loss: 0.0602 - acc: 0.985 - ETA: 8s - loss: 0.0612 - acc: 0.986 - ETA: 7s - loss: 0.0616 - acc: 0.986 - ETA: 7s - loss: 0.0619 - acc: 0.985 - ETA: 6s - loss: 0.0608 - acc: 0.986 - ETA: 6s - loss: 0.0606 - acc: 0.986 - ETA: 6s - loss: 0.0596 - acc: 0.986 - ETA: 5s - loss: 0.0585 - acc: 0.986 - ETA: 5s - loss: 0.0579 - acc: 0.987 - ETA: 5s - loss: 0.0571 - acc: 0.987 - ETA: 4s - loss: 0.0590 - acc: 0.986 - ETA: 4s - loss: 0.0587 - acc: 0.987 - ETA: 3s - loss: 0.0574 - acc: 0.987 - ETA: 3s - loss: 0.0575 - acc: 0.988 - ETA: 3s - loss: 0.0577 - acc: 0.987 - ETA: 2s - loss: 0.0584 - acc: 0.987 - ETA: 2s - loss: 0.0578 - acc: 0.988 - ETA: 1s - loss: 0.0569 - acc: 0.988 - ETA: 1s - loss: 0.0567 - acc: 0.987 - ETA: 1s - loss: 0.0580 - acc: 0.987 - ETA: 0s - loss: 0.0584 - acc: 0.987 - ETA: 0s - loss: 0.0577 - acc: 0.987 - ETA: 0s - loss: 0.0572 - acc: 0.987 - 15s 7ms/step - loss: 0.0570 - acc: 0.9871 - val_loss: 0.0144 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01814 to 0.01444, saving model to weight_MobileNet.hdf5\n",
      "Epoch 24/50\n",
      "2247/2247 [==============================] - ETA: 13s - loss: 0.0578 - acc: 0.98 - ETA: 12s - loss: 0.0376 - acc: 0.99 - ETA: 12s - loss: 0.0371 - acc: 0.99 - ETA: 11s - loss: 0.0401 - acc: 0.99 - ETA: 11s - loss: 0.0441 - acc: 0.99 - ETA: 11s - loss: 0.0397 - acc: 0.99 - ETA: 11s - loss: 0.0401 - acc: 0.99 - ETA: 10s - loss: 0.0406 - acc: 0.99 - ETA: 10s - loss: 0.0416 - acc: 0.99 - ETA: 9s - loss: 0.0492 - acc: 0.9875 - ETA: 9s - loss: 0.0492 - acc: 0.987 - ETA: 8s - loss: 0.0496 - acc: 0.985 - ETA: 8s - loss: 0.0508 - acc: 0.986 - ETA: 8s - loss: 0.0510 - acc: 0.985 - ETA: 7s - loss: 0.0500 - acc: 0.986 - ETA: 7s - loss: 0.0497 - acc: 0.986 - ETA: 6s - loss: 0.0476 - acc: 0.987 - ETA: 6s - loss: 0.0481 - acc: 0.987 - ETA: 6s - loss: 0.0481 - acc: 0.987 - ETA: 6s - loss: 0.0476 - acc: 0.988 - ETA: 5s - loss: 0.0470 - acc: 0.988 - ETA: 5s - loss: 0.0469 - acc: 0.987 - ETA: 5s - loss: 0.0464 - acc: 0.988 - ETA: 4s - loss: 0.0448 - acc: 0.988 - ETA: 4s - loss: 0.0451 - acc: 0.988 - ETA: 3s - loss: 0.0461 - acc: 0.988 - ETA: 3s - loss: 0.0462 - acc: 0.988 - ETA: 2s - loss: 0.0460 - acc: 0.988 - ETA: 2s - loss: 0.0470 - acc: 0.988 - ETA: 2s - loss: 0.0462 - acc: 0.988 - ETA: 1s - loss: 0.0453 - acc: 0.988 - ETA: 1s - loss: 0.0450 - acc: 0.988 - ETA: 0s - loss: 0.0454 - acc: 0.989 - ETA: 0s - loss: 0.0456 - acc: 0.988 - ETA: 0s - loss: 0.0472 - acc: 0.987 - 16s 7ms/step - loss: 0.0480 - acc: 0.9875 - val_loss: 0.1434 - val_acc: 0.9644\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01444\n",
      "Epoch 25/50\n",
      "2247/2247 [==============================] - ETA: 12s - loss: 0.4443 - acc: 0.92 - ETA: 12s - loss: 0.2788 - acc: 0.94 - ETA: 12s - loss: 0.2256 - acc: 0.94 - ETA: 12s - loss: 0.2464 - acc: 0.94 - ETA: 11s - loss: 0.2121 - acc: 0.95 - ETA: 11s - loss: 0.1912 - acc: 0.96 - ETA: 10s - loss: 0.1672 - acc: 0.96 - ETA: 10s - loss: 0.1509 - acc: 0.97 - ETA: 10s - loss: 0.1381 - acc: 0.97 - ETA: 9s - loss: 0.1311 - acc: 0.9734 - ETA: 9s - loss: 0.1262 - acc: 0.973 - ETA: 8s - loss: 0.1212 - acc: 0.974 - ETA: 8s - loss: 0.1161 - acc: 0.976 - ETA: 8s - loss: 0.1107 - acc: 0.977 - ETA: 7s - loss: 0.1069 - acc: 0.977 - ETA: 7s - loss: 0.1069 - acc: 0.975 - ETA: 7s - loss: 0.1032 - acc: 0.976 - ETA: 6s - loss: 0.0994 - acc: 0.976 - ETA: 6s - loss: 0.0975 - acc: 0.977 - ETA: 5s - loss: 0.0944 - acc: 0.978 - ETA: 5s - loss: 0.0915 - acc: 0.979 - ETA: 5s - loss: 0.0890 - acc: 0.980 - ETA: 4s - loss: 0.0885 - acc: 0.979 - ETA: 4s - loss: 0.0881 - acc: 0.979 - ETA: 4s - loss: 0.0860 - acc: 0.980 - ETA: 3s - loss: 0.0848 - acc: 0.980 - ETA: 3s - loss: 0.0826 - acc: 0.981 - ETA: 2s - loss: 0.0816 - acc: 0.981 - ETA: 2s - loss: 0.0809 - acc: 0.980 - ETA: 2s - loss: 0.0790 - acc: 0.981 - ETA: 1s - loss: 0.0789 - acc: 0.980 - ETA: 1s - loss: 0.0794 - acc: 0.980 - ETA: 0s - loss: 0.0781 - acc: 0.980 - ETA: 0s - loss: 0.0766 - acc: 0.980 - ETA: 0s - loss: 0.0757 - acc: 0.980 - 15s 7ms/step - loss: 0.0755 - acc: 0.9804 - val_loss: 0.0159 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.01444\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2247/2247 [==============================] - ETA: 14s - loss: 0.0287 - acc: 1.00 - ETA: 13s - loss: 0.0437 - acc: 0.99 - ETA: 12s - loss: 0.0359 - acc: 0.99 - ETA: 12s - loss: 0.0308 - acc: 0.99 - ETA: 11s - loss: 0.0291 - acc: 0.99 - ETA: 11s - loss: 0.0289 - acc: 0.99 - ETA: 10s - loss: 0.0343 - acc: 0.99 - ETA: 10s - loss: 0.0345 - acc: 0.99 - ETA: 10s - loss: 0.0349 - acc: 0.99 - ETA: 9s - loss: 0.0337 - acc: 0.9969 - ETA: 9s - loss: 0.0325 - acc: 0.997 - ETA: 9s - loss: 0.0341 - acc: 0.997 - ETA: 8s - loss: 0.0330 - acc: 0.997 - ETA: 8s - loss: 0.0320 - acc: 0.997 - ETA: 8s - loss: 0.0313 - acc: 0.997 - ETA: 7s - loss: 0.0335 - acc: 0.997 - ETA: 7s - loss: 0.0331 - acc: 0.997 - ETA: 6s - loss: 0.0343 - acc: 0.996 - ETA: 6s - loss: 0.0347 - acc: 0.995 - ETA: 5s - loss: 0.0350 - acc: 0.996 - ETA: 5s - loss: 0.0367 - acc: 0.993 - ETA: 5s - loss: 0.0378 - acc: 0.992 - ETA: 4s - loss: 0.0394 - acc: 0.991 - ETA: 4s - loss: 0.0403 - acc: 0.991 - ETA: 4s - loss: 0.0408 - acc: 0.991 - ETA: 3s - loss: 0.0403 - acc: 0.991 - ETA: 3s - loss: 0.0397 - acc: 0.991 - ETA: 2s - loss: 0.0393 - acc: 0.991 - ETA: 2s - loss: 0.0391 - acc: 0.991 - ETA: 2s - loss: 0.0392 - acc: 0.992 - ETA: 1s - loss: 0.0398 - acc: 0.991 - ETA: 1s - loss: 0.0406 - acc: 0.991 - ETA: 0s - loss: 0.0411 - acc: 0.991 - ETA: 0s - loss: 0.0408 - acc: 0.990 - ETA: 0s - loss: 0.0405 - acc: 0.991 - 15s 7ms/step - loss: 0.0404 - acc: 0.9911 - val_loss: 0.0091 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01444 to 0.00909, saving model to weight_MobileNet.hdf5\n",
      "Epoch 27/50\n",
      "2247/2247 [==============================] - ETA: 11s - loss: 0.0507 - acc: 1.00 - ETA: 12s - loss: 0.0387 - acc: 1.00 - ETA: 11s - loss: 0.0389 - acc: 0.99 - ETA: 11s - loss: 0.0423 - acc: 0.99 - ETA: 11s - loss: 0.0415 - acc: 0.99 - ETA: 10s - loss: 0.0426 - acc: 0.98 - ETA: 10s - loss: 0.0421 - acc: 0.98 - ETA: 10s - loss: 0.0414 - acc: 0.99 - ETA: 9s - loss: 0.0418 - acc: 0.9896 - ETA: 9s - loss: 0.0412 - acc: 0.990 - ETA: 8s - loss: 0.0386 - acc: 0.991 - ETA: 8s - loss: 0.0380 - acc: 0.992 - ETA: 8s - loss: 0.0376 - acc: 0.992 - ETA: 7s - loss: 0.0379 - acc: 0.993 - ETA: 7s - loss: 0.0376 - acc: 0.992 - ETA: 7s - loss: 0.0369 - acc: 0.993 - ETA: 6s - loss: 0.0363 - acc: 0.993 - ETA: 6s - loss: 0.0361 - acc: 0.993 - ETA: 6s - loss: 0.0361 - acc: 0.993 - ETA: 5s - loss: 0.0367 - acc: 0.993 - ETA: 5s - loss: 0.0369 - acc: 0.993 - ETA: 4s - loss: 0.0362 - acc: 0.993 - ETA: 4s - loss: 0.0357 - acc: 0.993 - ETA: 4s - loss: 0.0349 - acc: 0.994 - ETA: 3s - loss: 0.0351 - acc: 0.994 - ETA: 3s - loss: 0.0355 - acc: 0.994 - ETA: 3s - loss: 0.0356 - acc: 0.994 - ETA: 2s - loss: 0.0349 - acc: 0.995 - ETA: 2s - loss: 0.0359 - acc: 0.994 - ETA: 1s - loss: 0.0369 - acc: 0.994 - ETA: 1s - loss: 0.0364 - acc: 0.994 - ETA: 1s - loss: 0.0361 - acc: 0.994 - ETA: 0s - loss: 0.0354 - acc: 0.994 - ETA: 0s - loss: 0.0362 - acc: 0.994 - ETA: 0s - loss: 0.0362 - acc: 0.994 - 14s 6ms/step - loss: 0.0361 - acc: 0.9942 - val_loss: 0.0099 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00909\n",
      "Epoch 28/50\n",
      "2247/2247 [==============================] - ETA: 11s - loss: 0.0242 - acc: 1.00 - ETA: 12s - loss: 0.0553 - acc: 0.98 - ETA: 11s - loss: 0.0447 - acc: 0.98 - ETA: 11s - loss: 0.0428 - acc: 0.99 - ETA: 11s - loss: 0.0390 - acc: 0.99 - ETA: 11s - loss: 0.0370 - acc: 0.99 - ETA: 10s - loss: 0.0402 - acc: 0.99 - ETA: 10s - loss: 0.0434 - acc: 0.99 - ETA: 9s - loss: 0.0414 - acc: 0.9931 - ETA: 9s - loss: 0.0401 - acc: 0.993 - ETA: 9s - loss: 0.0396 - acc: 0.992 - ETA: 8s - loss: 0.0389 - acc: 0.993 - ETA: 8s - loss: 0.0375 - acc: 0.994 - ETA: 8s - loss: 0.0368 - acc: 0.993 - ETA: 7s - loss: 0.0385 - acc: 0.992 - ETA: 7s - loss: 0.0376 - acc: 0.993 - ETA: 6s - loss: 0.0401 - acc: 0.992 - ETA: 6s - loss: 0.0416 - acc: 0.992 - ETA: 6s - loss: 0.0398 - acc: 0.992 - ETA: 6s - loss: 0.0400 - acc: 0.992 - ETA: 5s - loss: 0.0399 - acc: 0.991 - ETA: 5s - loss: 0.0394 - acc: 0.992 - ETA: 4s - loss: 0.0388 - acc: 0.992 - ETA: 4s - loss: 0.0384 - acc: 0.992 - ETA: 4s - loss: 0.0385 - acc: 0.993 - ETA: 3s - loss: 0.0389 - acc: 0.992 - ETA: 3s - loss: 0.0383 - acc: 0.993 - ETA: 2s - loss: 0.0378 - acc: 0.992 - ETA: 2s - loss: 0.0367 - acc: 0.993 - ETA: 2s - loss: 0.0365 - acc: 0.993 - ETA: 1s - loss: 0.0358 - acc: 0.993 - ETA: 1s - loss: 0.0359 - acc: 0.992 - ETA: 0s - loss: 0.0354 - acc: 0.992 - ETA: 0s - loss: 0.0349 - acc: 0.993 - ETA: 0s - loss: 0.0345 - acc: 0.993 - 15s 7ms/step - loss: 0.0344 - acc: 0.9933 - val_loss: 0.0064 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00909 to 0.00638, saving model to weight_MobileNet.hdf5\n",
      "Epoch 29/50\n",
      "2247/2247 [==============================] - ETA: 12s - loss: 0.0327 - acc: 0.98 - ETA: 13s - loss: 0.0227 - acc: 0.99 - ETA: 13s - loss: 0.0205 - acc: 0.99 - ETA: 12s - loss: 0.0195 - acc: 0.99 - ETA: 12s - loss: 0.0176 - acc: 0.99 - ETA: 12s - loss: 0.0162 - acc: 0.99 - ETA: 11s - loss: 0.0171 - acc: 0.99 - ETA: 11s - loss: 0.0190 - acc: 0.99 - ETA: 10s - loss: 0.0184 - acc: 0.99 - ETA: 10s - loss: 0.0204 - acc: 0.99 - ETA: 10s - loss: 0.0192 - acc: 0.99 - ETA: 9s - loss: 0.0229 - acc: 0.9948 - ETA: 9s - loss: 0.0221 - acc: 0.995 - ETA: 8s - loss: 0.0228 - acc: 0.994 - ETA: 8s - loss: 0.0231 - acc: 0.994 - ETA: 8s - loss: 0.0252 - acc: 0.993 - ETA: 7s - loss: 0.0248 - acc: 0.993 - ETA: 7s - loss: 0.0240 - acc: 0.993 - ETA: 6s - loss: 0.0241 - acc: 0.994 - ETA: 6s - loss: 0.0236 - acc: 0.994 - ETA: 5s - loss: 0.0239 - acc: 0.994 - ETA: 5s - loss: 0.0238 - acc: 0.995 - ETA: 5s - loss: 0.0250 - acc: 0.994 - ETA: 4s - loss: 0.0246 - acc: 0.994 - ETA: 4s - loss: 0.0256 - acc: 0.993 - ETA: 3s - loss: 0.0250 - acc: 0.994 - ETA: 3s - loss: 0.0254 - acc: 0.994 - ETA: 2s - loss: 0.0252 - acc: 0.994 - ETA: 2s - loss: 0.0249 - acc: 0.994 - ETA: 2s - loss: 0.0246 - acc: 0.994 - ETA: 1s - loss: 0.0249 - acc: 0.994 - ETA: 1s - loss: 0.0255 - acc: 0.994 - ETA: 0s - loss: 0.0253 - acc: 0.994 - ETA: 0s - loss: 0.0293 - acc: 0.994 - ETA: 0s - loss: 0.0295 - acc: 0.994 - 16s 7ms/step - loss: 0.0295 - acc: 0.9942 - val_loss: 0.0061 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00638 to 0.00613, saving model to weight_MobileNet.hdf5\n",
      "Epoch 30/50\n",
      "2247/2247 [==============================] - ETA: 12s - loss: 0.0334 - acc: 0.98 - ETA: 12s - loss: 0.0273 - acc: 0.99 - ETA: 12s - loss: 0.0227 - acc: 0.99 - ETA: 12s - loss: 0.0208 - acc: 0.99 - ETA: 12s - loss: 0.0190 - acc: 0.99 - ETA: 11s - loss: 0.0228 - acc: 0.99 - ETA: 11s - loss: 0.0258 - acc: 0.99 - ETA: 11s - loss: 0.0238 - acc: 0.99 - ETA: 10s - loss: 0.0232 - acc: 0.99 - ETA: 10s - loss: 0.0237 - acc: 0.99 - ETA: 10s - loss: 0.0247 - acc: 0.99 - ETA: 9s - loss: 0.0251 - acc: 0.9935 - ETA: 9s - loss: 0.0248 - acc: 0.994 - ETA: 8s - loss: 0.0263 - acc: 0.992 - ETA: 8s - loss: 0.0280 - acc: 0.991 - ETA: 8s - loss: 0.0273 - acc: 0.991 - ETA: 7s - loss: 0.0274 - acc: 0.991 - ETA: 7s - loss: 0.0266 - acc: 0.992 - ETA: 6s - loss: 0.0264 - acc: 0.992 - ETA: 6s - loss: 0.0261 - acc: 0.993 - ETA: 5s - loss: 0.0260 - acc: 0.993 - ETA: 5s - loss: 0.0277 - acc: 0.992 - ETA: 5s - loss: 0.0277 - acc: 0.992 - ETA: 4s - loss: 0.0279 - acc: 0.992 - ETA: 4s - loss: 0.0276 - acc: 0.992 - ETA: 3s - loss: 0.0278 - acc: 0.992 - ETA: 3s - loss: 0.0272 - acc: 0.993 - ETA: 3s - loss: 0.0288 - acc: 0.992 - ETA: 2s - loss: 0.0285 - acc: 0.993 - ETA: 2s - loss: 0.0279 - acc: 0.993 - ETA: 1s - loss: 0.0277 - acc: 0.993 - ETA: 1s - loss: 0.0279 - acc: 0.993 - ETA: 0s - loss: 0.0281 - acc: 0.992 - ETA: 0s - loss: 0.0288 - acc: 0.992 - ETA: 0s - loss: 0.0282 - acc: 0.992 - 17s 7ms/step - loss: 0.0282 - acc: 0.9929 - val_loss: 0.0053 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00613 to 0.00531, saving model to weight_MobileNet.hdf5\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2247/2247 [==============================] - ETA: 17s - loss: 0.0265 - acc: 1.00 - ETA: 15s - loss: 0.0231 - acc: 1.00 - ETA: 15s - loss: 0.0274 - acc: 0.99 - ETA: 14s - loss: 0.0278 - acc: 0.99 - ETA: 13s - loss: 0.0273 - acc: 0.99 - ETA: 13s - loss: 0.0268 - acc: 0.99 - ETA: 12s - loss: 0.0271 - acc: 0.99 - ETA: 12s - loss: 0.0280 - acc: 0.99 - ETA: 12s - loss: 0.0286 - acc: 0.99 - ETA: 11s - loss: 0.0278 - acc: 0.99 - ETA: 11s - loss: 0.0261 - acc: 0.99 - ETA: 10s - loss: 0.0250 - acc: 0.99 - ETA: 10s - loss: 0.0243 - acc: 0.99 - ETA: 9s - loss: 0.0239 - acc: 0.9955 - ETA: 9s - loss: 0.0245 - acc: 0.994 - ETA: 8s - loss: 0.0245 - acc: 0.995 - ETA: 8s - loss: 0.0250 - acc: 0.995 - ETA: 7s - loss: 0.0249 - acc: 0.995 - ETA: 7s - loss: 0.0250 - acc: 0.995 - ETA: 6s - loss: 0.0244 - acc: 0.995 - ETA: 6s - loss: 0.0253 - acc: 0.994 - ETA: 5s - loss: 0.0256 - acc: 0.995 - ETA: 5s - loss: 0.0259 - acc: 0.995 - ETA: 4s - loss: 0.0253 - acc: 0.995 - ETA: 4s - loss: 0.0250 - acc: 0.995 - ETA: 3s - loss: 0.0254 - acc: 0.995 - ETA: 3s - loss: 0.0258 - acc: 0.995 - ETA: 3s - loss: 0.0254 - acc: 0.995 - ETA: 2s - loss: 0.0261 - acc: 0.995 - ETA: 2s - loss: 0.0256 - acc: 0.995 - ETA: 1s - loss: 0.0251 - acc: 0.995 - ETA: 1s - loss: 0.0245 - acc: 0.995 - ETA: 0s - loss: 0.0239 - acc: 0.995 - ETA: 0s - loss: 0.0239 - acc: 0.995 - ETA: 0s - loss: 0.0233 - acc: 0.995 - 16s 7ms/step - loss: 0.0232 - acc: 0.9955 - val_loss: 0.0047 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00531 to 0.00473, saving model to weight_MobileNet.hdf5\n",
      "Epoch 32/50\n",
      "2247/2247 [==============================] - ETA: 13s - loss: 0.0889 - acc: 0.96 - ETA: 12s - loss: 0.0797 - acc: 0.96 - ETA: 12s - loss: 0.0612 - acc: 0.97 - ETA: 11s - loss: 0.0492 - acc: 0.98 - ETA: 11s - loss: 0.0427 - acc: 0.98 - ETA: 11s - loss: 0.0452 - acc: 0.98 - ETA: 10s - loss: 0.0403 - acc: 0.98 - ETA: 10s - loss: 0.0373 - acc: 0.98 - ETA: 9s - loss: 0.0343 - acc: 0.9896 - ETA: 9s - loss: 0.0319 - acc: 0.990 - ETA: 9s - loss: 0.0296 - acc: 0.991 - ETA: 8s - loss: 0.0288 - acc: 0.992 - ETA: 8s - loss: 0.0282 - acc: 0.992 - ETA: 7s - loss: 0.0278 - acc: 0.993 - ETA: 7s - loss: 0.0272 - acc: 0.993 - ETA: 7s - loss: 0.0261 - acc: 0.994 - ETA: 6s - loss: 0.0251 - acc: 0.994 - ETA: 6s - loss: 0.0247 - acc: 0.994 - ETA: 6s - loss: 0.0240 - acc: 0.995 - ETA: 5s - loss: 0.0236 - acc: 0.995 - ETA: 5s - loss: 0.0229 - acc: 0.995 - ETA: 4s - loss: 0.0229 - acc: 0.995 - ETA: 4s - loss: 0.0234 - acc: 0.995 - ETA: 4s - loss: 0.0232 - acc: 0.995 - ETA: 3s - loss: 0.0230 - acc: 0.995 - ETA: 3s - loss: 0.0229 - acc: 0.995 - ETA: 3s - loss: 0.0228 - acc: 0.995 - ETA: 2s - loss: 0.0223 - acc: 0.996 - ETA: 2s - loss: 0.0226 - acc: 0.996 - ETA: 2s - loss: 0.0221 - acc: 0.996 - ETA: 1s - loss: 0.0223 - acc: 0.996 - ETA: 1s - loss: 0.0218 - acc: 0.996 - ETA: 0s - loss: 0.0213 - acc: 0.996 - ETA: 0s - loss: 0.0214 - acc: 0.996 - ETA: 0s - loss: 0.0211 - acc: 0.996 - 16s 7ms/step - loss: 0.0211 - acc: 0.9964 - val_loss: 0.0056 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00473\n",
      "Epoch 33/50\n",
      "2247/2247 [==============================] - ETA: 12s - loss: 0.0071 - acc: 1.00 - ETA: 12s - loss: 0.0108 - acc: 1.00 - ETA: 12s - loss: 0.0119 - acc: 1.00 - ETA: 11s - loss: 0.0180 - acc: 0.99 - ETA: 11s - loss: 0.0160 - acc: 0.99 - ETA: 11s - loss: 0.0161 - acc: 0.99 - ETA: 10s - loss: 0.0157 - acc: 0.99 - ETA: 10s - loss: 0.0158 - acc: 0.99 - ETA: 10s - loss: 0.0153 - acc: 0.99 - ETA: 9s - loss: 0.0147 - acc: 0.9984 - ETA: 9s - loss: 0.0147 - acc: 0.998 - ETA: 8s - loss: 0.0167 - acc: 0.997 - ETA: 8s - loss: 0.0178 - acc: 0.996 - ETA: 8s - loss: 0.0185 - acc: 0.996 - ETA: 7s - loss: 0.0204 - acc: 0.995 - ETA: 7s - loss: 0.0205 - acc: 0.995 - ETA: 6s - loss: 0.0204 - acc: 0.995 - ETA: 6s - loss: 0.0204 - acc: 0.995 - ETA: 6s - loss: 0.0203 - acc: 0.995 - ETA: 5s - loss: 0.0199 - acc: 0.996 - ETA: 5s - loss: 0.0210 - acc: 0.994 - ETA: 5s - loss: 0.0212 - acc: 0.994 - ETA: 4s - loss: 0.0213 - acc: 0.994 - ETA: 4s - loss: 0.0211 - acc: 0.994 - ETA: 3s - loss: 0.0212 - acc: 0.995 - ETA: 3s - loss: 0.0208 - acc: 0.995 - ETA: 3s - loss: 0.0207 - acc: 0.995 - ETA: 2s - loss: 0.0208 - acc: 0.995 - ETA: 2s - loss: 0.0214 - acc: 0.994 - ETA: 1s - loss: 0.0217 - acc: 0.994 - ETA: 1s - loss: 0.0238 - acc: 0.994 - ETA: 1s - loss: 0.0235 - acc: 0.994 - ETA: 0s - loss: 0.0231 - acc: 0.994 - ETA: 0s - loss: 0.0228 - acc: 0.994 - ETA: 0s - loss: 0.0224 - acc: 0.994 - 15s 7ms/step - loss: 0.0224 - acc: 0.9947 - val_loss: 0.0049 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00473\n",
      "Epoch 34/50\n",
      "2247/2247 [==============================] - ETA: 12s - loss: 0.0080 - acc: 1.00 - ETA: 11s - loss: 0.0152 - acc: 1.00 - ETA: 11s - loss: 0.0158 - acc: 1.00 - ETA: 11s - loss: 0.0201 - acc: 0.99 - ETA: 11s - loss: 0.0186 - acc: 0.99 - ETA: 11s - loss: 0.0174 - acc: 0.99 - ETA: 10s - loss: 0.0175 - acc: 0.99 - ETA: 10s - loss: 0.0176 - acc: 0.99 - ETA: 9s - loss: 0.0165 - acc: 0.9983 - ETA: 9s - loss: 0.0192 - acc: 0.996 - ETA: 9s - loss: 0.0202 - acc: 0.997 - ETA: 8s - loss: 0.0193 - acc: 0.997 - ETA: 8s - loss: 0.0207 - acc: 0.996 - ETA: 8s - loss: 0.0204 - acc: 0.996 - ETA: 7s - loss: 0.0213 - acc: 0.995 - ETA: 7s - loss: 0.0207 - acc: 0.996 - ETA: 6s - loss: 0.0205 - acc: 0.995 - ETA: 6s - loss: 0.0200 - acc: 0.995 - ETA: 6s - loss: 0.0201 - acc: 0.995 - ETA: 5s - loss: 0.0222 - acc: 0.994 - ETA: 5s - loss: 0.0227 - acc: 0.994 - ETA: 5s - loss: 0.0224 - acc: 0.995 - ETA: 4s - loss: 0.0233 - acc: 0.994 - ETA: 4s - loss: 0.0242 - acc: 0.994 - ETA: 3s - loss: 0.0270 - acc: 0.992 - ETA: 3s - loss: 0.0265 - acc: 0.992 - ETA: 3s - loss: 0.0269 - acc: 0.992 - ETA: 2s - loss: 0.0263 - acc: 0.992 - ETA: 2s - loss: 0.0261 - acc: 0.993 - ETA: 1s - loss: 0.0256 - acc: 0.993 - ETA: 1s - loss: 0.0255 - acc: 0.993 - ETA: 1s - loss: 0.0254 - acc: 0.993 - ETA: 0s - loss: 0.0254 - acc: 0.993 - ETA: 0s - loss: 0.0249 - acc: 0.994 - ETA: 0s - loss: 0.0244 - acc: 0.994 - 15s 7ms/step - loss: 0.0243 - acc: 0.9942 - val_loss: 0.0055 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00473\n",
      "Epoch 35/50\n",
      "2247/2247 [==============================] - ETA: 12s - loss: 0.0102 - acc: 1.00 - ETA: 12s - loss: 0.0098 - acc: 1.00 - ETA: 12s - loss: 0.0083 - acc: 1.00 - ETA: 11s - loss: 0.0093 - acc: 1.00 - ETA: 11s - loss: 0.0097 - acc: 1.00 - ETA: 11s - loss: 0.0108 - acc: 1.00 - ETA: 10s - loss: 0.0117 - acc: 1.00 - ETA: 10s - loss: 0.0126 - acc: 1.00 - ETA: 10s - loss: 0.0135 - acc: 1.00 - ETA: 9s - loss: 0.0130 - acc: 1.0000 - ETA: 9s - loss: 0.0127 - acc: 1.000 - ETA: 8s - loss: 0.0127 - acc: 1.000 - ETA: 8s - loss: 0.0129 - acc: 1.000 - ETA: 8s - loss: 0.0141 - acc: 0.998 - ETA: 7s - loss: 0.0137 - acc: 0.999 - ETA: 7s - loss: 0.0146 - acc: 0.999 - ETA: 6s - loss: 0.0148 - acc: 0.999 - ETA: 6s - loss: 0.0145 - acc: 0.999 - ETA: 6s - loss: 0.0144 - acc: 0.999 - ETA: 5s - loss: 0.0140 - acc: 0.999 - ETA: 5s - loss: 0.0142 - acc: 0.998 - ETA: 4s - loss: 0.0141 - acc: 0.998 - ETA: 4s - loss: 0.0139 - acc: 0.998 - ETA: 4s - loss: 0.0141 - acc: 0.998 - ETA: 3s - loss: 0.0148 - acc: 0.998 - ETA: 3s - loss: 0.0143 - acc: 0.998 - ETA: 3s - loss: 0.0142 - acc: 0.998 - ETA: 2s - loss: 0.0141 - acc: 0.998 - ETA: 2s - loss: 0.0139 - acc: 0.998 - ETA: 1s - loss: 0.0142 - acc: 0.998 - ETA: 1s - loss: 0.0143 - acc: 0.998 - ETA: 1s - loss: 0.0152 - acc: 0.998 - ETA: 0s - loss: 0.0149 - acc: 0.998 - ETA: 0s - loss: 0.0152 - acc: 0.997 - ETA: 0s - loss: 0.0157 - acc: 0.997 - 15s 7ms/step - loss: 0.0156 - acc: 0.9973 - val_loss: 0.0050 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00473\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2247/2247 [==============================] - ETA: 13s - loss: 0.0254 - acc: 0.98 - ETA: 13s - loss: 0.0215 - acc: 0.99 - ETA: 13s - loss: 0.0173 - acc: 0.99 - ETA: 12s - loss: 0.0155 - acc: 0.99 - ETA: 12s - loss: 0.0175 - acc: 0.99 - ETA: 11s - loss: 0.0160 - acc: 0.99 - ETA: 11s - loss: 0.0159 - acc: 0.99 - ETA: 10s - loss: 0.0180 - acc: 0.99 - ETA: 10s - loss: 0.0179 - acc: 0.99 - ETA: 9s - loss: 0.0169 - acc: 0.9969 - ETA: 9s - loss: 0.0169 - acc: 0.997 - ETA: 9s - loss: 0.0175 - acc: 0.997 - ETA: 8s - loss: 0.0183 - acc: 0.997 - ETA: 8s - loss: 0.0183 - acc: 0.997 - ETA: 7s - loss: 0.0176 - acc: 0.997 - ETA: 7s - loss: 0.0190 - acc: 0.997 - ETA: 7s - loss: 0.0189 - acc: 0.997 - ETA: 6s - loss: 0.0187 - acc: 0.997 - ETA: 6s - loss: 0.0182 - acc: 0.997 - ETA: 5s - loss: 0.0185 - acc: 0.997 - ETA: 5s - loss: 0.0184 - acc: 0.997 - ETA: 5s - loss: 0.0178 - acc: 0.997 - ETA: 4s - loss: 0.0179 - acc: 0.998 - ETA: 4s - loss: 0.0182 - acc: 0.998 - ETA: 3s - loss: 0.0178 - acc: 0.998 - ETA: 3s - loss: 0.0175 - acc: 0.998 - ETA: 3s - loss: 0.0178 - acc: 0.998 - ETA: 2s - loss: 0.0174 - acc: 0.998 - ETA: 2s - loss: 0.0170 - acc: 0.998 - ETA: 2s - loss: 0.0168 - acc: 0.998 - ETA: 1s - loss: 0.0165 - acc: 0.998 - ETA: 1s - loss: 0.0173 - acc: 0.998 - ETA: 0s - loss: 0.0169 - acc: 0.998 - ETA: 0s - loss: 0.0166 - acc: 0.998 - ETA: 0s - loss: 0.0166 - acc: 0.998 - 15s 7ms/step - loss: 0.0165 - acc: 0.9982 - val_loss: 0.0053 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00473\n",
      "Epoch 37/50\n",
      "2247/2247 [==============================] - ETA: 12s - loss: 0.0146 - acc: 1.00 - ETA: 12s - loss: 0.0109 - acc: 1.00 - ETA: 12s - loss: 0.0097 - acc: 1.00 - ETA: 11s - loss: 0.0097 - acc: 1.00 - ETA: 11s - loss: 0.0082 - acc: 1.00 - ETA: 11s - loss: 0.0090 - acc: 1.00 - ETA: 10s - loss: 0.0134 - acc: 0.99 - ETA: 10s - loss: 0.0128 - acc: 0.99 - ETA: 10s - loss: 0.0145 - acc: 0.99 - ETA: 9s - loss: 0.0138 - acc: 0.9984 - ETA: 9s - loss: 0.0129 - acc: 0.998 - ETA: 8s - loss: 0.0124 - acc: 0.998 - ETA: 8s - loss: 0.0123 - acc: 0.998 - ETA: 8s - loss: 0.0123 - acc: 0.998 - ETA: 7s - loss: 0.0122 - acc: 0.999 - ETA: 7s - loss: 0.0128 - acc: 0.999 - ETA: 6s - loss: 0.0126 - acc: 0.999 - ETA: 6s - loss: 0.0124 - acc: 0.999 - ETA: 6s - loss: 0.0122 - acc: 0.999 - ETA: 5s - loss: 0.0118 - acc: 0.999 - ETA: 5s - loss: 0.0123 - acc: 0.998 - ETA: 5s - loss: 0.0125 - acc: 0.998 - ETA: 4s - loss: 0.0124 - acc: 0.998 - ETA: 4s - loss: 0.0125 - acc: 0.998 - ETA: 3s - loss: 0.0134 - acc: 0.998 - ETA: 3s - loss: 0.0132 - acc: 0.998 - ETA: 3s - loss: 0.0132 - acc: 0.998 - ETA: 2s - loss: 0.0129 - acc: 0.998 - ETA: 2s - loss: 0.0136 - acc: 0.997 - ETA: 1s - loss: 0.0140 - acc: 0.997 - ETA: 1s - loss: 0.0145 - acc: 0.997 - ETA: 1s - loss: 0.0144 - acc: 0.997 - ETA: 0s - loss: 0.0142 - acc: 0.997 - ETA: 0s - loss: 0.0140 - acc: 0.997 - ETA: 0s - loss: 0.0144 - acc: 0.997 - 15s 7ms/step - loss: 0.0143 - acc: 0.9978 - val_loss: 0.0074 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00473\n",
      "Epoch 38/50\n",
      "2247/2247 [==============================] - ETA: 13s - loss: 0.0124 - acc: 1.00 - ETA: 12s - loss: 0.0129 - acc: 1.00 - ETA: 12s - loss: 0.0266 - acc: 0.99 - ETA: 12s - loss: 0.0209 - acc: 0.99 - ETA: 11s - loss: 0.0200 - acc: 0.99 - ETA: 11s - loss: 0.0174 - acc: 0.99 - ETA: 10s - loss: 0.0170 - acc: 0.99 - ETA: 10s - loss: 0.0163 - acc: 0.99 - ETA: 9s - loss: 0.0153 - acc: 0.9983 - ETA: 9s - loss: 0.0143 - acc: 0.998 - ETA: 9s - loss: 0.0144 - acc: 0.998 - ETA: 8s - loss: 0.0136 - acc: 0.998 - ETA: 8s - loss: 0.0126 - acc: 0.998 - ETA: 8s - loss: 0.0126 - acc: 0.998 - ETA: 7s - loss: 0.0133 - acc: 0.999 - ETA: 7s - loss: 0.0139 - acc: 0.999 - ETA: 6s - loss: 0.0131 - acc: 0.999 - ETA: 6s - loss: 0.0146 - acc: 0.998 - ETA: 6s - loss: 0.0147 - acc: 0.998 - ETA: 5s - loss: 0.0142 - acc: 0.998 - ETA: 5s - loss: 0.0160 - acc: 0.997 - ETA: 4s - loss: 0.0162 - acc: 0.997 - ETA: 4s - loss: 0.0157 - acc: 0.997 - ETA: 4s - loss: 0.0164 - acc: 0.996 - ETA: 3s - loss: 0.0165 - acc: 0.996 - ETA: 3s - loss: 0.0161 - acc: 0.997 - ETA: 3s - loss: 0.0159 - acc: 0.997 - ETA: 2s - loss: 0.0160 - acc: 0.997 - ETA: 2s - loss: 0.0170 - acc: 0.996 - ETA: 1s - loss: 0.0166 - acc: 0.996 - ETA: 1s - loss: 0.0162 - acc: 0.996 - ETA: 1s - loss: 0.0161 - acc: 0.996 - ETA: 0s - loss: 0.0160 - acc: 0.996 - ETA: 0s - loss: 0.0157 - acc: 0.996 - ETA: 0s - loss: 0.0156 - acc: 0.996 - 15s 7ms/step - loss: 0.0156 - acc: 0.9969 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00473 to 0.00160, saving model to weight_MobileNet.hdf5\n",
      "Epoch 39/50\n",
      "2247/2247 [==============================] - ETA: 13s - loss: 0.0083 - acc: 1.00 - ETA: 12s - loss: 0.0072 - acc: 1.00 - ETA: 12s - loss: 0.0141 - acc: 0.99 - ETA: 11s - loss: 0.0157 - acc: 0.99 - ETA: 11s - loss: 0.0135 - acc: 0.99 - ETA: 11s - loss: 0.0118 - acc: 0.99 - ETA: 11s - loss: 0.0114 - acc: 0.99 - ETA: 10s - loss: 0.0122 - acc: 0.99 - ETA: 10s - loss: 0.0119 - acc: 0.99 - ETA: 10s - loss: 0.0114 - acc: 0.99 - ETA: 9s - loss: 0.0106 - acc: 0.9986 - ETA: 9s - loss: 0.0106 - acc: 0.998 - ETA: 8s - loss: 0.0102 - acc: 0.998 - ETA: 8s - loss: 0.0141 - acc: 0.997 - ETA: 7s - loss: 0.0135 - acc: 0.997 - ETA: 7s - loss: 0.0131 - acc: 0.998 - ETA: 7s - loss: 0.0128 - acc: 0.998 - ETA: 6s - loss: 0.0128 - acc: 0.998 - ETA: 6s - loss: 0.0125 - acc: 0.998 - ETA: 5s - loss: 0.0123 - acc: 0.998 - ETA: 5s - loss: 0.0125 - acc: 0.998 - ETA: 5s - loss: 0.0124 - acc: 0.998 - ETA: 4s - loss: 0.0120 - acc: 0.998 - ETA: 4s - loss: 0.0120 - acc: 0.998 - ETA: 3s - loss: 0.0116 - acc: 0.998 - ETA: 3s - loss: 0.0120 - acc: 0.998 - ETA: 3s - loss: 0.0119 - acc: 0.998 - ETA: 2s - loss: 0.0117 - acc: 0.998 - ETA: 2s - loss: 0.0123 - acc: 0.997 - ETA: 2s - loss: 0.0121 - acc: 0.997 - ETA: 1s - loss: 0.0118 - acc: 0.998 - ETA: 1s - loss: 0.0119 - acc: 0.998 - ETA: 0s - loss: 0.0119 - acc: 0.998 - ETA: 0s - loss: 0.0118 - acc: 0.998 - ETA: 0s - loss: 0.0118 - acc: 0.998 - 16s 7ms/step - loss: 0.0117 - acc: 0.9982 - val_loss: 0.0031 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00160\n",
      "Epoch 40/50\n",
      "2247/2247 [==============================] - ETA: 14s - loss: 0.0031 - acc: 1.00 - ETA: 13s - loss: 0.0026 - acc: 1.00 - ETA: 12s - loss: 0.0043 - acc: 1.00 - ETA: 12s - loss: 0.0084 - acc: 1.00 - ETA: 11s - loss: 0.0090 - acc: 1.00 - ETA: 11s - loss: 0.0087 - acc: 1.00 - ETA: 11s - loss: 0.0108 - acc: 1.00 - ETA: 10s - loss: 0.0140 - acc: 0.99 - ETA: 10s - loss: 0.0128 - acc: 0.99 - ETA: 9s - loss: 0.0135 - acc: 0.9984 - ETA: 9s - loss: 0.0144 - acc: 0.997 - ETA: 9s - loss: 0.0152 - acc: 0.997 - ETA: 8s - loss: 0.0145 - acc: 0.997 - ETA: 8s - loss: 0.0139 - acc: 0.997 - ETA: 7s - loss: 0.0136 - acc: 0.997 - ETA: 7s - loss: 0.0140 - acc: 0.998 - ETA: 7s - loss: 0.0139 - acc: 0.998 - ETA: 6s - loss: 0.0137 - acc: 0.998 - ETA: 6s - loss: 0.0133 - acc: 0.998 - ETA: 6s - loss: 0.0136 - acc: 0.997 - ETA: 5s - loss: 0.0136 - acc: 0.997 - ETA: 5s - loss: 0.0133 - acc: 0.997 - ETA: 4s - loss: 0.0142 - acc: 0.997 - ETA: 4s - loss: 0.0136 - acc: 0.997 - ETA: 4s - loss: 0.0132 - acc: 0.997 - ETA: 3s - loss: 0.0130 - acc: 0.997 - ETA: 3s - loss: 0.0126 - acc: 0.997 - ETA: 2s - loss: 0.0124 - acc: 0.997 - ETA: 2s - loss: 0.0131 - acc: 0.997 - ETA: 2s - loss: 0.0128 - acc: 0.997 - ETA: 1s - loss: 0.0126 - acc: 0.998 - ETA: 1s - loss: 0.0127 - acc: 0.998 - ETA: 0s - loss: 0.0127 - acc: 0.998 - ETA: 0s - loss: 0.0127 - acc: 0.998 - ETA: 0s - loss: 0.0129 - acc: 0.997 - 15s 7ms/step - loss: 0.0129 - acc: 0.9978 - val_loss: 0.0022 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00160\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2247/2247 [==============================] - ETA: 14s - loss: 0.0117 - acc: 1.00 - ETA: 13s - loss: 0.0180 - acc: 0.99 - ETA: 12s - loss: 0.0151 - acc: 0.99 - ETA: 12s - loss: 0.0122 - acc: 0.99 - ETA: 11s - loss: 0.0117 - acc: 0.99 - ETA: 11s - loss: 0.0106 - acc: 0.99 - ETA: 11s - loss: 0.0108 - acc: 0.99 - ETA: 10s - loss: 0.0105 - acc: 0.99 - ETA: 10s - loss: 0.0112 - acc: 0.99 - ETA: 9s - loss: 0.0118 - acc: 0.9984 - ETA: 9s - loss: 0.0110 - acc: 0.998 - ETA: 9s - loss: 0.0104 - acc: 0.998 - ETA: 8s - loss: 0.0099 - acc: 0.998 - ETA: 8s - loss: 0.0102 - acc: 0.998 - ETA: 7s - loss: 0.0104 - acc: 0.999 - ETA: 7s - loss: 0.0103 - acc: 0.999 - ETA: 7s - loss: 0.0108 - acc: 0.999 - ETA: 6s - loss: 0.0109 - acc: 0.999 - ETA: 6s - loss: 0.0147 - acc: 0.998 - ETA: 5s - loss: 0.0143 - acc: 0.998 - ETA: 5s - loss: 0.0138 - acc: 0.998 - ETA: 5s - loss: 0.0139 - acc: 0.998 - ETA: 4s - loss: 0.0136 - acc: 0.998 - ETA: 4s - loss: 0.0134 - acc: 0.998 - ETA: 3s - loss: 0.0132 - acc: 0.998 - ETA: 3s - loss: 0.0128 - acc: 0.998 - ETA: 3s - loss: 0.0127 - acc: 0.998 - ETA: 2s - loss: 0.0124 - acc: 0.998 - ETA: 2s - loss: 0.0123 - acc: 0.998 - ETA: 1s - loss: 0.0129 - acc: 0.998 - ETA: 1s - loss: 0.0131 - acc: 0.998 - ETA: 1s - loss: 0.0130 - acc: 0.998 - ETA: 0s - loss: 0.0129 - acc: 0.998 - ETA: 0s - loss: 0.0126 - acc: 0.998 - ETA: 0s - loss: 0.0124 - acc: 0.998 - 15s 7ms/step - loss: 0.0124 - acc: 0.9987 - val_loss: 0.0028 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00160\n",
      "Epoch 42/50\n",
      "2247/2247 [==============================] - ETA: 12s - loss: 0.0030 - acc: 1.00 - ETA: 11s - loss: 0.0165 - acc: 1.00 - ETA: 11s - loss: 0.0156 - acc: 1.00 - ETA: 11s - loss: 0.0139 - acc: 1.00 - ETA: 11s - loss: 0.0131 - acc: 1.00 - ETA: 10s - loss: 0.0145 - acc: 0.99 - ETA: 10s - loss: 0.0141 - acc: 0.99 - ETA: 10s - loss: 0.0138 - acc: 0.99 - ETA: 10s - loss: 0.0127 - acc: 0.99 - ETA: 9s - loss: 0.0120 - acc: 0.9984 - ETA: 9s - loss: 0.0121 - acc: 0.998 - ETA: 9s - loss: 0.0129 - acc: 0.998 - ETA: 8s - loss: 0.0128 - acc: 0.998 - ETA: 8s - loss: 0.0137 - acc: 0.997 - ETA: 7s - loss: 0.0136 - acc: 0.997 - ETA: 7s - loss: 0.0138 - acc: 0.998 - ETA: 7s - loss: 0.0132 - acc: 0.998 - ETA: 6s - loss: 0.0128 - acc: 0.998 - ETA: 6s - loss: 0.0125 - acc: 0.998 - ETA: 6s - loss: 0.0124 - acc: 0.998 - ETA: 5s - loss: 0.0120 - acc: 0.998 - ETA: 5s - loss: 0.0132 - acc: 0.997 - ETA: 5s - loss: 0.0132 - acc: 0.998 - ETA: 4s - loss: 0.0134 - acc: 0.998 - ETA: 4s - loss: 0.0132 - acc: 0.998 - ETA: 3s - loss: 0.0133 - acc: 0.998 - ETA: 3s - loss: 0.0130 - acc: 0.998 - ETA: 2s - loss: 0.0140 - acc: 0.997 - ETA: 2s - loss: 0.0140 - acc: 0.997 - ETA: 2s - loss: 0.0139 - acc: 0.997 - ETA: 1s - loss: 0.0138 - acc: 0.998 - ETA: 1s - loss: 0.0135 - acc: 0.998 - ETA: 0s - loss: 0.0134 - acc: 0.998 - ETA: 0s - loss: 0.0131 - acc: 0.998 - ETA: 0s - loss: 0.0131 - acc: 0.998 - 16s 7ms/step - loss: 0.0130 - acc: 0.9982 - val_loss: 0.0092 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00160\n",
      "Epoch 43/50\n",
      "2247/2247 [==============================] - ETA: 12s - loss: 0.0076 - acc: 1.00 - ETA: 12s - loss: 0.0105 - acc: 1.00 - ETA: 12s - loss: 0.0079 - acc: 1.00 - ETA: 12s - loss: 0.0097 - acc: 1.00 - ETA: 12s - loss: 0.0094 - acc: 1.00 - ETA: 11s - loss: 0.0084 - acc: 1.00 - ETA: 11s - loss: 0.0093 - acc: 1.00 - ETA: 11s - loss: 0.0088 - acc: 1.00 - ETA: 10s - loss: 0.0092 - acc: 1.00 - ETA: 10s - loss: 0.0087 - acc: 1.00 - ETA: 9s - loss: 0.0083 - acc: 1.0000 - ETA: 9s - loss: 0.0081 - acc: 1.000 - ETA: 8s - loss: 0.0083 - acc: 1.000 - ETA: 8s - loss: 0.0088 - acc: 1.000 - ETA: 8s - loss: 0.0085 - acc: 1.000 - ETA: 7s - loss: 0.0082 - acc: 1.000 - ETA: 7s - loss: 0.0085 - acc: 1.000 - ETA: 6s - loss: 0.0091 - acc: 1.000 - ETA: 6s - loss: 0.0090 - acc: 1.000 - ETA: 6s - loss: 0.0097 - acc: 0.999 - ETA: 5s - loss: 0.0099 - acc: 0.999 - ETA: 5s - loss: 0.0119 - acc: 0.998 - ETA: 4s - loss: 0.0116 - acc: 0.998 - ETA: 4s - loss: 0.0112 - acc: 0.998 - ETA: 4s - loss: 0.0109 - acc: 0.998 - ETA: 3s - loss: 0.0105 - acc: 0.998 - ETA: 3s - loss: 0.0109 - acc: 0.998 - ETA: 2s - loss: 0.0108 - acc: 0.998 - ETA: 2s - loss: 0.0105 - acc: 0.998 - ETA: 2s - loss: 0.0106 - acc: 0.999 - ETA: 1s - loss: 0.0103 - acc: 0.999 - ETA: 1s - loss: 0.0101 - acc: 0.999 - ETA: 0s - loss: 0.0099 - acc: 0.999 - ETA: 0s - loss: 0.0099 - acc: 0.999 - ETA: 0s - loss: 0.0097 - acc: 0.999 - 16s 7ms/step - loss: 0.0097 - acc: 0.9991 - val_loss: 0.0076 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00160\n",
      "Epoch 44/50\n",
      "2247/2247 [==============================] - ETA: 14s - loss: 0.0100 - acc: 1.00 - ETA: 13s - loss: 0.0072 - acc: 1.00 - ETA: 12s - loss: 0.0133 - acc: 0.99 - ETA: 12s - loss: 0.0119 - acc: 0.99 - ETA: 11s - loss: 0.0099 - acc: 0.99 - ETA: 11s - loss: 0.0090 - acc: 0.99 - ETA: 10s - loss: 0.0083 - acc: 0.99 - ETA: 10s - loss: 0.0088 - acc: 0.99 - ETA: 10s - loss: 0.0088 - acc: 0.99 - ETA: 9s - loss: 0.0087 - acc: 0.9984 - ETA: 9s - loss: 0.0083 - acc: 0.998 - ETA: 9s - loss: 0.0082 - acc: 0.998 - ETA: 8s - loss: 0.0077 - acc: 0.998 - ETA: 8s - loss: 0.0079 - acc: 0.998 - ETA: 7s - loss: 0.0077 - acc: 0.999 - ETA: 7s - loss: 0.0076 - acc: 0.999 - ETA: 7s - loss: 0.0088 - acc: 0.998 - ETA: 6s - loss: 0.0088 - acc: 0.998 - ETA: 6s - loss: 0.0085 - acc: 0.998 - ETA: 5s - loss: 0.0085 - acc: 0.998 - ETA: 5s - loss: 0.0085 - acc: 0.998 - ETA: 5s - loss: 0.0084 - acc: 0.998 - ETA: 4s - loss: 0.0086 - acc: 0.998 - ETA: 4s - loss: 0.0084 - acc: 0.998 - ETA: 4s - loss: 0.0087 - acc: 0.998 - ETA: 3s - loss: 0.0085 - acc: 0.998 - ETA: 3s - loss: 0.0084 - acc: 0.998 - ETA: 2s - loss: 0.0082 - acc: 0.998 - ETA: 2s - loss: 0.0087 - acc: 0.998 - ETA: 2s - loss: 0.0086 - acc: 0.999 - ETA: 1s - loss: 0.0088 - acc: 0.999 - ETA: 1s - loss: 0.0088 - acc: 0.999 - ETA: 0s - loss: 0.0088 - acc: 0.999 - ETA: 0s - loss: 0.0089 - acc: 0.999 - ETA: 0s - loss: 0.0089 - acc: 0.999 - 15s 7ms/step - loss: 0.0089 - acc: 0.9991 - val_loss: 0.0048 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00160\n",
      "Epoch 45/50\n",
      "2247/2247 [==============================] - ETA: 13s - loss: 0.0015 - acc: 1.00 - ETA: 13s - loss: 0.0094 - acc: 1.00 - ETA: 12s - loss: 0.0077 - acc: 1.00 - ETA: 12s - loss: 0.0066 - acc: 1.00 - ETA: 11s - loss: 0.0064 - acc: 1.00 - ETA: 11s - loss: 0.0059 - acc: 1.00 - ETA: 10s - loss: 0.0057 - acc: 1.00 - ETA: 10s - loss: 0.0086 - acc: 0.99 - ETA: 10s - loss: 0.0084 - acc: 0.99 - ETA: 9s - loss: 0.0080 - acc: 0.9984 - ETA: 9s - loss: 0.0083 - acc: 0.998 - ETA: 9s - loss: 0.0081 - acc: 0.998 - ETA: 8s - loss: 0.0076 - acc: 0.998 - ETA: 8s - loss: 0.0072 - acc: 0.998 - ETA: 7s - loss: 0.0070 - acc: 0.999 - ETA: 7s - loss: 0.0078 - acc: 0.999 - ETA: 7s - loss: 0.0083 - acc: 0.998 - ETA: 6s - loss: 0.0080 - acc: 0.998 - ETA: 6s - loss: 0.0077 - acc: 0.998 - ETA: 5s - loss: 0.0078 - acc: 0.998 - ETA: 5s - loss: 0.0077 - acc: 0.998 - ETA: 5s - loss: 0.0075 - acc: 0.998 - ETA: 4s - loss: 0.0077 - acc: 0.998 - ETA: 4s - loss: 0.0075 - acc: 0.998 - ETA: 3s - loss: 0.0075 - acc: 0.998 - ETA: 3s - loss: 0.0073 - acc: 0.998 - ETA: 3s - loss: 0.0071 - acc: 0.998 - ETA: 2s - loss: 0.0072 - acc: 0.998 - ETA: 2s - loss: 0.0070 - acc: 0.998 - ETA: 1s - loss: 0.0068 - acc: 0.999 - ETA: 1s - loss: 0.0076 - acc: 0.998 - ETA: 1s - loss: 0.0077 - acc: 0.998 - ETA: 0s - loss: 0.0077 - acc: 0.998 - ETA: 0s - loss: 0.0080 - acc: 0.998 - ETA: 0s - loss: 0.0079 - acc: 0.998 - 15s 7ms/step - loss: 0.0080 - acc: 0.9987 - val_loss: 0.0043 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00160\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2247/2247 [==============================] - ETA: 11s - loss: 0.0358 - acc: 0.98 - ETA: 12s - loss: 0.0203 - acc: 0.99 - ETA: 11s - loss: 0.0209 - acc: 0.99 - ETA: 11s - loss: 0.0168 - acc: 0.99 - ETA: 11s - loss: 0.0146 - acc: 0.99 - ETA: 11s - loss: 0.0134 - acc: 0.99 - ETA: 10s - loss: 0.0131 - acc: 0.99 - ETA: 10s - loss: 0.0121 - acc: 0.99 - ETA: 10s - loss: 0.0111 - acc: 0.99 - ETA: 9s - loss: 0.0123 - acc: 0.9969 - ETA: 9s - loss: 0.0114 - acc: 0.997 - ETA: 9s - loss: 0.0160 - acc: 0.996 - ETA: 8s - loss: 0.0159 - acc: 0.996 - ETA: 8s - loss: 0.0156 - acc: 0.996 - ETA: 7s - loss: 0.0156 - acc: 0.996 - ETA: 7s - loss: 0.0151 - acc: 0.997 - ETA: 7s - loss: 0.0159 - acc: 0.996 - ETA: 6s - loss: 0.0153 - acc: 0.996 - ETA: 6s - loss: 0.0156 - acc: 0.996 - ETA: 5s - loss: 0.0150 - acc: 0.996 - ETA: 5s - loss: 0.0156 - acc: 0.996 - ETA: 5s - loss: 0.0156 - acc: 0.996 - ETA: 4s - loss: 0.0154 - acc: 0.996 - ETA: 4s - loss: 0.0151 - acc: 0.996 - ETA: 3s - loss: 0.0148 - acc: 0.996 - ETA: 3s - loss: 0.0145 - acc: 0.997 - ETA: 3s - loss: 0.0140 - acc: 0.997 - ETA: 2s - loss: 0.0137 - acc: 0.997 - ETA: 2s - loss: 0.0135 - acc: 0.997 - ETA: 2s - loss: 0.0133 - acc: 0.997 - ETA: 1s - loss: 0.0130 - acc: 0.997 - ETA: 1s - loss: 0.0127 - acc: 0.997 - ETA: 0s - loss: 0.0124 - acc: 0.997 - ETA: 0s - loss: 0.0122 - acc: 0.997 - ETA: 0s - loss: 0.0123 - acc: 0.997 - 15s 7ms/step - loss: 0.0122 - acc: 0.9973 - val_loss: 0.0032 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00160\n",
      "Epoch 47/50\n",
      "2247/2247 [==============================] - ETA: 12s - loss: 0.0550 - acc: 0.98 - ETA: 11s - loss: 0.0326 - acc: 0.99 - ETA: 11s - loss: 0.0241 - acc: 0.99 - ETA: 11s - loss: 0.0196 - acc: 0.99 - ETA: 11s - loss: 0.0196 - acc: 0.99 - ETA: 10s - loss: 0.0178 - acc: 0.99 - ETA: 10s - loss: 0.0195 - acc: 0.99 - ETA: 10s - loss: 0.0184 - acc: 0.99 - ETA: 9s - loss: 0.0166 - acc: 0.9948 - ETA: 9s - loss: 0.0186 - acc: 0.993 - ETA: 9s - loss: 0.0176 - acc: 0.994 - ETA: 8s - loss: 0.0178 - acc: 0.993 - ETA: 8s - loss: 0.0175 - acc: 0.994 - ETA: 8s - loss: 0.0170 - acc: 0.994 - ETA: 7s - loss: 0.0161 - acc: 0.994 - ETA: 7s - loss: 0.0154 - acc: 0.995 - ETA: 6s - loss: 0.0149 - acc: 0.995 - ETA: 6s - loss: 0.0149 - acc: 0.995 - ETA: 6s - loss: 0.0149 - acc: 0.995 - ETA: 5s - loss: 0.0144 - acc: 0.996 - ETA: 5s - loss: 0.0142 - acc: 0.996 - ETA: 5s - loss: 0.0140 - acc: 0.996 - ETA: 4s - loss: 0.0137 - acc: 0.996 - ETA: 4s - loss: 0.0132 - acc: 0.996 - ETA: 3s - loss: 0.0130 - acc: 0.996 - ETA: 3s - loss: 0.0129 - acc: 0.997 - ETA: 3s - loss: 0.0131 - acc: 0.996 - ETA: 2s - loss: 0.0128 - acc: 0.996 - ETA: 2s - loss: 0.0128 - acc: 0.996 - ETA: 1s - loss: 0.0125 - acc: 0.996 - ETA: 1s - loss: 0.0121 - acc: 0.997 - ETA: 1s - loss: 0.0119 - acc: 0.997 - ETA: 0s - loss: 0.0122 - acc: 0.996 - ETA: 0s - loss: 0.0122 - acc: 0.996 - ETA: 0s - loss: 0.0123 - acc: 0.996 - 15s 7ms/step - loss: 0.0123 - acc: 0.9969 - val_loss: 0.0038 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00160\n",
      "Epoch 48/50\n",
      "2247/2247 [==============================] - ETA: 12s - loss: 0.0026 - acc: 1.00 - ETA: 12s - loss: 0.0032 - acc: 1.00 - ETA: 12s - loss: 0.0049 - acc: 1.00 - ETA: 11s - loss: 0.0060 - acc: 1.00 - ETA: 11s - loss: 0.0053 - acc: 1.00 - ETA: 11s - loss: 0.0074 - acc: 1.00 - ETA: 10s - loss: 0.0132 - acc: 0.99 - ETA: 10s - loss: 0.0130 - acc: 0.99 - ETA: 9s - loss: 0.0125 - acc: 0.9983 - ETA: 9s - loss: 0.0124 - acc: 0.998 - ETA: 9s - loss: 0.0118 - acc: 0.998 - ETA: 8s - loss: 0.0116 - acc: 0.998 - ETA: 8s - loss: 0.0115 - acc: 0.998 - ETA: 8s - loss: 0.0109 - acc: 0.998 - ETA: 7s - loss: 0.0105 - acc: 0.999 - ETA: 7s - loss: 0.0103 - acc: 0.999 - ETA: 7s - loss: 0.0101 - acc: 0.999 - ETA: 6s - loss: 0.0099 - acc: 0.999 - ETA: 6s - loss: 0.0099 - acc: 0.999 - ETA: 5s - loss: 0.0112 - acc: 0.998 - ETA: 5s - loss: 0.0109 - acc: 0.998 - ETA: 5s - loss: 0.0106 - acc: 0.998 - ETA: 4s - loss: 0.0105 - acc: 0.998 - ETA: 4s - loss: 0.0103 - acc: 0.998 - ETA: 3s - loss: 0.0102 - acc: 0.998 - ETA: 3s - loss: 0.0100 - acc: 0.998 - ETA: 3s - loss: 0.0099 - acc: 0.998 - ETA: 2s - loss: 0.0098 - acc: 0.998 - ETA: 2s - loss: 0.0096 - acc: 0.998 - ETA: 1s - loss: 0.0096 - acc: 0.999 - ETA: 1s - loss: 0.0095 - acc: 0.999 - ETA: 1s - loss: 0.0094 - acc: 0.999 - ETA: 0s - loss: 0.0096 - acc: 0.999 - ETA: 0s - loss: 0.0098 - acc: 0.998 - ETA: 0s - loss: 0.0096 - acc: 0.998 - 15s 7ms/step - loss: 0.0096 - acc: 0.9987 - val_loss: 0.0034 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00160\n",
      "Epoch 49/50\n",
      "2247/2247 [==============================] - ETA: 13s - loss: 0.0030 - acc: 1.00 - ETA: 13s - loss: 0.0061 - acc: 1.00 - ETA: 13s - loss: 0.0043 - acc: 1.00 - ETA: 12s - loss: 0.0087 - acc: 0.99 - ETA: 12s - loss: 0.0130 - acc: 0.99 - ETA: 11s - loss: 0.0110 - acc: 0.99 - ETA: 11s - loss: 0.0136 - acc: 0.99 - ETA: 11s - loss: 0.0136 - acc: 0.99 - ETA: 10s - loss: 0.0134 - acc: 0.99 - ETA: 10s - loss: 0.0123 - acc: 0.99 - ETA: 9s - loss: 0.0118 - acc: 0.9972 - ETA: 9s - loss: 0.0112 - acc: 0.997 - ETA: 8s - loss: 0.0119 - acc: 0.996 - ETA: 8s - loss: 0.0111 - acc: 0.996 - ETA: 8s - loss: 0.0106 - acc: 0.996 - ETA: 7s - loss: 0.0104 - acc: 0.997 - ETA: 7s - loss: 0.0104 - acc: 0.997 - ETA: 6s - loss: 0.0101 - acc: 0.997 - ETA: 6s - loss: 0.0098 - acc: 0.997 - ETA: 5s - loss: 0.0095 - acc: 0.997 - ETA: 5s - loss: 0.0098 - acc: 0.997 - ETA: 5s - loss: 0.0098 - acc: 0.997 - ETA: 4s - loss: 0.0100 - acc: 0.998 - ETA: 4s - loss: 0.0097 - acc: 0.998 - ETA: 3s - loss: 0.0104 - acc: 0.998 - ETA: 3s - loss: 0.0107 - acc: 0.998 - ETA: 3s - loss: 0.0107 - acc: 0.998 - ETA: 2s - loss: 0.0103 - acc: 0.998 - ETA: 2s - loss: 0.0100 - acc: 0.998 - ETA: 1s - loss: 0.0099 - acc: 0.998 - ETA: 1s - loss: 0.0098 - acc: 0.998 - ETA: 1s - loss: 0.0098 - acc: 0.998 - ETA: 0s - loss: 0.0096 - acc: 0.998 - ETA: 0s - loss: 0.0094 - acc: 0.998 - ETA: 0s - loss: 0.0098 - acc: 0.998 - 15s 7ms/step - loss: 0.0098 - acc: 0.9982 - val_loss: 0.0041 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00160\n",
      "Epoch 50/50\n",
      "2247/2247 [==============================] - ETA: 13s - loss: 0.0084 - acc: 1.00 - ETA: 12s - loss: 0.0084 - acc: 1.00 - ETA: 12s - loss: 0.0115 - acc: 0.99 - ETA: 12s - loss: 0.0094 - acc: 0.99 - ETA: 11s - loss: 0.0110 - acc: 0.99 - ETA: 11s - loss: 0.0100 - acc: 0.99 - ETA: 10s - loss: 0.0102 - acc: 0.99 - ETA: 10s - loss: 0.0094 - acc: 0.99 - ETA: 9s - loss: 0.0136 - acc: 0.9931 - ETA: 9s - loss: 0.0133 - acc: 0.993 - ETA: 9s - loss: 0.0127 - acc: 0.994 - ETA: 8s - loss: 0.0125 - acc: 0.994 - ETA: 8s - loss: 0.0123 - acc: 0.995 - ETA: 8s - loss: 0.0116 - acc: 0.995 - ETA: 7s - loss: 0.0111 - acc: 0.995 - ETA: 7s - loss: 0.0114 - acc: 0.996 - ETA: 6s - loss: 0.0109 - acc: 0.996 - ETA: 6s - loss: 0.0105 - acc: 0.996 - ETA: 6s - loss: 0.0103 - acc: 0.996 - ETA: 5s - loss: 0.0099 - acc: 0.996 - ETA: 5s - loss: 0.0095 - acc: 0.997 - ETA: 4s - loss: 0.0093 - acc: 0.997 - ETA: 4s - loss: 0.0091 - acc: 0.997 - ETA: 4s - loss: 0.0091 - acc: 0.997 - ETA: 3s - loss: 0.0090 - acc: 0.997 - ETA: 3s - loss: 0.0089 - acc: 0.997 - ETA: 3s - loss: 0.0086 - acc: 0.997 - ETA: 2s - loss: 0.0085 - acc: 0.997 - ETA: 2s - loss: 0.0082 - acc: 0.997 - ETA: 1s - loss: 0.0084 - acc: 0.997 - ETA: 1s - loss: 0.0084 - acc: 0.998 - ETA: 1s - loss: 0.0083 - acc: 0.998 - ETA: 0s - loss: 0.0081 - acc: 0.998 - ETA: 0s - loss: 0.0087 - acc: 0.998 - ETA: 0s - loss: 0.0085 - acc: 0.998 - 15s 7ms/step - loss: 0.0084 - acc: 0.9982 - val_loss: 0.0052 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00160\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), callbacks=[mcp_save], batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
